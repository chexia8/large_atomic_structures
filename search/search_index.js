var __index = {"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"augmented_partition","text":"<p>Welcome to the documentation for the <code>augmented_partition</code> code! Here you will find everything you need to get started with your own Python package.</p> <p>Check out the corresponding \u2b50YouTube tutorial\u2b50 for a video overview!</p>"},{"location":"example_docs/about/changelog.html","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"example_docs/about/changelog.html#001","title":"[0.0.1]","text":""},{"location":"example_docs/about/changelog.html#added","title":"Added","text":"<ul> <li>The initial release!</li> </ul>"},{"location":"example_docs/about/license.html","title":"License","text":"LICENSE.md<pre><code>BSD 3-Clause License\n\nCopyright (c) 2024\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\n   contributors may be used to endorse or promote products derived from\n   this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre>"},{"location":"example_docs/code/hints.html","title":"Type Hinting","text":""},{"location":"example_docs/code/hints.html#overview","title":"Overview","text":"<p>In the sample functions provided with the augmented_partition repository, you will see something like:</p> <pre><code>def make_array(val: float, length: int = 3) -&gt; NDArray:\n</code></pre> <p>If you aren't familiar with type-hinting, that's what the <code>: float</code>, <code>: int</code>, and <code>-&gt; NDArray</code> are indicating. They tell the user what the expected types are for each parameter and return. They are not enforced in any way; they are merely hints (as the name suggests). It is always advisable to use type hints in your code, so get in the habit of doing so!</p> <p>Tip</p> <p>If you have to import a given function solely for type-hinting purposes, you should put it within an <code>if TYPE_CHECKING</code> block (as demonstrated in <code>/src/augmented_partition/examples/sample.py</code>). It will then only be imported when using a type-checking utility, reducing the overall import time of your module.</p> <p>Note</p> <p>You do not need to touch the <code>py.typed</code> file. It is a marker that Python uses to indicate that type-hinting should be used in any programs that depend on your code.</p>"},{"location":"example_docs/code/hints.html#type-checking","title":"Type Checking","text":"<p>As mentioned, the type hints are just that: hints. If you want to ensure that the types are strictly adhered to across your codebase, you can use mypy to do so. This is a slightly more advanced tool, however, so is not something you need to worry about right now.</p>"},{"location":"example_docs/code/source.html","title":"Source Code","text":""},{"location":"example_docs/code/source.html#adding-your-code","title":"Adding Your Code","text":"<p>All source code (i.e. your various modules, functions, classes, and so on) should be placed in the <code>/src/&lt;MyPackageName&gt;</code> directory. A sample file named <code>examples/sample.py</code> is included here as a representative example, which you should replace.</p> <p>All the code in the <code>src</code> directory can be imported now that you have installed your package.</p> <p>Tip</p> <p>As an example, you can import and use the demonstration augmented_partition.examples.sample functions as follows:</p> <pre><code>from MyPackageName.examples.sample import add, make_array\n\nprint(add(1, 2))  # 3\nprint(make_array(3, length=4))  # [3, 3, 3, 3]\n</code></pre> <p>Note</p> <p>For any subfolder within <code>src/&lt;MyPackageName&gt;</code> containing Python code, you must have an <code>__init__.py</code> file, which will tell Python that this is a module you can import.</p>"},{"location":"example_docs/code/source.html#docstrings","title":"Docstrings","text":"<p>The code comments beneath each function are called docstrings. They should provide an overview of the purpose of the function, the various parameters, and the return values (if any). Here, we are using the NumPy style docstrings, but you can pick a different style if you like later on.</p>"},{"location":"example_docs/code/source.html#linting-and-formatting","title":"Linting and Formatting","text":"<p>When you installed the <code>[dev]</code> dependencies, you also installed <code>ruff</code>, which is a versatile Python linter to clean up your code. To run <code>ruff</code>, use the following command in the base directory: <code>ruff check --fix</code>. This will also be done automatically on pull requests via the pre-commit CI tool.</p>"},{"location":"example_docs/code/tests.html","title":"Testing","text":""},{"location":"example_docs/code/tests.html#overview","title":"Overview","text":"<p>Writing effective tests for your code is a crucial part of the programming process. It is the best way to ensure that changes you make to your codebase throughout the development process do not break the core functionality of your code. This may be your first time writing tests, but trust me that it is essential.</p>"},{"location":"example_docs/code/tests.html#pytest","title":"Pytest","text":"<p>Put any unit tests in the <code>/tests</code> folder. A sample test (i.e. <code>/tests/sample/examples/test_sample.py</code>) is included as a representative example.</p> <p>Note</p> <p>All your testing scripts should start with <code>test_</code> in the filename.</p> <p>When you installed the package with the <code>[dev]</code> extras, you installed everything you need to run your unit tests. To run the unit tests locally, run <code>pytest .</code> in the base directory. It will let you know if any tests fail and what the reason is for each failure.</p>"},{"location":"example_docs/code/tests.html#code-coverage","title":"Code Coverage","text":"<p>The <code>/.codecov.yml</code> file is a configuration file for Codecov, which will tell you the fraction of lines covered by your test suite if the GitHub integration is enabled.</p> <p>In order for Codecov to work properly, you will need to make a Codecov account and activate it on your newly made repository. You will also need to add the <code>CODECOV_TOKEN</code> secret to your repository.</p>"},{"location":"example_docs/github/commits.html","title":"Saving Your Work","text":"<p>There are still a few more steps left, but at this point you will want to make sure to save your work!</p>"},{"location":"example_docs/github/commits.html#pushing-your-changes","title":"Pushing Your Changes","text":"<p>Commit any changes you've made and push them to your repository</p> <p>If you are using a program like GitKraken, this will involve the following steps:</p> <ol> <li>Save your work.</li> <li>Recommended: make a new branch for your work (e.g. <code>develop</code>)</li> <li>Click \"Stage all changes\".</li> <li>Add a helpful commit message.</li> <li>Commit the changes.</li> <li>Click \"push\".</li> </ol> <p>Tip</p> <p>It is advisable to make changes in a new branch rather than in <code>main</code> so that you can ensure your unit tests pass before the code is merged into the codebase.</p> <p>Then go on GitHub to see your changes. Assuming you pushed your changes to a new branch, you'll likely see a message asking if you want to make a Pull Request to merge in your changes into the <code>main</code> branch.</p>"},{"location":"example_docs/github/workflows.html","title":"GitHub Actions","text":""},{"location":"example_docs/github/workflows.html#workflows","title":"Workflows","text":"<p>The last major piece of the puzzle is GitHub Actions, which is an automated suite of workflows that run every time a commit or pull request is made. The GitHub workflows can be found in the <code>.github/workflows</code> folder.</p>"},{"location":"example_docs/github/workflows.html#tests","title":"Tests","text":"<p>The <code>/.github/workflows/tests.yaml</code> file contains the workflow to have GitHub automatically run the full suite of tests on every commit and pull request. For the most basic case outlined here, you do not need to make any modifications (other than, perhaps, the desired Python versions you wish to test on).</p> <p>By default, the test suite is set up to install the following packages:</p> <pre><code>pip install -r tests/requirements.txt\npip install .[dev]\n</code></pre> <p>As you can see above, it will install specific versions of the dependencies outlined in <code>/tests/requirements.txt</code>. Unlike <code>pyproject.toml</code>, you want to include specific versions here so that your test suite is reproducible.</p> <p>The <code>/.github/dependabot.yml</code> file is set up such that Dependabot will automatically open pull requests to update any versions in your <code>/tests/requirements.txt</code> file as they come out so that your code will always be tested on the newest releases of the various dependencies. This will ensure that your code doesn't break as dependencies update, but if it does, you will know what needs fixing.</p>"},{"location":"example_docs/github/workflows.html#documentation","title":"Documentation","text":"<p>The <code>/.github/workflows/docs.yaml</code> file contains the workflow to have GitHub test the build process for the documentation and deploy it (if enabled).</p> <p>To have your documentation automatically deployed on a GitHub webpage:</p> <ol> <li>Go to the settings page of your repository.</li> <li>Click the \"Pages\" section under \"Code and automation.\"</li> <li>Select \"Deploy from a branch\" under \"Source\"</li> <li>Set the branch to be \"gh-pages\" with \"/ (root)\" as the folder.</li> <li>Wait a minute and refresh the page. You'll see a message that your site is live with a URL to the documentation.</li> </ol> <p></p> <p>Once this process is done, the documentation will be live and will update with each commit.</p>"},{"location":"example_docs/github/workflows.html#release","title":"Release","text":"<p>The <code>/.github/workflows/release.yaml</code> file contains the workflow to have GitHub upload your package to PyPI every time you mint a new release on GitHub. This is a slightly more advanced topic that you can read more about at a later time, but it's there for when you need it.</p>"},{"location":"example_docs/installation/install.html","title":"Pip Installing","text":"<p>Now it's time to install your Python package! You will want to install your Python package in \"editable\" mode, which means you won't have to re-install your code every time you make updates to it. Additionally, you will want to install several optional dependencies (listed under the <code>[project.optional-dependencies]</code> header in <code>pyproject.toml</code>) to ensure that you can test and build the documentation for your code.</p> <p>With all this in mind, you will want to run the following in the command line from the base of the package directory:</p> <pre><code>pip install -e .[dev,docs]\n</code></pre> <p>Here, the <code>-e</code> means editable mode, the <code>.</code> means the current directory, and the <code>[dev,docs]</code> means it will install the \"dev\" and \"docs\" optional dependency set listed in the <code>pyproject.toml</code> file.</p> <p>Tip</p> <p>You should generally start from a clean Python environment, such as a new Conda environment if you are using Anaconda or one of its variants.</p> <p>To make sure you installed your package successfully, open a Python console and run <code>import &lt;MyPackageName&gt;</code>. It should return without any errors. If there are errors, it's likely because you forgot to replace a \"augmented_partition\" placeholder with the name of your package.</p>"},{"location":"example_docs/installation/pyproject.html","title":"pyproject.toml","text":"<p>The <code>pyproject.toml</code> file contains all of the necessary information on how Python will install your package.</p>"},{"location":"example_docs/installation/pyproject.html#metadata","title":"Metadata","text":"<p>There are several metadata-related fields that you will likely want to update. You should have already updated the <code>name</code> of the package in a prior step when you replaced \"augmented_partition\" everywhere, but you will also want to change the following:</p> <ul> <li><code>description</code></li> <li><code>license</code> (if you changed the default <code>LICENSE.md</code> file)</li> <li><code>authors</code></li> <li><code>keywords</code></li> </ul> <p>Aside from the <code>name</code>, none of the above are strictly necessary and can be left as-is (or removed) if you are unsure.</p>"},{"location":"example_docs/installation/pyproject.html#dependencies","title":"Dependencies","text":"<p>The most important fields to update are related to the dependencies: the Python packages that your own code relies on. This will ensure that they are automatically installed when installing your Python package.</p> <p>The required dependencies are listed under the <code>[project]</code> header in the <code>dependencies</code> field. By default, the augmented_partition repository lists <code>[\"numpy\"]</code>. Include any dependencies you want in this list, separated by commas. This should be all the packages you import in your code that are not standard Python libraries.</p> <p>Tip</p> <p>Not sure what dependencies you need just yet? No problem. You can come back to this later.</p> <p>Note</p> <p>If you know a specific minimum version is needed for your code, you should set that here as well (e.g. <code>[\"numpy&gt;=1.23.0\"]</code>). However, only use this when it is necessary so that users aren't restricted to a given version without a valid reason.</p>"},{"location":"example_docs/installation/pyproject.html#python-version","title":"Python Version","text":"<p>If you know your code can only run on certain Python versions, you should specify that in the <code>requires-python</code> field under the <code>[project]</code> header. When in doubt, we recommend setting it to the range of currently supported Python versions (specifically those with security and bugfix statuses).</p> <p>You can also update the listed versions in the <code>classifiers</code> field, although this is only for informational purposes. The list of supported Python classifier fields can be found on the corresponding PyPI page.</p>"},{"location":"example_docs/intro/resources.html","title":"Resources","text":""},{"location":"example_docs/intro/resources.html#software-development","title":"Software Development","text":"<p>Looking for external resources to get started with software development? Here are some useful ones:</p> <ul> <li>Scientific Python Development Guide</li> <li>Turing Way Guide for Reproducible Research</li> <li>Turing Way Guide for Project Design</li> </ul>"},{"location":"example_docs/intro/resources.html#git-and-version-control","title":"Git and Version Control","text":"<p>For git and version control specifically:</p> <ul> <li>Git Guides</li> <li>Software Carpentry</li> </ul> <p>For GitHub:</p> <ul> <li>GitHub Docs</li> <li>GitHub Skills</li> </ul>"},{"location":"example_docs/intro/why.html","title":"Why?","text":""},{"location":"example_docs/intro/why.html#purpose","title":"Purpose","text":"<p>The first question to address is: why? Why use a augmented_partition repository like this? Why make a Python package at all, as opposed to writing custom scripts or Jupyter Notebooks?</p> <p>The answer, in short, is sustainable and reproducible software development. Here are some of the benefits:</p> <ul> <li>Your package can be easily installed by others using <code>pip</code>.</li> <li>Your package can have automated unit tests that run every time you make a commit, making sure you don't accidentally break your own code.</li> <li>You can easily make and share documentation with no hassle.</li> <li>You will instantly be adopting good programming practices that will help you for life.</li> </ul> <p>Of course, there are many more reasons, but hopefully that's convincing enough!</p>"},{"location":"example_docs/intro/why.html#alternatives","title":"Alternatives","text":"<p>This is by no means the only augmented_partition of its kind. Some alternatives include:</p> <ul> <li>cookiecutter and cookiecutter-cms</li> <li>pyscaffold</li> <li>python-package-augmented_partition</li> </ul> <p>... and many more.</p> <p>Feel free to use them if you wish! This augmented_partition repository exists because we are all opinionated people, and this augmented_partition focuses on things that I value most. But the point is to just use something that works well for you.</p>"},{"location":"example_docs/mkdocs/build.html","title":"Building the Docs","text":""},{"location":"example_docs/mkdocs/build.html#the-mkdocsyml-file","title":"The <code>mkdocs.yml</code> File","text":"<p>Once you have added your documentation, you will need to update the <code>/mkdocs.yml</code> file with information about how you want to arrange the files. Specifically, you will need to update the <code>nav</code> secction of the <code>mkdocs.yml</code> file to point to all your individual <code>.md</code> files, organizing them by category.</p> <p>Note</p> <p>Keep the <code>- Code Documentation: reference/</code> line in the <code>nav</code> section of <code>mkdocs.yml</code>. It will automatically transform your docstrings into beautiful documentation! The rest of the <code>nav</code> items you can replace.</p>"},{"location":"example_docs/mkdocs/build.html#the-build-process","title":"The Build Process","text":"<p>To see how your documentation will look in advance, you can build it locally by running the following command in the base directory:</p> <pre><code>mkdocs serve\n</code></pre> <p>A URL will be printed out that you can open in your browser.</p>"},{"location":"example_docs/mkdocs/build.html#deploying-the-docs","title":"Deploying the Docs","text":"<p>To allow your documentation to be visible via GitHub Pages, go to \"Settings &gt; Pages\" in your repository's settings and make sure \"Branch\" is set to \"gh-pages\" instead of \"main\".</p>"},{"location":"example_docs/mkdocs/docs.html","title":"Writing the Docs","text":""},{"location":"example_docs/mkdocs/docs.html#mkdocs","title":"Mkdocs","text":"<p>Now it's time to write some documentation! This isn't very difficult, and of course you're reading some documentation right now. The documentation is written using markdown, which is the same way GitHub comments are formatted.</p> <p>Tip</p> <p>Check out the Markdown Guide for an overview of the basic syntax.</p> <p>This augmented_partition repository uses a documentation format called mkdocs, specifically a useful theme called Material for Mkdocs. This enables many wonderful goodies like the \"tip\" callout you see above and much more.</p>"},{"location":"example_docs/mkdocs/docs.html#adding-markdown-files","title":"Adding Markdown Files","text":"<p>Your documentation will live in the <code>/docs</code> folder. You can think of each markdown (<code>.md</code>) file as being a specific page in the documentation, and each folder as being a related collection of pages. The markdown page you are reading right now is found at <code>/docs/example_docs/mkdocs/docs.md</code>, for instance. Of course, you will want to replce the <code>/docs/example_docs</code> folder with your own documentation.</p> <p>Note</p> <p>You typically do not need to touch the <code>/docs/gen_ref_pages.py</code> script. It is used to automatically build the documentation for your code from its docstrings.</p>"},{"location":"example_docs/setup/basics.html","title":"Initial Changes","text":"<p>At this point, you now have your augmented_partition repository on GitHub and locally on your machine. Now it's time to start making some modifications.</p>"},{"location":"example_docs/setup/basics.html#readme","title":"README","text":"<p>The first thing to do is update the README (<code>/README.md</code>), which should contain a user-friendly summary of what your package is all about. This can be whatever you want. Feel free to be creative!</p>"},{"location":"example_docs/setup/basics.html#license","title":"License","text":"<p>The augmented_partition repository comes premade with a sample license (<code>/LICENSE.md</code>), in this case the very popular and permissive BSD 3-Clause license. Feel free to change this for your own project or keep it as-is if you don't quite know yet.</p> <p>Tip</p> <p>There are many licenses that one can consider. A comprehensive list can be found on the Open Source Initiative website, but a less overwhelming route is to use choosealicense.com.</p>"},{"location":"example_docs/setup/basics.html#code-of-conduct","title":"Code of Conduct","text":"<p>The augmented_partition repository ships with a premade Code of Conduct (<code>/CODE_OF_CONDUCT.md</code>) that is obtained from the Contributor Covenant. Of course, you can feel free to keep or change this as you see fit, but it is often a good idea to have a code of conduct for public repositories.</p>"},{"location":"example_docs/setup/name.html","title":"Updating the Name","text":"<p>Now for your first major task: replace all instances of the word \"augmented_partition\" with your desired package name.</p> <p>Note</p> <p>Don't forget to update the name of the <code>/src/augmented_partition</code> folder, e.g. so that it is of the form <code>src/&lt;MyPackageName&gt;</code>.</p> <p>Tip</p> <p>If you're using Visual Studio Code as your editor, you can do <code>ctrl+shift+H</code> to find-and-replace all instances of \"augmented_partition\" with your own package name.</p> <p></p>"},{"location":"example_docs/setup/prep.html","title":"Preparatory Steps","text":""},{"location":"example_docs/setup/prep.html#naming-your-package","title":"Naming Your Package","text":"<p>So, you have an idea for your own Python package. The first thing you'll need to do is come up with a name!</p> <p>Tip</p> <p>If you plan on making a Python package that is widely distributed, first check to see if the name already exists on PyPI.</p>"},{"location":"example_docs/setup/prep.html#making-a-repository","title":"Making a Repository","text":"<p>With a nice name in mind, create a new repository using this augmented_partition. Give it a name, a description, and decide if you want it to be public or private.</p> <p></p>"},{"location":"example_docs/setup/prep.html#cloning-your-repository","title":"Cloning Your Repository","text":"<p>You'll now want to clone the repository to your local machine so you can easily make changes.</p>"},{"location":"example_docs/setup/prep.html#via-a-desktop-client","title":"Via a Desktop Client","text":"<p>You can use a desktop client to interface with GitHub. It is worthwhile to learn how to use such a program for your day-to-day work.</p> <p>Tip</p> <p>We strongly suggest using GitKraken to interface with git and GitHub. GitKraken Pro is also free for students.</p> <p></p>"},{"location":"example_docs/setup/prep.html#via-the-command-line","title":"Via the Command Line","text":"<p>If you prefer, you can clone the repository via the following command in the command-line, provided you have git installed.</p> <pre><code>git clone https://github.com/MyAccountName/MyPackageName\n</code></pre> <p>You can get the URL directly from the GitHub page when you click the green \"&lt;&gt; Code\" button.</p>"},{"location":"reference/SUMMARY.html","title":"SUMMARY","text":"<ul> <li>augmented_partition<ul> <li>examples<ul> <li>sample</li> </ul> </li> <li>lib_equiformer<ul> <li>SO2_operations</li> <li>SO3</li> <li>activation</li> <li>layer_norm</li> <li>process_irreps</li> <li>radial_function</li> <li>wigner</li> </ul> </li> <li>model<ul> <li>compute_env</li> <li>data</li> <li>network</li> <li>structure</li> <li>training</li> <li>transformer_block</li> <li>utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/augmented_partition/examples/sample.html","title":"sample","text":""},{"location":"reference/augmented_partition/examples/sample.html#augmented_partition.examples.sample.add","title":"add","text":"<pre><code>add(a: float, b: float) -&gt; float\n</code></pre> <p>A function that adds two numbers.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>float</code>)           \u2013            <p>First number to add.</p> </li> <li> <code>b</code>               (<code>float</code>)           \u2013            <p>Second number to add.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The sum of a and b.</p> </li> </ul> Source code in <code>augmented_partition/examples/sample.py</code> <pre><code>def add(a: float, b: float) -&gt; float:\n    \"\"\"\n    A function that adds two numbers.\n\n    Parameters\n    ----------\n    a\n        First number to add.\n    b\n        Second number to add.\n\n    Returns\n    -------\n    float\n        The sum of a and b.\n    \"\"\"\n    return a + b\n</code></pre>"},{"location":"reference/augmented_partition/examples/sample.html#augmented_partition.examples.sample.divide","title":"divide","text":"<pre><code>divide(a: float, b: float) -&gt; float\n</code></pre> <p>A function that divides two numbers, i.e. a/b.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>float</code>)           \u2013            <p>The numerator</p> </li> <li> <code>b</code>               (<code>float</code>)           \u2013            <p>The denominator</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The value for a/b</p> </li> </ul> Source code in <code>augmented_partition/examples/sample.py</code> <pre><code>def divide(a: float, b: float) -&gt; float:\n    \"\"\"\n    A function that divides two numbers, i.e. a/b.\n\n    Parameters\n    ----------\n    a\n        The numerator\n    b\n        The denominator\n\n    Returns\n    -------\n    float\n        The value for a/b\n    \"\"\"\n    if b == 0:\n        raise ValueError(\"Uh oh! The value for b should not be 0.\")\n\n    return a / b\n</code></pre>"},{"location":"reference/augmented_partition/examples/sample.html#augmented_partition.examples.sample.make_array","title":"make_array","text":"<pre><code>make_array(val: float, length: int = 3) -&gt; NDArray\n</code></pre> <p>A function to transform a number into a numpy array.</p> <p>Parameters:</p> <ul> <li> <code>val</code>               (<code>float</code>)           \u2013            <p>Number to turn into an array.</p> </li> <li> <code>length</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The length of the array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>An array composed of <code>val</code>.</p> </li> </ul> Source code in <code>augmented_partition/examples/sample.py</code> <pre><code>def make_array(val: float, length: int = 3) -&gt; NDArray:\n    \"\"\"\n    A function to transform a number into a numpy array.\n\n    Parameters\n    ----------\n    val\n        Number to turn into an array.\n    length\n        The length of the array.\n\n    Returns\n    -------\n    NDArray\n        An array composed of `val`.\n    \"\"\"\n    return np.array([val] * length)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html","title":"SO2_operations","text":""},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution","title":"SO2_Convolution","text":"<pre><code>SO2_Convolution(sphere_channels, m_output_channels, lmax, mmax, mappingReduced, internal_weights=True, edge_channels_list=None, extra_m0_output_channels=None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>SO(2) Block: Perform SO(2) convolutions for all m (orders)</p> <p>Args:     sphere_channels (int):      Number of spherical channels     m_output_channels (int):    Number of output channels used during the SO(2) conv     lmax (int):                 Max degree (l)     mmax (int):                 Max order (m)     mappingReduced (CoefficientMappingModule): Used to extract a subset of m components     internal_weights (bool):    If True, not using radial function to multiply inputs features     edge_channels_list (list:int):  List of sizes of invariant edge embedding. For example, [input_channels, hidden_channels, hidden_channels].     extra_m0_output_channels (int): If not None, return <code>out_embedding</code> (SO3_Embedding) and <code>extra_m0_features</code> (Tensor).</p> Source code in <code>augmented_partition/lib_equiformer/SO2_operations.py</code> <pre><code>def __init__(\n    self,\n    sphere_channels,\n    m_output_channels,\n    lmax,\n    mmax,\n    mappingReduced,\n    internal_weights=True,\n    edge_channels_list=None,\n    extra_m0_output_channels=None,\n):\n    super().__init__()\n\n    self.sphere_channels = sphere_channels\n    self.m_output_channels = m_output_channels\n    self.lmax = lmax\n    self.mmax = mmax\n    self.mappingReduced = mappingReduced\n    self.internal_weights = internal_weights\n    self.edge_channels_list = copy.deepcopy(edge_channels_list)\n    self.extra_m0_output_channels = extra_m0_output_channels\n\n    num_channels_rad = 0  # for radial function\n\n    num_channels_m0 = 0\n    num_coefficients = self.lmax + 1\n    num_channels_m0 = (\n        num_channels_m0 + num_coefficients * self.sphere_channels\n    )  # m = 0 input block, size of [(l_max + 1) * 3/1*sphere_channels]\n\n    # SO(2) convolution for m = 0\n    m0_output_channels = self.m_output_channels * (\n        num_channels_m0 // self.sphere_channels\n    )  # m = 0 output block, size of [(l_max + 1) * m_output_channels]\n    if self.extra_m0_output_channels is not None:\n        m0_output_channels = (\n            m0_output_channels + self.extra_m0_output_channels\n        )  # m = 0 output block, size of [(l_max + 1) * m_output_channels + extra_m0_output_channels]\n    self.fc_m0 = Linear(\n        num_channels_m0, m0_output_channels\n    )  # Linear layer for m = 0 output block, dims [(l_max + 1) * 3/1*sphere_channels] -&gt; [(l_max + 1) * m_output_channels]\n    num_channels_rad = (\n        num_channels_rad + self.fc_m0.in_features\n    )  # for radial function, size of [(l_max + 1) * 3/1*sphere_channels]\n\n    # SO(2) convolution for non-zero m\n    self.so2_m_conv = nn.ModuleList()\n    for m in range(1, self.mmax + 1):\n        self.so2_m_conv.append(\n            SO2_m_Convolution(\n                m,\n                self.sphere_channels,\n                self.m_output_channels,\n                self.lmax,\n                self.mmax,\n            )\n        )\n        num_channels_rad = num_channels_rad + self.so2_m_conv[-1].fc.in_features\n\n    # Embedding function of distance (this is done outside already)\n    self.rad_func = None  #\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.edge_channels_list","title":"edge_channels_list  <code>instance-attribute</code>","text":"<pre><code>edge_channels_list = deepcopy(edge_channels_list)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.extra_m0_output_channels","title":"extra_m0_output_channels  <code>instance-attribute</code>","text":"<pre><code>extra_m0_output_channels = extra_m0_output_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.fc_m0","title":"fc_m0  <code>instance-attribute</code>","text":"<pre><code>fc_m0 = Linear(num_channels_m0, m0_output_channels)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.internal_weights","title":"internal_weights  <code>instance-attribute</code>","text":"<pre><code>internal_weights = internal_weights\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.m_output_channels","title":"m_output_channels  <code>instance-attribute</code>","text":"<pre><code>m_output_channels = m_output_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.mappingReduced","title":"mappingReduced  <code>instance-attribute</code>","text":"<pre><code>mappingReduced = mappingReduced\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.rad_func","title":"rad_func  <code>instance-attribute</code>","text":"<pre><code>rad_func = None\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.so2_m_conv","title":"so2_m_conv  <code>instance-attribute</code>","text":"<pre><code>so2_m_conv = ModuleList()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.sphere_channels","title":"sphere_channels  <code>instance-attribute</code>","text":"<pre><code>sphere_channels = sphere_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_Convolution.forward","title":"forward","text":"<pre><code>forward(x, x_edge)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO2_operations.py</code> <pre><code>def forward(self, x, x_edge):\n    num_edges = len(x_edge)\n    out = []\n\n    # Reshape the spherical harmonics based on m (order)\n    x._m_primary(self.mappingReduced)\n\n    # radial function\n    if self.rad_func is not None:\n        x_edge = self.rad_func(x_edge)\n    offset_rad = 0\n\n    # Compute m=0 coefficients separately since they only have real values (no imaginary)\n    x_0 = x.embedding.narrow(\n        1, 0, self.mappingReduced.m_size[0]\n    )  # m=0 coefficient block of the embeddings, shape [num_edges, (l_max + 1), 3/1*sphere_channels]\n    x_0 = x_0.reshape(\n        num_edges, -1\n    )  # reshape the m=0 coefficients, shape [num_edges, (l_max + 1)*3/1*sphere_channels]\n    x_0 = self.fc_m0(x_0)  # apply linear layer to the m=0 coefficients\n\n    x_0_extra = None\n    # extract extra m0 features\n    if self.extra_m0_output_channels is not None:\n        x_0_extra = x_0.narrow(-1, 0, self.extra_m0_output_channels)\n        x_0 = x_0.narrow(\n            -1,\n            self.extra_m0_output_channels,\n            (self.fc_m0.out_features - self.extra_m0_output_channels),\n        )\n\n    x_0 = x_0.view(num_edges, -1, self.m_output_channels)\n    # x.embedding[:, 0 : self.mappingReduced.m_size[0]] = x_0\n    out.append(x_0)\n    offset_rad = offset_rad + self.fc_m0.in_features\n\n    # Compute the values for the m &gt; 0 coefficients\n    offset = self.mappingReduced.m_size[0]\n    for m in range(1, self.mmax + 1):\n        # Get the m order coefficients\n        x_m = x.embedding.narrow(\n            1, offset, 2 * self.mappingReduced.m_size[m]\n        )  # size of the m-th block of the embeddings, shape [num_edges, 2*(l_max - m + 1), 3/1*sphere_channels]\n        x_m = x_m.reshape(num_edges, 2, -1)\n\n        # Perform SO(2) convolution\n        if self.rad_func is not None:\n            x_edge_m = x_edge.narrow(\n                1, offset_rad, self.so2_m_conv[m - 1].fc.in_features\n            )\n            x_edge_m = x_edge_m.reshape(\n                num_edges, 1, self.so2_m_conv[m - 1].fc.in_features\n            )\n            x_m = x_m * x_edge_m\n        x_m = self.so2_m_conv[m - 1](x_m)\n        x_m = x_m.view(num_edges, -1, self.m_output_channels)\n        # x.embedding[:, offset : offset + 2 * self.mappingReduced.m_size[m]] = x_m\n        out.append(x_m)\n        offset = offset + 2 * self.mappingReduced.m_size[m]\n        offset_rad = offset_rad + self.so2_m_conv[m - 1].fc.in_features\n\n    out = torch.cat(out, dim=1)\n    out_embedding = SO3_Embedding(\n        0, x.lmax, self.m_output_channels, device=x.device, dtype=x.dtype\n    )\n    out_embedding.set_embedding(out)\n    out_embedding.set_lmax_mmax([self.lmax], [self.mmax])\n\n    # Reshape the spherical harmonics based on l (degree)\n    out_embedding._l_primary(self.mappingReduced)\n\n    if self.extra_m0_output_channels is not None:\n        return out_embedding, x_0_extra\n    else:\n        return out_embedding\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_m_Convolution","title":"SO2_m_Convolution","text":"<pre><code>SO2_m_Convolution(m, sphere_channels, m_output_channels, lmax, mmax)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>SO(2) Conv: Perform an SO(2) convolution on features corresponding to +- m</p> <p>Args:     m (int):                    Order of the spherical harmonic coefficients     sphere_channels (int):      Number of spherical channels     m_output_channels (int):    Number of output channels used during the SO(2) conv     lmax (int):                 Max degree (l)     mmax (int):                 Max order (m)</p> Source code in <code>augmented_partition/lib_equiformer/SO2_operations.py</code> <pre><code>def __init__(self, m, sphere_channels, m_output_channels, lmax, mmax):\n    super().__init__()\n\n    self.m = m\n    self.sphere_channels = sphere_channels\n    self.m_output_channels = m_output_channels\n    self.lmax = lmax\n    self.mmax = mmax\n\n    num_channels = 0\n    num_coefficents = 0\n\n    if self.mmax &gt;= self.m:\n        num_coefficents = self.lmax - self.m + 1\n\n    num_channels = num_channels + num_coefficents * self.sphere_channels\n\n    assert num_channels &gt; 0\n\n    self.fc = Linear(\n        num_channels,\n        2 * self.m_output_channels * (num_channels // self.sphere_channels),\n        bias=False,\n    )\n    self.fc.weight.data.mul_(1 / math.sqrt(2))\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_m_Convolution.fc","title":"fc  <code>instance-attribute</code>","text":"<pre><code>fc = Linear(num_channels, 2 * m_output_channels * num_channels // sphere_channels, bias=False)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_m_Convolution.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_m_Convolution.m","title":"m  <code>instance-attribute</code>","text":"<pre><code>m = m\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_m_Convolution.m_output_channels","title":"m_output_channels  <code>instance-attribute</code>","text":"<pre><code>m_output_channels = m_output_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_m_Convolution.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_m_Convolution.sphere_channels","title":"sphere_channels  <code>instance-attribute</code>","text":"<pre><code>sphere_channels = sphere_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO2_operations.html#augmented_partition.lib_equiformer.SO2_operations.SO2_m_Convolution.forward","title":"forward","text":"<pre><code>forward(x_m)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO2_operations.py</code> <pre><code>def forward(self, x_m):\n    x_m = self.fc(x_m)\n    x_r = x_m.narrow(2, 0, self.fc.out_features // 2)\n    x_i = x_m.narrow(2, self.fc.out_features // 2, self.fc.out_features // 2)\n    x_m_r = x_r.narrow(1, 0, 1) - x_i.narrow(1, 1, 1)  # x_r[:, 0] - x_i[:, 1]\n    x_m_i = x_r.narrow(1, 1, 1) + x_i.narrow(1, 0, 1)  # x_r[:, 1] + x_i[:, 0]\n    return torch.cat((x_m_r, x_m_i), dim=1)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html","title":"SO3","text":"<p>Copyright \u00a9 Facebook, Inc. and its affiliates.</p> <p>This source code is licensed under the MIT license found in the LICENSE file in the root directory of this source tree.</p> <p>TODO:     1. Simplify the case when <code>num_resolutions</code> == 1.     2. Remove indexing when the shape is the same.     3. Move some functions outside classes and to separate files.</p>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.CoefficientMappingModule","title":"CoefficientMappingModule","text":"<pre><code>CoefficientMappingModule(lmax, mmax)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Helper module for coefficients used to reshape l \u2194 m and to get coefficients of specific degree or order</p> <p>Args:     lmax (int):   Maximum degree of the spherical harmonics     mmax (int):   Maximum order of the spherical harmonics</p> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def __init__(self, lmax, mmax):\n    super().__init__()\n\n    self.lmax = lmax\n    self.mmax = mmax\n\n    # Temporarily use `cpu` as device and this will be overwritten.\n    self.device = \"cpu\"\n\n    # Compute the degree (l) and order (m) for each entry of the embedding\n    l_harmonic = torch.tensor([], device=self.device).long()\n    m_harmonic = torch.tensor([], device=self.device).long()\n    m_complex = torch.tensor([], device=self.device).long()\n\n    res_size = torch.zeros(\n        [1], device=self.device\n    ).long()  # 1 used to be `num_resolutions`\n\n    offset = 0\n    for l_idx in range(self.lmax + 1):\n        mmax = min(self.mmax, l_idx)\n        m = torch.arange(-mmax, mmax + 1, device=self.device).long()\n        m_complex = torch.cat([m_complex, m], dim=0)\n        m_harmonic = torch.cat([m_harmonic, torch.abs(m).long()], dim=0)\n        l_harmonic = torch.cat([l_harmonic, m.fill_(l_idx).long()], dim=0)\n    res_size[0] = len(l_harmonic) - offset\n    offset = len(l_harmonic)\n\n    num_coefficients = len(l_harmonic)\n    # `self.to_m` moves m components from different L to contiguous index\n    to_m = torch.zeros([num_coefficients, num_coefficients], device=self.device)\n    m_size = torch.zeros([self.mmax + 1], device=self.device).long()\n\n    # The following is implemented poorly - very slow. It only gets called\n    # a few times so haven't optimized.\n    offset = 0\n    for m in range(self.mmax + 1):\n        idx_r, idx_i = self.complex_idx(m, -1, m_complex, l_harmonic)\n\n        for idx_out, idx_in in enumerate(idx_r):\n            to_m[idx_out + offset, idx_in] = 1.0\n        offset = offset + len(idx_r)\n\n        m_size[m] = int(len(idx_r))\n\n        for idx_out, idx_in in enumerate(idx_i):\n            to_m[idx_out + offset, idx_in] = 1.0\n        offset = offset + len(idx_i)\n\n    to_m = to_m.detach()\n\n    # save tensors and they will be moved to GPU\n    self.register_buffer(\"l_harmonic\", l_harmonic)\n    self.register_buffer(\"m_harmonic\", m_harmonic)\n    self.register_buffer(\"m_complex\", m_complex)\n    self.register_buffer(\"res_size\", res_size)\n    self.register_buffer(\"to_m\", to_m)\n    self.register_buffer(\"m_size\", m_size)\n\n    # for caching the output of `coefficient_idx`\n    self.lmax_cache, self.mmax_cache = None, None\n    self.mask_indices_cache = None\n    self.rotate_inv_rescale_cache = None\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.CoefficientMappingModule.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = 'cpu'\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.CoefficientMappingModule.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.CoefficientMappingModule.mask_indices_cache","title":"mask_indices_cache  <code>instance-attribute</code>","text":"<pre><code>mask_indices_cache = None\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.CoefficientMappingModule.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.CoefficientMappingModule.rotate_inv_rescale_cache","title":"rotate_inv_rescale_cache  <code>instance-attribute</code>","text":"<pre><code>rotate_inv_rescale_cache = None\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.CoefficientMappingModule.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def __repr__(self):\n    return f\"{self.__class__.__name__}(lmax={self.lmax}, mmax={self.mmax})\"\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.CoefficientMappingModule.coefficient_idx","title":"coefficient_idx","text":"<pre><code>coefficient_idx(lmax, mmax)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def coefficient_idx(self, lmax, mmax):\n    if (\n        self.lmax_cache is not None\n        and self.mmax_cache is not None\n        and self.lmax_cache == lmax\n        and self.mmax_cache == mmax\n        and self.mask_indices_cache is not None\n    ):\n        return self.mask_indices_cache\n\n    mask = torch.bitwise_and(self.l_harmonic.le(lmax), self.m_harmonic.le(mmax))\n    self.device = mask.device\n    indices = torch.arange(len(mask), device=self.device)\n    mask_indices = torch.masked_select(indices, mask)\n    self.lmax_cache, self.mmax_cache = lmax, mmax\n    self.mask_indices_cache = mask_indices\n    return self.mask_indices_cache\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.CoefficientMappingModule.complex_idx","title":"complex_idx","text":"<pre><code>complex_idx(m, lmax, m_complex, l_harmonic)\n</code></pre> <p>Add <code>m_complex</code> and <code>l_harmonic</code> to the input arguments since we cannot use <code>self.m_complex</code>.</p> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def complex_idx(self, m, lmax, m_complex, l_harmonic):\n    \"\"\"\n    Add `m_complex` and `l_harmonic` to the input arguments\n    since we cannot use `self.m_complex`.\n    \"\"\"\n    if lmax == -1:\n        lmax = self.lmax\n\n    indices = torch.arange(len(l_harmonic), device=self.device)\n    # Real part\n    mask_r = torch.bitwise_and(l_harmonic.le(lmax), m_complex.eq(m))\n    mask_idx_r = torch.masked_select(indices, mask_r)\n\n    mask_idx_i = torch.tensor([], device=self.device).long()\n    # Imaginary part\n    if m != 0:\n        mask_i = torch.bitwise_and(l_harmonic.le(lmax), m_complex.eq(-m))\n        mask_idx_i = torch.masked_select(indices, mask_i)\n\n    return mask_idx_r, mask_idx_i\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.CoefficientMappingModule.get_rotate_inv_rescale","title":"get_rotate_inv_rescale","text":"<pre><code>get_rotate_inv_rescale(lmax, mmax)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def get_rotate_inv_rescale(self, lmax, mmax):\n    if (\n        self.lmax_cache is not None\n        and self.mmax_cache is not None\n        and self.lmax_cache == lmax\n        and self.mmax_cache == mmax\n        and self.rotate_inv_rescale_cache is not None\n    ):\n        return self.rotate_inv_rescale_cache\n\n    if self.mask_indices_cache is None:\n        self.coefficient_idx(lmax, mmax)\n\n    rotate_inv_rescale = torch.ones(\n        (1, (lmax + 1) ** 2, (lmax + 1) ** 2), device=self.device\n    )\n    for l_idx in range(lmax + 1):\n        if l_idx &lt;= mmax:\n            continue\n        start_idx = l_idx**2\n        length = 2 * l_idx + 1\n        rescale_factor = math.sqrt(length / (2 * mmax + 1))\n        rotate_inv_rescale[\n            :, start_idx : (start_idx + length), start_idx : (start_idx + length)\n        ] = rescale_factor\n    rotate_inv_rescale = rotate_inv_rescale[:, :, self.mask_indices_cache]\n    self.rotate_inv_rescale_cache = rotate_inv_rescale\n    return self.rotate_inv_rescale_cache\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Embedding","title":"SO3_Embedding","text":"<pre><code>SO3_Embedding(length, lmax, num_channels, device, dtype)\n</code></pre> <p>Helper functions for performing operations on irreps embedding</p> <p>Args:     length (int):           Batch size     lmax   (int):           Maximum degree of the spherical harmonics     num_channels (int):     Number of channels     device:                 Device of the output     dtype:                  type of the output tensors</p> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def __init__(self, length, lmax, num_channels, device, dtype):\n    super().__init__()\n\n    self.lmax = lmax\n\n    self.num_channels = num_channels\n    self.device = device\n    self.dtype = dtype\n\n    self.num_coefficients = 0\n    self.num_coefficients = self.num_coefficients + int((self.lmax + 1) ** 2)\n\n    embedding = torch.zeros(\n        length,\n        self.num_coefficients,\n        self.num_channels,\n        device=self.device,\n        dtype=self.dtype,\n    )\n\n    self.set_embedding(embedding)\n    self.set_lmax_mmax(self.lmax, self.lmax)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Embedding.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Embedding.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Embedding.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Embedding.num_channels","title":"num_channels  <code>instance-attribute</code>","text":"<pre><code>num_channels = num_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Embedding.num_coefficients","title":"num_coefficients  <code>instance-attribute</code>","text":"<pre><code>num_coefficients = num_coefficients + int(lmax + 1 ** 2)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Embedding.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def clone(self):\n    clone = SO3_Embedding(0, self.lmax, self.num_channels, self.device, self.dtype)\n    clone.set_embedding(self.embedding.clone())\n    return clone\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Embedding.expand_edge","title":"expand_edge","text":"<pre><code>expand_edge(edge_index)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def expand_edge(self, edge_index):\n    x_expand = SO3_Embedding(\n        0, self.lmax, self.num_channels, self.device, self.dtype\n    )\n    x_expand.set_embedding(self.embedding[edge_index])\n    return x_expand\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Embedding.set_embedding","title":"set_embedding","text":"<pre><code>set_embedding(embedding)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def set_embedding(self, embedding):\n    self.length = len(embedding)\n    self.embedding = embedding\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Embedding.set_lmax_mmax","title":"set_lmax_mmax","text":"<pre><code>set_lmax_mmax(lmax, mmax)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def set_lmax_mmax(self, lmax, mmax):\n    # if its a list: # TODO: check if this is correct\n    if isinstance(lmax, list):\n        self.lmax = lmax[0]\n        self.mmax = mmax[0]\n    else:\n        self.lmax = lmax\n        self.mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_LinearV2","title":"SO3_LinearV2","text":"<pre><code>SO3_LinearV2(in_features, out_features, lmax)\n</code></pre> <p>               Bases: <code>Module</code></p> <ol> <li>Applies bias to scalar features only</li> </ol> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def __init__(self, in_features, out_features, lmax):  # , bias=True):\n    \"\"\"\n    1. Use `torch.einsum` to prevent slicing and concatenation\n    2. Need to specify some behaviors in `no_weight_decay` and weight initialization.\n    3. Applies bias to scalar features only\n    \"\"\"\n    super().__init__()\n    self.in_features = in_features\n    self.out_features = out_features\n    self.lmax = lmax\n\n    self.weight = torch.nn.Parameter(\n        torch.randn((self.lmax + 1), out_features, in_features)\n    )\n    bound = 1 / math.sqrt(self.in_features)\n    torch.nn.init.uniform_(self.weight, -bound, bound)\n    self.bias = torch.nn.Parameter(torch.zeros(out_features))\n\n    expand_index = torch.zeros([(lmax + 1) ** 2]).long()\n    for l_idx in range(lmax + 1):\n        start_idx = l_idx**2\n        length = 2 * l_idx + 1\n        expand_index[start_idx : (start_idx + length)] = l_idx\n    self.register_buffer(\"expand_index\", expand_index)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_LinearV2.bias","title":"bias  <code>instance-attribute</code>","text":"<pre><code>bias = Parameter(zeros(out_features))\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_LinearV2.in_features","title":"in_features  <code>instance-attribute</code>","text":"<pre><code>in_features = in_features\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_LinearV2.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_LinearV2.out_features","title":"out_features  <code>instance-attribute</code>","text":"<pre><code>out_features = out_features\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_LinearV2.weight","title":"weight  <code>instance-attribute</code>","text":"<pre><code>weight = Parameter(randn(lmax + 1, out_features, in_features))\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_LinearV2.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def __repr__(self):\n    return f\"{self.__class__.__name__}(in_features={self.in_features}, out_features={self.out_features}, lmax={self.lmax})\"\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_LinearV2.forward","title":"forward","text":"<pre><code>forward(input_embedding)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def forward(self, input_embedding):\n    weight = torch.index_select(\n        self.weight, dim=0, index=self.expand_index\n    )  # [(L_max + 1) ** 2, C_out, C_in]\n    out = torch.einsum(\n        \"bmi, moi -&gt; bmo\", input_embedding.embedding, weight\n    )  # [N, (L_max + 1) ** 2, C_out]\n    bias = self.bias.view(1, 1, self.out_features)\n    out[:, 0:1, :] = out.narrow(1, 0, 1) + bias  # add bias to scalar features\n\n    out_embedding = SO3_Embedding(\n        0,\n        input_embedding.lmax,\n        self.out_features,\n        device=input_embedding.device,\n        dtype=input_embedding.dtype,\n    )\n    out_embedding.set_embedding(out)\n    out_embedding.set_lmax_mmax(input_embedding.lmax, input_embedding.lmax)\n\n    return out_embedding\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Rotation","title":"SO3_Rotation","text":"<pre><code>SO3_Rotation(lmax)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Helper functions for Wigner-D rotations</p> <p>Args:     lmax (int):   Maximum degree of the spherical harmonics</p> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def __init__(self, lmax):\n    super().__init__()\n    self.lmax = lmax\n    self.mapping = CoefficientMappingModule(self.lmax, self.lmax)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Rotation.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Rotation.mapping","title":"mapping  <code>instance-attribute</code>","text":"<pre><code>mapping = CoefficientMappingModule(lmax, lmax)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Rotation.RotationToWignerDMatrix","title":"RotationToWignerDMatrix","text":"<pre><code>RotationToWignerDMatrix(edge_rot_mat, start_lmax, end_lmax)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def RotationToWignerDMatrix(self, edge_rot_mat, start_lmax, end_lmax):\n    x = edge_rot_mat @ edge_rot_mat.new_tensor([0.0, 1.0, 0.0])\n    alpha, beta = o3.xyz_to_angles(x)\n    R = (\n        o3.angles_to_matrix(alpha, beta, torch.zeros_like(alpha)).transpose(-1, -2)\n        @ edge_rot_mat\n    )\n    gamma = torch.atan2(R[..., 0, 2], R[..., 0, 0])\n\n    size = (end_lmax + 1) ** 2 - (start_lmax) ** 2\n    wigner = torch.zeros(len(alpha), size, size, device=self.device)\n    start = 0\n    for lmax in range(start_lmax, end_lmax + 1):\n        block = wigner_D(lmax, alpha, beta, gamma)\n        end = start + block.size()[1]\n        wigner[:, start:end, start:end] = block\n        start = end\n\n    return wigner.detach()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Rotation.rotate","title":"rotate","text":"<pre><code>rotate(embedding, out_lmax, out_mmax)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def rotate(self, embedding, out_lmax, out_mmax):\n    out_mask = self.mapping.coefficient_idx(out_lmax, out_mmax)\n    wigner = self.wigner[:, out_mask, :]\n    return torch.bmm(wigner, embedding)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Rotation.rotate_inv","title":"rotate_inv","text":"<pre><code>rotate_inv(embedding, in_lmax, in_mmax)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def rotate_inv(self, embedding, in_lmax, in_mmax):\n    in_mask = self.mapping.coefficient_idx(in_lmax, in_mmax)\n    wigner_inv = self.wigner_inv[:, :, in_mask]\n    wigner_inv_rescale = self.mapping.get_rotate_inv_rescale(in_lmax, in_mmax)\n    wigner_inv = wigner_inv * wigner_inv_rescale\n    return torch.bmm(wigner_inv, embedding)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/SO3.html#augmented_partition.lib_equiformer.SO3.SO3_Rotation.set_wigner","title":"set_wigner","text":"<pre><code>set_wigner(rot_mat3x3)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/SO3.py</code> <pre><code>def set_wigner(self, rot_mat3x3):\n    self.device, self.dtype = rot_mat3x3.device, rot_mat3x3.dtype\n    # length = len(rot_mat3x3)\n    self.wigner = self.RotationToWignerDMatrix(rot_mat3x3, 0, self.lmax)\n    self.wigner_inv = torch.transpose(self.wigner, 1, 2).contiguous()\n    self.wigner = self.wigner.detach()\n    self.wigner_inv = self.wigner_inv.detach()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html","title":"activation","text":""},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.GateActivation","title":"GateActivation","text":"<pre><code>GateActivation(lmax, mmax, num_channels)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def __init__(self, lmax, mmax, num_channels):\n    super().__init__()\n\n    self.lmax = lmax\n    self.mmax = mmax\n    self.num_channels = num_channels\n\n    # compute `expand_index` based on `lmax` and `mmax`\n    num_components = 0\n    for l_dx in range(1, self.lmax + 1):\n        num_m_components = min((2 * l_dx + 1), (2 * self.mmax + 1))\n        num_components = num_components + num_m_components\n    expand_index = torch.zeros([num_components]).long()\n    start_idx = 0\n    for l_dx in range(1, self.lmax + 1):\n        length = min((2 * l_dx + 1), (2 * self.mmax + 1))\n        expand_index[start_idx : (start_idx + length)] = l_dx - 1\n        start_idx = start_idx + length\n    self.register_buffer(\n        \"expand_index\", expand_index\n    )  # for lmax=4, tensor([0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n\n    self.scalar_act = (\n        torch.nn.SiLU()\n    )  # SwiGLU(self.num_channels, self.num_channels)  # #\n    self.gate_act = torch.nn.Sigmoid()  # torch.nn.SiLU() # #\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.GateActivation.gate_act","title":"gate_act  <code>instance-attribute</code>","text":"<pre><code>gate_act = Sigmoid()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.GateActivation.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.GateActivation.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.GateActivation.num_channels","title":"num_channels  <code>instance-attribute</code>","text":"<pre><code>num_channels = num_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.GateActivation.scalar_act","title":"scalar_act  <code>instance-attribute</code>","text":"<pre><code>scalar_act = SiLU()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.GateActivation.forward","title":"forward","text":"<pre><code>forward(gating_scalars, input_tensors)\n</code></pre> <p><code>gating_scalars</code>: shape [N, lmax * num_channels] <code>input_tensors</code>: shape  [N, (lmax + 1) ** 2, num_channels]</p> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def forward(self, gating_scalars, input_tensors):\n    \"\"\"\n    `gating_scalars`: shape [N, lmax * num_channels]\n    `input_tensors`: shape  [N, (lmax + 1) ** 2, num_channels]\n    \"\"\"\n\n    gating_scalars = self.gate_act(gating_scalars)\n    gating_scalars = gating_scalars.reshape(\n        gating_scalars.shape[0], self.lmax, self.num_channels\n    )\n    gating_scalars = torch.index_select(\n        gating_scalars, dim=1, index=self.expand_index\n    )\n\n    input_tensors_scalars = input_tensors.narrow(1, 0, 1)\n    input_tensors_scalars = self.scalar_act(input_tensors_scalars)\n\n    input_tensors_vectors = input_tensors.narrow(1, 1, input_tensors.shape[1] - 1)\n    input_tensors_vectors = input_tensors_vectors * gating_scalars\n\n    return torch.cat((input_tensors_scalars, input_tensors_vectors), dim=1)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.S2Activation","title":"S2Activation","text":"<pre><code>S2Activation(lmax, mmax)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Assume we only have one resolution</p> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def __init__(self, lmax, mmax):\n    super().__init__()\n    self.lmax = lmax\n    self.mmax = mmax\n    self.act = torch.nn.SiLU()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.S2Activation.act","title":"act  <code>instance-attribute</code>","text":"<pre><code>act = SiLU()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.S2Activation.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.S2Activation.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.S2Activation.forward","title":"forward","text":"<pre><code>forward(inputs, SO3_grid)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def forward(self, inputs, SO3_grid):\n    to_grid_mat = SO3_grid[self.lmax][self.mmax].get_to_grid_mat(\n        device=None\n    )  # `device` is not used\n    from_grid_mat = SO3_grid[self.lmax][self.mmax].get_from_grid_mat(device=None)\n    x_grid = torch.einsum(\"bai, zic -&gt; zbac\", to_grid_mat, inputs)\n    x_grid = self.act(x_grid)\n    return torch.einsum(\"bai, zbac -&gt; zic\", from_grid_mat, x_grid)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSiLU","title":"ScaledSiLU","text":"<pre><code>ScaledSiLU(inplace=False)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def __init__(self, inplace=False):\n    super().__init__()\n    self.inplace = inplace\n    self.scale_factor = 1.6791767923989418\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSiLU.inplace","title":"inplace  <code>instance-attribute</code>","text":"<pre><code>inplace = inplace\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSiLU.scale_factor","title":"scale_factor  <code>instance-attribute</code>","text":"<pre><code>scale_factor = 1.6791767923989418\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSiLU.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def extra_repr(self):\n    str_s = f\"scale_factor={self.scale_factor}\"\n    if self.inplace:\n        str_s = str_s + \", inplace=True\"\n    return str_s\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSiLU.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def forward(self, inputs):\n    return F.silu(inputs, inplace=self.inplace) * self.scale_factor\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSigmoid","title":"ScaledSigmoid","text":"<pre><code>ScaledSigmoid()\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.scale_factor = 1.8467055342154763\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSigmoid.scale_factor","title":"scale_factor  <code>instance-attribute</code>","text":"<pre><code>scale_factor = 1.8467055342154763\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSigmoid.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def forward(self, x):\n    return torch.sigmoid(x) * self.scale_factor\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSmoothLeakyReLU","title":"ScaledSmoothLeakyReLU","text":"<pre><code>ScaledSmoothLeakyReLU()\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.act = SmoothLeakyReLU(0.2)\n    self.scale_factor = 1.531320475574866\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSmoothLeakyReLU.act","title":"act  <code>instance-attribute</code>","text":"<pre><code>act = SmoothLeakyReLU(0.2)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSmoothLeakyReLU.scale_factor","title":"scale_factor  <code>instance-attribute</code>","text":"<pre><code>scale_factor = 1.531320475574866\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSmoothLeakyReLU.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def extra_repr(self):\n    return f\"negative_slope={self.act.alpha}, scale_factor={self.scale_factor}\"\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSmoothLeakyReLU.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def forward(self, x):\n    return self.act(x) * self.scale_factor\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSwiGLU","title":"ScaledSwiGLU","text":"<pre><code>ScaledSwiGLU(in_channels, out_channels, bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def __init__(self, in_channels, out_channels, bias=True):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.w = torch.nn.Linear(in_channels, 2 * out_channels, bias=bias)\n    self.act = ScaledSiLU()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSwiGLU.act","title":"act  <code>instance-attribute</code>","text":"<pre><code>act = ScaledSiLU()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSwiGLU.in_channels","title":"in_channels  <code>instance-attribute</code>","text":"<pre><code>in_channels = in_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSwiGLU.out_channels","title":"out_channels  <code>instance-attribute</code>","text":"<pre><code>out_channels = out_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSwiGLU.w","title":"w  <code>instance-attribute</code>","text":"<pre><code>w = Linear(in_channels, 2 * out_channels, bias=bias)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.ScaledSwiGLU.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def forward(self, inputs):\n    w = self.w(inputs)\n    w_1 = w.narrow(-1, 0, self.out_channels)\n    w_1 = self.act(w_1)\n    w_2 = w.narrow(-1, self.out_channels, self.out_channels)\n    return w_1 * w_2\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SeparableS2Activation","title":"SeparableS2Activation","text":"<pre><code>SeparableS2Activation(lmax, mmax)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def __init__(self, lmax, mmax):\n    super().__init__()\n\n    self.lmax = lmax\n    self.mmax = mmax\n\n    self.scalar_act = torch.nn.SiLU()\n    self.s2_act = S2Activation(self.lmax, self.mmax)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SeparableS2Activation.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SeparableS2Activation.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SeparableS2Activation.s2_act","title":"s2_act  <code>instance-attribute</code>","text":"<pre><code>s2_act = S2Activation(lmax, mmax)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SeparableS2Activation.scalar_act","title":"scalar_act  <code>instance-attribute</code>","text":"<pre><code>scalar_act = SiLU()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SeparableS2Activation.forward","title":"forward","text":"<pre><code>forward(input_scalars, input_tensors, SO3_grid)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def forward(self, input_scalars, input_tensors, SO3_grid):\n    output_scalars = self.scalar_act(input_scalars)\n    output_scalars = output_scalars.reshape(\n        output_scalars.shape[0], 1, output_scalars.shape[-1]\n    )\n    output_tensors = self.s2_act(input_tensors, SO3_grid)\n    return torch.cat(\n        (output_scalars, output_tensors.narrow(1, 1, output_tensors.shape[1] - 1)),\n        dim=1,\n    )\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SmoothLeakyReLU","title":"SmoothLeakyReLU","text":"<pre><code>SmoothLeakyReLU(negative_slope=0.2)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def __init__(self, negative_slope=0.2):\n    super().__init__()\n    self.alpha = negative_slope\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SmoothLeakyReLU.alpha","title":"alpha  <code>instance-attribute</code>","text":"<pre><code>alpha = negative_slope\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SmoothLeakyReLU.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def extra_repr(self):\n    return f\"negative_slope={self.alpha}\"\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SmoothLeakyReLU.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def forward(self, x):\n    x1 = ((1 + self.alpha) / 2) * x\n    x2 = ((1 - self.alpha) / 2) * x * (2 * torch.sigmoid(x) - 1)\n    return x1 + x2\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SwiGLU","title":"SwiGLU","text":"<pre><code>SwiGLU(in_channels, out_channels, bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def __init__(self, in_channels, out_channels, bias=True):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.w = torch.nn.Linear(in_channels, 2 * out_channels, bias=bias)\n    self.act = torch.nn.SiLU()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SwiGLU.act","title":"act  <code>instance-attribute</code>","text":"<pre><code>act = SiLU()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SwiGLU.in_channels","title":"in_channels  <code>instance-attribute</code>","text":"<pre><code>in_channels = in_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SwiGLU.out_channels","title":"out_channels  <code>instance-attribute</code>","text":"<pre><code>out_channels = out_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SwiGLU.w","title":"w  <code>instance-attribute</code>","text":"<pre><code>w = Linear(in_channels, 2 * out_channels, bias=bias)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/activation.html#augmented_partition.lib_equiformer.activation.SwiGLU.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/activation.py</code> <pre><code>def forward(self, inputs):\n    w = self.w(inputs)\n    w_1 = w.narrow(-1, 0, self.out_channels)\n    w_1 = self.act(w_1)\n    w_2 = w.narrow(-1, self.out_channels, self.out_channels)\n    return w_1 * w_2\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html","title":"layer_norm","text":"<ol> <li> <p>Normalize features of shape (N, sphere_basis, C), with sphere_basis = (lmax + 1) ** 2.</p> </li> <li> <p>The difference from <code>layer_norm.py</code> is that all type-L vectors have the same number of channels and input features are of shape (N, sphere_basis, C).</p> </li> </ol>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantDegreeLayerScale","title":"EquivariantDegreeLayerScale","text":"<pre><code>EquivariantDegreeLayerScale(lmax, num_channels, scale_factor=2.0)\n</code></pre> <p>               Bases: <code>Module</code></p> <ol> <li>Similar to Layer Scale used in CaiT (Going Deeper With Image Transformers (ICCV'21)), we scale the output of both attention and FFN.</li> <li>For degree L &gt; 0, we scale down the square root of 2 * L, which is to emulate halving the number of channels when using higher L.</li> </ol> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def __init__(self, lmax, num_channels, scale_factor=2.0):\n    super().__init__()\n\n    self.lmax = lmax\n    self.num_channels = num_channels\n    self.scale_factor = scale_factor\n\n    self.affine_weight = nn.Parameter(\n        torch.ones(1, (self.lmax + 1), self.num_channels)\n    )\n    for l_dx in range(1, self.lmax + 1):\n        self.affine_weight.data[0, l_dx, :].mul_(\n            1.0 / math.sqrt(self.scale_factor * l_dx)\n        )\n    expand_index = get_l_to_all_m_expand_index(self.lmax)\n    self.register_buffer(\"expand_index\", expand_index)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantDegreeLayerScale.affine_weight","title":"affine_weight  <code>instance-attribute</code>","text":"<pre><code>affine_weight = Parameter(ones(1, lmax + 1, num_channels))\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantDegreeLayerScale.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantDegreeLayerScale.num_channels","title":"num_channels  <code>instance-attribute</code>","text":"<pre><code>num_channels = num_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantDegreeLayerScale.scale_factor","title":"scale_factor  <code>instance-attribute</code>","text":"<pre><code>scale_factor = scale_factor\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantDegreeLayerScale.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def __repr__(self):\n    return f\"{self.__class__.__name__}(lmax={self.lmax}, num_channels={self.num_channels}, scale_factor={self.scale_factor})\"\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantDegreeLayerScale.forward","title":"forward","text":"<pre><code>forward(node_input)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def forward(self, node_input):\n    weight = torch.index_select(\n        self.affine_weight, dim=1, index=self.expand_index\n    )  # [1, (L_max + 1)**2, C]\n    return node_input * weight  # [N, (L_max + 1)**2, C]\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArray","title":"EquivariantLayerNormArray","text":"<pre><code>EquivariantLayerNormArray(lmax, num_channels, eps=1e-05, affine=True, normalization='component')\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def __init__(\n    self, lmax, num_channels, eps=1e-5, affine=True, normalization=\"component\"\n):\n    super().__init__()\n\n    self.lmax = lmax\n    self.num_channels = num_channels\n    self.eps = eps\n    self.affine = affine\n\n    if affine:\n        self.affine_weight = nn.Parameter(torch.ones(lmax + 1, num_channels))\n        self.affine_bias = nn.Parameter(torch.zeros(num_channels))\n    else:\n        self.register_parameter(\"affine_weight\", None)\n        self.register_parameter(\"affine_bias\", None)\n\n    assert normalization in [\"norm\", \"component\"]\n    self.normalization = normalization\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArray.affine","title":"affine  <code>instance-attribute</code>","text":"<pre><code>affine = affine\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArray.affine_bias","title":"affine_bias  <code>instance-attribute</code>","text":"<pre><code>affine_bias = Parameter(zeros(num_channels))\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArray.affine_weight","title":"affine_weight  <code>instance-attribute</code>","text":"<pre><code>affine_weight = Parameter(ones(lmax + 1, num_channels))\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArray.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArray.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArray.normalization","title":"normalization  <code>instance-attribute</code>","text":"<pre><code>normalization = normalization\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArray.num_channels","title":"num_channels  <code>instance-attribute</code>","text":"<pre><code>num_channels = num_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArray.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def __repr__(self):\n    return f\"{self.__class__.__name__}(lmax={self.lmax}, num_channels={self.num_channels}, eps={self.eps})\"\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArray.forward","title":"forward","text":"<pre><code>forward(node_input)\n</code></pre> <p>Assume input is of shape [N, sphere_basis, C]</p> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>@torch.cuda.amp.autocast(enabled=False)\ndef forward(self, node_input):\n    \"\"\"\n    Assume input is of shape [N, sphere_basis, C]\n    \"\"\"\n\n    out = []\n\n    for l_dx in range(self.lmax + 1):\n        start_idx = l_dx**2\n        length = 2 * l_dx + 1\n\n        feature = node_input.narrow(1, start_idx, length)\n\n        # For scalars, first compute and subtract the mean\n        if l_dx == 0:\n            feature_mean = torch.mean(feature, dim=2, keepdim=True)\n            feature = feature - feature_mean\n\n        # Then compute the rescaling factor (norm of each feature vector)\n        # Rescaling of the norms themselves based on the option \"normalization\"\n        if self.normalization == \"norm\":\n            feature_norm = feature.pow(2).sum(dim=1, keepdim=True)  # [N, 1, C]\n        elif self.normalization == \"component\":\n            feature_norm = feature.pow(2).mean(dim=1, keepdim=True)  # [N, 1, C]\n\n        feature_norm = torch.mean(feature_norm, dim=2, keepdim=True)  # [N, 1, 1]\n        feature_norm = (feature_norm + self.eps).pow(-0.5)\n\n        if self.affine:\n            weight = self.affine_weight.narrow(0, l_dx, 1)  # [1, C]\n            weight = weight.view(1, 1, -1)  # [1, 1, C]\n            feature_norm = feature_norm * weight  # [N, 1, C]\n\n        feature = feature * feature_norm\n\n        if self.affine and l_dx == 0:\n            bias = self.affine_bias\n            bias = bias.view(1, 1, -1)\n            feature = feature + bias\n\n        out.append(feature)\n\n    return torch.cat(out, dim=1)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics","title":"EquivariantLayerNormArraySphericalHarmonics","text":"<pre><code>EquivariantLayerNormArraySphericalHarmonics(lmax, num_channels, eps=1e-05, affine=True, normalization='component', std_balance_degrees=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <ol> <li>Normalize over L = 0.</li> <li>Normalize across all m components from degrees L &gt; 0.</li> <li>Do not normalize separately for different L (L &gt; 0).</li> </ol> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def __init__(\n    self,\n    lmax,\n    num_channels,\n    eps=1e-5,\n    affine=True,\n    normalization=\"component\",\n    std_balance_degrees=True,\n):\n    super().__init__()\n\n    self.lmax = lmax\n    self.num_channels = num_channels\n    self.eps = eps\n    self.affine = affine\n    self.std_balance_degrees = std_balance_degrees\n\n    # for L = 0\n    self.norm_l0 = torch.nn.LayerNorm(\n        self.num_channels, eps=self.eps, elementwise_affine=self.affine\n    )\n\n    # for L &gt; 0\n    if self.affine:\n        self.affine_weight = nn.Parameter(torch.ones(self.lmax, self.num_channels))\n    else:\n        self.register_parameter(\"affine_weight\", None)\n\n    assert normalization in [\"norm\", \"component\"]\n    self.normalization = normalization\n\n    if self.std_balance_degrees:\n        balance_degree_weight = torch.zeros((self.lmax + 1) ** 2 - 1, 1)\n        for l_dx in range(1, self.lmax + 1):\n            start_idx = l_dx**2 - 1\n            length = 2 * l_dx + 1\n            balance_degree_weight[start_idx : (start_idx + length), :] = (\n                1.0 / length\n            )\n        balance_degree_weight = balance_degree_weight / self.lmax\n        self.register_buffer(\"balance_degree_weight\", balance_degree_weight)\n    else:\n        self.balance_degree_weight = None\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.affine","title":"affine  <code>instance-attribute</code>","text":"<pre><code>affine = affine\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.affine_weight","title":"affine_weight  <code>instance-attribute</code>","text":"<pre><code>affine_weight = Parameter(ones(lmax, num_channels))\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.balance_degree_weight","title":"balance_degree_weight  <code>instance-attribute</code>","text":"<pre><code>balance_degree_weight = None\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.norm_l0","title":"norm_l0  <code>instance-attribute</code>","text":"<pre><code>norm_l0 = LayerNorm(num_channels, eps=eps, elementwise_affine=affine)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.normalization","title":"normalization  <code>instance-attribute</code>","text":"<pre><code>normalization = normalization\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.num_channels","title":"num_channels  <code>instance-attribute</code>","text":"<pre><code>num_channels = num_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.std_balance_degrees","title":"std_balance_degrees  <code>instance-attribute</code>","text":"<pre><code>std_balance_degrees = std_balance_degrees\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def __repr__(self):\n    return f\"{self.__class__.__name__}(lmax={self.lmax}, num_channels={self.num_channels}, eps={self.eps}, std_balance_degrees={self.std_balance_degrees})\"\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantLayerNormArraySphericalHarmonics.forward","title":"forward","text":"<pre><code>forward(node_input)\n</code></pre> <p>Assume input is of shape [N, sphere_basis, C]</p> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>@torch.cuda.amp.autocast(enabled=False)\ndef forward(self, node_input):\n    \"\"\"\n    Assume input is of shape [N, sphere_basis, C]\n    \"\"\"\n\n    out = []\n\n    # for L = 0\n    feature = node_input.narrow(1, 0, 1)\n    feature = self.norm_l0(feature)\n    out.append(feature)\n\n    # for L &gt; 0\n    if self.lmax &gt; 0:\n        num_m_components = (self.lmax + 1) ** 2\n        feature = node_input.narrow(1, 1, num_m_components - 1)\n\n        # Then compute the rescaling factor (norm of each feature vector)\n        # Rescaling of the norms themselves based on the option \"normalization\"\n        if self.normalization == \"norm\":\n            feature_norm = feature.pow(2).sum(dim=1, keepdim=True)  # [N, 1, C]\n        elif self.normalization == \"component\":\n            if self.std_balance_degrees:\n                feature_norm = feature.pow(\n                    2\n                )  # [N, (L_max + 1)**2 - 1, C], without L = 0\n                feature_norm = torch.einsum(\n                    \"nic, ia -&gt; nac\", feature_norm, self.balance_degree_weight\n                )  # [N, 1, C]\n            else:\n                feature_norm = feature.pow(2).mean(dim=1, keepdim=True)  # [N, 1, C]\n\n        feature_norm = torch.mean(feature_norm, dim=2, keepdim=True)  # [N, 1, 1]\n        feature_norm = (feature_norm + self.eps).pow(-0.5)\n\n        for l_dx in range(1, self.lmax + 1):\n            start_idx = l_dx**2\n            length = 2 * l_dx + 1\n            feature = node_input.narrow(1, start_idx, length)  # [N, (2L + 1), C]\n            if self.affine:\n                weight = self.affine_weight.narrow(0, (l_dx - 1), 1)  # [1, C]\n                weight = weight.view(1, 1, -1)  # [1, 1, C]\n                feature_scale = feature_norm * weight  # [N, 1, C]\n            else:\n                feature_scale = feature_norm\n            feature = feature * feature_scale\n            out.append(feature)\n\n    return torch.cat(out, dim=1)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonics","title":"EquivariantRMSNormArraySphericalHarmonics","text":"<pre><code>EquivariantRMSNormArraySphericalHarmonics(lmax, num_channels, eps=1e-05, affine=True, normalization='component')\n</code></pre> <p>               Bases: <code>Module</code></p> <ol> <li>Normalize across all m components from degrees L &gt;= 0.</li> </ol> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def __init__(\n    self, lmax, num_channels, eps=1e-5, affine=True, normalization=\"component\"\n):\n    super().__init__()\n\n    self.lmax = lmax\n    self.num_channels = num_channels\n    self.eps = eps\n    self.affine = affine\n\n    # for L &gt;= 0\n    if self.affine:\n        self.affine_weight = nn.Parameter(\n            torch.ones((self.lmax + 1), self.num_channels)\n        )\n    else:\n        self.register_parameter(\"affine_weight\", None)\n\n    assert normalization in [\"norm\", \"component\"]\n    self.normalization = normalization\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonics.affine","title":"affine  <code>instance-attribute</code>","text":"<pre><code>affine = affine\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonics.affine_weight","title":"affine_weight  <code>instance-attribute</code>","text":"<pre><code>affine_weight = Parameter(ones(lmax + 1, num_channels))\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonics.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonics.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonics.normalization","title":"normalization  <code>instance-attribute</code>","text":"<pre><code>normalization = normalization\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonics.num_channels","title":"num_channels  <code>instance-attribute</code>","text":"<pre><code>num_channels = num_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonics.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def __repr__(self):\n    return f\"{self.__class__.__name__}(lmax={self.lmax}, num_channels={self.num_channels}, eps={self.eps})\"\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonics.forward","title":"forward","text":"<pre><code>forward(node_input)\n</code></pre> <p>Assume input is of shape [N, sphere_basis, C]</p> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>@torch.cuda.amp.autocast(enabled=False)\ndef forward(self, node_input):\n    \"\"\"\n    Assume input is of shape [N, sphere_basis, C]\n    \"\"\"\n\n    out = []\n\n    # for L &gt;= 0\n    feature = node_input\n    if self.normalization == \"norm\":\n        feature_norm = feature.pow(2).sum(dim=1, keepdim=True)  # [N, 1, C]\n    elif self.normalization == \"component\":\n        feature_norm = feature.pow(2).mean(dim=1, keepdim=True)  # [N, 1, C]\n\n    feature_norm = torch.mean(feature_norm, dim=2, keepdim=True)  # [N, 1, 1]\n    feature_norm = (feature_norm + self.eps).pow(-0.5)\n\n    for l_dx in range(self.lmax + 1):\n        start_idx = l_dx**2\n        length = 2 * l_dx + 1\n        feature = node_input.narrow(1, start_idx, length)  # [N, (2L + 1), C]\n        if self.affine:\n            weight = self.affine_weight.narrow(0, l_dx, 1)  # [1, C]\n            weight = weight.view(1, 1, -1)  # [1, 1, C]\n            feature_scale = feature_norm * weight  # [N, 1, C]\n        else:\n            feature_scale = feature_norm\n        feature = feature * feature_scale\n        out.append(feature)\n\n    return torch.cat(out, dim=1)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2","title":"EquivariantRMSNormArraySphericalHarmonicsV2","text":"<pre><code>EquivariantRMSNormArraySphericalHarmonicsV2(lmax, num_channels, eps=1e-05, affine=True, normalization='component', centering=True, std_balance_degrees=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <ol> <li>Normalize across all m components from degrees L &gt;= 0.</li> <li>Expand weights and multiply with normalized feature to prevent slicing and concatenation.</li> </ol> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def __init__(\n    self,\n    lmax,\n    num_channels,\n    eps=1e-5,\n    affine=True,\n    normalization=\"component\",\n    centering=True,\n    std_balance_degrees=True,\n):\n    super().__init__()\n\n    self.lmax = lmax\n    self.num_channels = num_channels\n    self.eps = eps\n    self.affine = affine\n    self.centering = centering\n    self.std_balance_degrees = std_balance_degrees\n\n    # for L &gt;= 0\n    if self.affine:\n        self.affine_weight = nn.Parameter(\n            torch.ones((self.lmax + 1), self.num_channels)\n        )\n        if self.centering:\n            self.affine_bias = nn.Parameter(torch.zeros(self.num_channels))\n        else:\n            self.register_parameter(\"affine_bias\", None)\n    else:\n        self.register_parameter(\"affine_weight\", None)\n        self.register_parameter(\"affine_bias\", None)\n\n    assert normalization in [\"norm\", \"component\"]\n    self.normalization = normalization\n\n    expand_index = get_l_to_all_m_expand_index(self.lmax)\n    self.register_buffer(\"expand_index\", expand_index)\n\n    if self.std_balance_degrees:\n        balance_degree_weight = torch.zeros((self.lmax + 1) ** 2, 1)\n        for l_dx in range(self.lmax + 1):\n            start_idx = l_dx**2\n            length = 2 * l_dx + 1\n            balance_degree_weight[start_idx : (start_idx + length), :] = (\n                1.0 / length\n            )\n        balance_degree_weight = balance_degree_weight / (self.lmax + 1)\n        self.register_buffer(\"balance_degree_weight\", balance_degree_weight)\n    else:\n        self.balance_degree_weight = None\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.affine","title":"affine  <code>instance-attribute</code>","text":"<pre><code>affine = affine\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.affine_bias","title":"affine_bias  <code>instance-attribute</code>","text":"<pre><code>affine_bias = Parameter(zeros(num_channels))\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.affine_weight","title":"affine_weight  <code>instance-attribute</code>","text":"<pre><code>affine_weight = Parameter(ones(lmax + 1, num_channels))\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.balance_degree_weight","title":"balance_degree_weight  <code>instance-attribute</code>","text":"<pre><code>balance_degree_weight = None\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.centering","title":"centering  <code>instance-attribute</code>","text":"<pre><code>centering = centering\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.normalization","title":"normalization  <code>instance-attribute</code>","text":"<pre><code>normalization = normalization\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.num_channels","title":"num_channels  <code>instance-attribute</code>","text":"<pre><code>num_channels = num_channels\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.std_balance_degrees","title":"std_balance_degrees  <code>instance-attribute</code>","text":"<pre><code>std_balance_degrees = std_balance_degrees\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def __repr__(self):\n    return f\"{self.__class__.__name__}(lmax={self.lmax}, num_channels={self.num_channels}, eps={self.eps}, centering={self.centering}, std_balance_degrees={self.std_balance_degrees})\"\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.EquivariantRMSNormArraySphericalHarmonicsV2.forward","title":"forward","text":"<pre><code>forward(node_input)\n</code></pre> <p>Assume input is of shape [N, sphere_basis, C]</p> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>@torch.cuda.amp.autocast(enabled=False)\ndef forward(self, node_input):\n    \"\"\"\n    Assume input is of shape [N, sphere_basis, C]\n    \"\"\"\n\n    feature = node_input\n\n    if self.centering:\n        feature_l0 = feature.narrow(1, 0, 1)\n        feature_l0_mean = feature_l0.mean(dim=2, keepdim=True)  # [N, 1, 1]\n        feature_l0 = feature_l0 - feature_l0_mean\n        feature = torch.cat(\n            (feature_l0, feature.narrow(1, 1, feature.shape[1] - 1)), dim=1\n        )\n\n    # for L &gt;= 0\n    if self.normalization == \"norm\":\n        assert not self.std_balance_degrees\n        feature_norm = feature.pow(2).sum(dim=1, keepdim=True)  # [N, 1, C]\n    elif self.normalization == \"component\":\n        if self.std_balance_degrees:\n            feature_norm = feature.pow(2)  # [N, (L_max + 1)**2, C]\n            feature_norm = torch.einsum(\n                \"nic, ia -&gt; nac\", feature_norm, self.balance_degree_weight\n            )  # [N, 1, C]\n        else:\n            feature_norm = feature.pow(2).mean(dim=1, keepdim=True)  # [N, 1, C]\n\n    feature_norm = torch.mean(feature_norm, dim=2, keepdim=True)  # [N, 1, 1]\n    feature_norm = (feature_norm + self.eps).pow(-0.5)\n\n    if self.affine:\n        weight = self.affine_weight.view(\n            1, (self.lmax + 1), self.num_channels\n        )  # [1, L_max + 1, C]\n        weight = torch.index_select(\n            weight, dim=1, index=self.expand_index\n        )  # [1, (L_max + 1)**2, C]\n        feature_norm = feature_norm * weight  # [N, (L_max + 1)**2, C]\n\n    out = feature * feature_norm\n\n    if self.affine and self.centering:\n        out[:, 0:1, :] = out.narrow(1, 0, 1) + self.affine_bias.view(\n            1, 1, self.num_channels\n        )\n\n    return out\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.get_l_to_all_m_expand_index","title":"get_l_to_all_m_expand_index","text":"<pre><code>get_l_to_all_m_expand_index(lmax)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def get_l_to_all_m_expand_index(lmax):\n    expand_index = torch.zeros([(lmax + 1) ** 2]).long()\n    for l_dx in range(lmax + 1):\n        start_idx = l_dx**2\n        length = 2 * l_dx + 1\n        expand_index[start_idx : (start_idx + length)] = l_dx\n    return expand_index\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/layer_norm.html#augmented_partition.lib_equiformer.layer_norm.get_normalization_layer","title":"get_normalization_layer","text":"<pre><code>get_normalization_layer(norm_type, lmax, num_channels, eps=1e-05, affine=True, normalization='component')\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/layer_norm.py</code> <pre><code>def get_normalization_layer(\n    norm_type, lmax, num_channels, eps=1e-5, affine=True, normalization=\"component\"\n):\n    assert norm_type in [\"layer_norm\", \"layer_norm_sh\", \"rms_norm_sh\"]\n    if norm_type == \"layer_norm\":\n        norm_class = EquivariantLayerNormArray\n    elif norm_type == \"layer_norm_sh\":\n        norm_class = EquivariantLayerNormArraySphericalHarmonics\n    elif norm_type == \"rms_norm_sh\":\n        norm_class = EquivariantRMSNormArraySphericalHarmonicsV2\n    else:\n        raise ValueError\n    return norm_class(lmax, num_channels, eps, affine, normalization)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html","title":"process_irreps","text":""},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp","title":"e3TensorDecomp","text":"<pre><code>e3TensorDecomp(net_irreps_out, out_js_list, default_dtype_torch, spinful=False, no_parity=False, if_sort=False, device_torch='cpu')\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/process_irreps.py</code> <pre><code>def __init__(\n    self,\n    net_irreps_out,\n    out_js_list,\n    default_dtype_torch,\n    spinful=False,\n    no_parity=False,\n    if_sort=False,\n    device_torch=\"cpu\",\n):\n    # if spinful:\n    #     default_dtype_torch = flt2cplx(default_dtype_torch)\n    self.dtype = default_dtype_torch\n    self.spinful = spinful\n\n    self.device = device_torch\n    self.out_js_list = out_js_list\n    if net_irreps_out is not None:\n        net_irreps_out = Irreps(net_irreps_out)\n\n    required_irreps_out = Irreps(None)\n    in_slices = [0]\n    wms = []  # wm = wigner_multiplier\n    H_slices = [0]\n    wms_H = []\n    for (\n        H_l1,\n        H_l2,\n    ) in out_js_list:  # for each l1 and l2 of required H blocks in the list\n        # = construct required_irreps_out =\n        mul = 1\n        required_irreps_out_single = irreps_from_l1l2(\n            H_l1, H_l2, mul, no_parity=no_parity\n        )\n        required_irreps_out += required_irreps_out_single\n\n        # = construct slices =\n        in_slices.append(\n            required_irreps_out.dim\n        )  # in_slices represent the orbital interaction\n        H_slices.append(\n            H_slices[-1] + (2 * H_l1 + 1) * (2 * H_l2 + 1)\n        )  # create matrix to store the reconstructed Hamiltonian blocks\n\n        # = get CG coefficients multiplier to act on net_out =\n        wm = []\n        wm_H = []\n        for _a, ir in required_irreps_out_single:\n            for _b in range(mul):\n                # about this 2l+1:\n                # we want the exact inverse of the w_3j symbol, i.e. torch.einsum(\"ijk,jkl-&gt;il\",w_3j(l,l1,l2),w_3j(l1,l2,l))==torch.eye(...). but this is not the case, since the CG coefficients are unitary and w_3j differ from CG coefficients by a constant factor. but we know from https://en.wikipedia.org/wiki/3-j_symbol#Mathematical_relation_to_Clebsch%E2%80%93Gordan_coefficients that 2l+1 is exactly the factor we want.\n                wm.append(\n                    wigner_3j(\n                        H_l1,\n                        H_l2,\n                        ir.l,\n                        dtype=default_dtype_torch,\n                        device=device_torch,\n                    )\n                )\n                wm_H.append(\n                    wigner_3j(\n                        ir.l,\n                        H_l1,\n                        H_l2,\n                        dtype=default_dtype_torch,\n                        device=device_torch,\n                    )\n                    * (2 * ir.l + 1)\n                )\n                # wm.append(wigner_3j(H_l1, H_l2, ir.l, dtype=default_dtype_torch, device=device_torch) * sqrt(2 * ir.l + 1))\n                # wm_H.append(wigner_3j(ir.l, H_l1, H_l2, dtype=default_dtype_torch, device=device_torch) * sqrt(2 * ir.l + 1))\n        wm = torch.cat(wm, dim=-1)\n        wm_H = torch.cat(wm_H, dim=0)\n        wms.append(wm)\n        wms_H.append(wm_H)\n\n    # = check net irreps out =\n    # if spinful:\n    #     required_irreps_out = required_irreps_out + required_irreps_out\n    if net_irreps_out is not None:\n        if if_sort:\n            assert net_irreps_out == required_irreps_out.sort().irreps.simplify(), (\n                f\"requires {required_irreps_out.sort().irreps.simplify()} but got {net_irreps_out}\"\n            )\n        else:\n            assert net_irreps_out == required_irreps_out, (\n                f\"requires {required_irreps_out} but got {net_irreps_out}\"\n            )\n\n    self.in_slices = in_slices\n    self.wms = wms\n    self.H_slices = H_slices\n    self.wms_H = wms_H\n\n    self.sort = None\n    # if if_sort:\n    #     self.sort = sort_irreps(required_irreps_out)\n\n    if self.sort is not None:\n        self.required_irreps_out = self.sort.irreps_out\n    else:\n        self.required_irreps_out = required_irreps_out\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.H_slices","title":"H_slices  <code>instance-attribute</code>","text":"<pre><code>H_slices = H_slices\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device_torch\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = default_dtype_torch\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.in_slices","title":"in_slices  <code>instance-attribute</code>","text":"<pre><code>in_slices = in_slices\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.out_js_list","title":"out_js_list  <code>instance-attribute</code>","text":"<pre><code>out_js_list = out_js_list\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.required_irreps_out","title":"required_irreps_out  <code>instance-attribute</code>","text":"<pre><code>required_irreps_out = irreps_out\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.sort","title":"sort  <code>instance-attribute</code>","text":"<pre><code>sort = None\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.spinful","title":"spinful  <code>instance-attribute</code>","text":"<pre><code>spinful = spinful\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.wms","title":"wms  <code>instance-attribute</code>","text":"<pre><code>wms = wms\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.wms_H","title":"wms_H  <code>instance-attribute</code>","text":"<pre><code>wms_H = wms_H\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.convert_mask","title":"convert_mask","text":"<pre><code>convert_mask(mask)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/process_irreps.py</code> <pre><code>def convert_mask(self, mask):\n    assert self.spinful\n    num_edges = mask.shape[0]\n    mask = mask.permute(0, 2, 1).reshape(num_edges, -1).repeat(1, 2)\n    if self.sort is not None:\n        mask = self.sort(mask)\n    return mask\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.get_H","title":"get_H","text":"<pre><code>get_H(net_out)\n</code></pre> <p>get openmx type H from net output</p> Source code in <code>augmented_partition/lib_equiformer/process_irreps.py</code> <pre><code>def get_H(self, net_out):\n    r\"\"\"get openmx type H from net output\"\"\"\n    if self.sort is not None:\n        net_out = self.sort.inverse(net_out)\n    out = []\n    for i in range(len(self.out_js_list)):\n        in_slice = slice(self.in_slices[i], self.in_slices[i + 1])\n        net_out_block = net_out[\n            :, in_slice\n        ]  # 25D output edge features are sliced according to dimension of each output l to get the right size for each required H\n        H_block = torch.sum(\n            self.wms[i][None, :, :, :] * net_out_block[:, None, None, :], dim=-1\n        )  # l3 converted back into l1 x l2 using Clebsch Gordan coefficients\n        out.append(H_block.reshape(net_out.shape[0], -1))\n    return torch.cat(\n        out, dim=-1\n    )  # output shape: [edge, (4 spin components,) H_flattened_concatenated]\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.e3TensorDecomp.get_net_out","title":"get_net_out","text":"<pre><code>get_net_out(H)\n</code></pre> <p>get net output from openmx type H</p> Source code in <code>augmented_partition/lib_equiformer/process_irreps.py</code> <pre><code>def get_net_out(self, H):\n    r\"\"\"get net output from openmx type H\"\"\"\n    out = []\n    for i in range(len(self.out_js_list)):\n        H_slice = slice(self.H_slices[i], self.H_slices[i + 1])\n\n        l1, l2 = self.out_js_list[i]\n        H_block = H[:, H_slice].reshape(-1, 2 * l1 + 1, 2 * l2 + 1)\n        net_out_block = torch.sum(\n            self.wms_H[i][None, :, :, :] * H_block[:, None, :, :], dim=(-1, -2)\n        )\n        out.append(net_out_block)\n    out = torch.cat(out, dim=-1)\n    if self.sort is not None:\n        out = self.sort(out)\n    return out\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.irreps_from_l1l2","title":"irreps_from_l1l2","text":"<pre><code>irreps_from_l1l2(l1, l2, mul, no_parity=True)\n</code></pre> <p>non-spinful example: l1=1, l2=2 (1x2) -&gt; required_irreps_full=1+2+3, required_irreps=1+2+3, required_irreps_x1=None</p> <p>spinful example: l1=1, l2=2 (1x0.5)x(2x0.5) -&gt; required_irreps_full = 1+2+3 + 0+1+2 + 1+2+3 + 2+3+4 required_irreps = (1+2+3)x0 = 1+2+3 required_irreps_x1 = (1+2+3)x1 = [0+1+2, 1+2+3, 2+3+4]</p> <p>notice that required_irreps_x1 is a list of Irreps</p> Source code in <code>augmented_partition/lib_equiformer/process_irreps.py</code> <pre><code>def irreps_from_l1l2(l1, l2, mul, no_parity=True):\n    r\"\"\"\n    non-spinful example: l1=1, l2=2 (1x2) -&gt;\n    required_irreps_full=1+2+3, required_irreps=1+2+3, required_irreps_x1=None\n\n    spinful example: l1=1, l2=2 (1x0.5)x(2x0.5) -&gt;\n    required_irreps_full = 1+2+3 + 0+1+2 + 1+2+3 + 2+3+4\n    required_irreps = (1+2+3)x0 = 1+2+3\n    required_irreps_x1 = (1+2+3)x1 = [0+1+2, 1+2+3, 2+3+4]\n\n    notice that required_irreps_x1 is a list of Irreps\n    \"\"\"\n    p = 1\n    if not no_parity:\n        p = (-1) ** (l1 + l2)\n    required_ls = range(abs(l1 - l2), l1 + l2 + 1)\n    return Irreps([(mul, (l_dx, p)) for l_dx in required_ls])\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.orbital_analysis","title":"orbital_analysis","text":"<pre><code>orbital_analysis(atom_orbitals, targets=None, no_parity=True)\n</code></pre> <p>example of atom_orbitals: {'42': [0, 0, 0, 1, 1, 2, 2], '16': [0, 0, 1, 1, 2]}</p> <p>required_block_type: 's' - specify; 'a' - all; 'o' - off-diagonal; 'd' - diagonal;</p> Source code in <code>augmented_partition/lib_equiformer/process_irreps.py</code> <pre><code>def orbital_analysis(\n    atom_orbitals, targets=None, no_parity=True\n):  # note that atom_orbitals represent the unique elements in the structure, not the actual number of atoms\n    r\"\"\"\n    example of atom_orbitals: {'42': [0, 0, 0, 1, 1, 2, 2], '16': [0, 0, 1, 1, 2]}\n\n    required_block_type: 's' - specify; 'a' - all; 'o' - off-diagonal; 'd' - diagonal;\n    \"\"\"\n    hoppings_list = []  # [{'42 16': [4, 3]}, ...]\n    for atom1, orbitals1 in atom_orbitals.items():\n        for atom2, orbitals2 in atom_orbitals.items():\n            hopping_key = atom1 + \" \" + atom2\n            # if element_pairs:\n            #     if hopping_key not in element_pairs:\n            #         continue\n            for orbital1 in range(len(orbitals1)):\n                for orbital2 in range(len(orbitals2)):\n                    hopping_orbital = [orbital1, orbital2]\n                    hoppings_list.append(\n                        {hopping_key: hopping_orbital}\n                    )  # keys of hopping_list contain the atomic numbers of the two atoms, values contain the orbital indices of the two interacting orbitals\n\n    il_list = []  # [[1, 1, 2, 0], ...] this means the hopping is from 1st l=1 orbital to 0th l=2 orbital.\n    for hopping in hoppings_list:\n        for N_M_str, block in hopping.items():\n            atom1, atom2 = N_M_str.split()\n            l1 = atom_orbitals[\n                atom1\n            ][\n                block[0]\n            ]  # finds the l of each orbital pair dictionary in the list, block contains the pair of orbital indices, which are mapped to ls by atomic_orbitals\n            l2 = atom_orbitals[atom2][block[1]]\n            il1 = (\n                block[0] - atom_orbitals[atom1].index(l1)\n            )  # il1 specifies the index of orbitals with the same l e.g. if orbital index is 3, but it is the 2nd l=1 orbital, it is 3-1 = 2, with 1 being the index of the first l=1 orbital\n            il2 = block[1] - atom_orbitals[atom2].index(l2)\n        il_list.append([l1, il1, l2, il2])\n\n    hoppings_list_mask = [\n        False for _ in range(len(hoppings_list))\n    ]  # if that hopping is already included, then it is True\n    targets = []\n    # net_out_irreps_list = []\n    net_out_irreps = Irreps(None)\n\n    # print(hoppings_list)\n    for hopping1_index in range(len(hoppings_list)):\n        target = {}\n        if not hoppings_list_mask[\n            hopping1_index\n        ]:  # make sure that there is no repeat in entries\n            hoppings_list_mask[hopping1_index] = True\n            target.update(\n                hoppings_list[hopping1_index]\n            )  # add the key (atomic species of the interacting atoms) and its values (orbital indices of the interacting orbitals) to the target dictionary.\n            for hopping2_index in range(len(hoppings_list)):\n                if (\n                    not hoppings_list_mask[hopping2_index]\n                    and (il_list[hopping1_index] == il_list[hopping2_index])\n                ):  # il1 = il2 means the two hoppings are similar (similar means that they have the same l1 and l2 and the same orbital indices among orbitals of the same l1 and l2 )\n                    target.update(\n                        hoppings_list[hopping2_index]\n                    )  # if the two are similar, the target dictionary should now contain an additional entry (atomic numbers + orbital indices of interacting orbitals).\n                    hoppings_list_mask[hopping2_index] = True\n            targets.append(\n                target\n            )  # each target in targets represent a specific group of similar orbital interactions, between the nth l1 orbital of atom 1 and the mth l2 orbital of atom 2\n\n            l1, l2 = il_list[hopping1_index][0], il_list[hopping1_index][2]\n            irreps_new = irreps_from_l1l2(l1, l2, 1, no_parity=no_parity)\n\n            net_out_irreps = net_out_irreps + irreps_new\n\n    return targets, net_out_irreps, net_out_irreps.sort()[0].simplify()\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/process_irreps.html#augmented_partition.lib_equiformer.process_irreps.process_targets","title":"process_targets","text":"<pre><code>process_targets(orbital_types, index_to_Z, targets)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/process_irreps.py</code> <pre><code>def process_targets(orbital_types, index_to_Z, targets):\n    Z_to_index = torch.full((100,), -1, dtype=torch.int64)\n    Z_to_index[index_to_Z] = torch.arange(len(index_to_Z))\n\n    orbital_types = [np.array(x, dtype=np.int32) for x in orbital_types]\n    orbital_types_cumsum = [\n        np.concatenate([np.zeros(1, dtype=np.int32), np.cumsum(2 * x + 1)])\n        for x in orbital_types\n    ]\n\n    # = process the orbital indices into block slices =\n    equivariant_blocks, out_js_list = [], []\n    out_slices = [0]\n    for target in targets:\n        out_js = None\n        equivariant_block = {}\n        for N_M_str, block_indices in target.items():\n            i, j = (Z_to_index[int(x)] for x in N_M_str.split())\n            block_slice = [\n                orbital_types_cumsum[i][\n                    block_indices[0]\n                ],  # defines the indices that indicate the start and end of the matrix block in row and column direction\n                orbital_types_cumsum[i][block_indices[0] + 1],\n                orbital_types_cumsum[j][block_indices[1]],\n                orbital_types_cumsum[j][block_indices[1] + 1],\n            ]\n            equivariant_block.update({N_M_str: block_slice})\n            if out_js is None:\n                out_js = (\n                    orbital_types[i][block_indices[0]],\n                    orbital_types[j][block_indices[1]],\n                )\n            else:\n                assert out_js == (\n                    orbital_types[i][block_indices[0]],\n                    orbital_types[j][block_indices[1]],\n                )\n        equivariant_blocks.append(equivariant_block)\n        out_js_list.append(tuple(map(int, out_js)))\n        out_slices.append(out_slices[-1] + (2 * out_js[0] + 1) * (2 * out_js[1] + 1))\n\n    return equivariant_blocks, out_js_list, out_slices\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/radial_function.html","title":"radial_function","text":""},{"location":"reference/augmented_partition/lib_equiformer/radial_function.html#augmented_partition.lib_equiformer.radial_function.RadialFunction","title":"RadialFunction","text":"<pre><code>RadialFunction(channels_list)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Contruct a radial function (linear layers + layer normalization + SiLU) given a list of channels edge_channels_list = [sphere_channels, m_output_channels, ..., m_output_channels]</p> Source code in <code>augmented_partition/lib_equiformer/radial_function.py</code> <pre><code>def __init__(self, channels_list):\n    super().__init__()\n    modules = []\n    input_channels = channels_list[0]\n\n    for i in range(len(channels_list)):\n        if i == 0:\n            continue\n\n        modules.append(nn.Linear(input_channels, channels_list[i], bias=True))\n        input_channels = channels_list[i]\n\n        if i == len(channels_list) - 1:\n            break\n\n        modules.append(nn.LayerNorm(channels_list[i]))\n        modules.append(torch.nn.SiLU())\n\n    self.net = nn.Sequential(*modules)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/radial_function.html#augmented_partition.lib_equiformer.radial_function.RadialFunction.net","title":"net  <code>instance-attribute</code>","text":"<pre><code>net = Sequential(*modules)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/radial_function.html#augmented_partition.lib_equiformer.radial_function.RadialFunction.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/radial_function.py</code> <pre><code>def forward(self, inputs):\n    return self.net(inputs)\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/wigner.html","title":"wigner","text":""},{"location":"reference/augmented_partition/lib_equiformer/wigner.html#augmented_partition.lib_equiformer.wigner.jd_path","title":"jd_path  <code>module-attribute</code>","text":"<pre><code>jd_path = this_file_directory / 'Jd.pt'\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/wigner.html#augmented_partition.lib_equiformer.wigner.this_file_directory","title":"this_file_directory  <code>module-attribute</code>","text":"<pre><code>this_file_directory = parent\n</code></pre>"},{"location":"reference/augmented_partition/lib_equiformer/wigner.html#augmented_partition.lib_equiformer.wigner.wigner_D","title":"wigner_D","text":"<pre><code>wigner_D(l_dx, alpha, beta, gamma)\n</code></pre> Source code in <code>augmented_partition/lib_equiformer/wigner.py</code> <pre><code>def wigner_D(l_dx, alpha, beta, gamma):\n    if not l_dx &lt; len(_Jd):\n        raise NotImplementedError(\n            f\"wigner D maximum l implemented is {len(_Jd) - 1}, send us an email to ask for more\"\n        )\n\n    alpha, beta, gamma = torch.broadcast_tensors(alpha, beta, gamma)\n    J = _Jd[l_dx].to(dtype=alpha.dtype, device=alpha.device)\n    Xa = _z_rot_mat(alpha, l_dx)\n    Xb = _z_rot_mat(beta, l_dx)\n    Xc = _z_rot_mat(gamma, l_dx)\n    return Xa @ J @ Xb @ J @ Xc\n</code></pre>"},{"location":"reference/augmented_partition/model/compute_env.html","title":"compute_env","text":""},{"location":"reference/augmented_partition/model/compute_env.html#augmented_partition.model.compute_env.dist_restart","title":"dist_restart","text":"<pre><code>dist_restart(restart_file, model, optimizer)\n</code></pre> Source code in <code>augmented_partition/model/compute_env.py</code> <pre><code>def dist_restart(restart_file, model, optimizer):\n    if restart_file is not None:\n        print(\"Restarting training from a saved model and optimizer state...\", flush=True)\n        checkpoint = torch.load(restart_file)\n        state_dict = checkpoint['model_state_dict']\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n        if dist.is_available() and dist.is_initialized():\n            # If the model was saved with DDP, remove the 'module' prefix that it might have (just in case)\n            if 'module.' in next(iter(checkpoint['model_state_dict'].keys())):\n                prefix = 'module.'\n                state_dict = {k[len(prefix):] if k.startswith(prefix) else k: v for k, v in state_dict.items()}\n            # with the current training setup, the module prefix is already removed\n            model.load_state_dict(state_dict)\n        else:\n            state_dict = remove_module_prefix(checkpoint['model_state_dict'])\n            model.load_state_dict(state_dict)\n\n    return model, optimizer\n</code></pre>"},{"location":"reference/augmented_partition/model/compute_env.html#augmented_partition.model.compute_env.initialize_compute_env","title":"initialize_compute_env","text":"<pre><code>initialize_compute_env()\n</code></pre> Source code in <code>augmented_partition/model/compute_env.py</code> <pre><code>def initialize_compute_env():\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    if 'SLURM_PROCID' in os.environ:  \n        rank = int(os.environ['SLURM_PROCID'])\n        world_size = int(os.environ['SLURM_NTASKS'])\n        local_rank = int(os.environ['SLURM_LOCALID'])\n        os.environ['RANK'] = str(rank)\n        os.environ['WORLD_SIZE'] = str(world_size)\n        os.environ['LOCAL_RANK'] = str(local_rank)\n        backend = 'gloo'  # Use NCCL for Piz Daint (edit: RDMA may be broken, switching to gloo)\n        dist.init_process_group(backend=backend, rank=rank, world_size=world_size)\n        print(\"Initialized process group in: SLURM\", flush=True)\n\n    else:  \n        rank = 0\n        world_size = 1\n        local_rank = 0\n        os.environ['MASTER_ADDR'] = '127.0.0.1'\n        os.environ['MASTER_PORT'] = '29500'\n        backend = 'gloo'  # Use Gloo for attelas (single GPU)\n        print(\"Initialized process group in: local\", flush=True)\n\n    if dist.is_initialized() and dist.get_rank() == 0:  \n        for i in range(world_size):\n            if i == rank:\n                print(f\"RANK: {rank}\", flush=True)\n                print(f\"WORLD_SIZE: {world_size}\", flush=True)\n                print(f\"LOCAL_RANK: {local_rank}\", flush=True)\n            dist.barrier()\n\n    return device, world_size\n</code></pre>"},{"location":"reference/augmented_partition/model/compute_env.html#augmented_partition.model.compute_env.only_rank_zero","title":"only_rank_zero","text":"<pre><code>only_rank_zero(func)\n</code></pre> Source code in <code>augmented_partition/model/compute_env.py</code> <pre><code>def only_rank_zero(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if dist.is_available() and dist.is_initialized():\n            if dist.get_rank() == 0:\n                return func(*args, **kwargs)\n        else:\n            return func(*args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"reference/augmented_partition/model/compute_env.html#augmented_partition.model.compute_env.remove_module_prefix","title":"remove_module_prefix","text":"<pre><code>remove_module_prefix(state_dict)\n</code></pre> Source code in <code>augmented_partition/model/compute_env.py</code> <pre><code>def remove_module_prefix(state_dict):\n    prefix = 'module.'\n    return {k[len(prefix):] if k.startswith(prefix) else k: v for k, v in state_dict.items()}\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html","title":"data","text":""},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.CustomDataset","title":"CustomDataset","text":"<pre><code>CustomDataset(data_list)\n</code></pre> <p>               Bases: <code>Dataset</code></p> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def __init__(self, data_list):\n    self.data_list = data_list\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.CustomDataset.data_list","title":"data_list  <code>instance-attribute</code>","text":"<pre><code>data_list = data_list\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.CustomDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def __getitem__(self, idx):\n    return self.data_list[idx]\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.CustomDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def __len__(self):\n    return len(self.data_list)\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.batch_data_material_cartesian","title":"batch_data_material_cartesian","text":"<pre><code>batch_data_material_cartesian(graph, start0, total_length, num_slices, save_file='None', equivariant_blocks=None, out_slices=None, construct_kernel=None, dtype=float32, slice_direction=0, add_virtual=True, two_way=False, extra_data=None, use_overlap=False)\n</code></pre> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def batch_data_material_cartesian(\n    graph,\n    start0,\n    total_length,\n    num_slices,\n    # test_list=None,\n    save_file=\"None\",\n    # cutoff=2,\n    equivariant_blocks=None,\n    out_slices=None,\n    construct_kernel=None,\n    dtype=torch.float32,\n    slice_direction=0,\n    add_virtual=True,\n    two_way=False,\n    extra_data=None,\n    use_overlap=False,\n):\n    data_list = []\n\n    start = start0\n    length = total_length / num_slices\n    num_atoms = 0\n    num_edges = 0\n\n    print(length)\n\n    for _i in range(num_slices):\n        train_data = createdata_subgraph_cartesian(\n            graph,\n            start,\n            length,\n            equivariant_blocks,\n            out_slices,\n            construct_kernel,\n            dtype,\n            slice_direction,\n            add_virtual,\n            two_way,\n            use_overlap=use_overlap,\n        )\n        torch.save(\n            train_data,\n            save_file\n            + \"_structure_\"\n            + \"_training_\"\n            + str(start)\n            + \"_\"\n            + str(start + length)\n            + \".pt\",\n        )\n        data_list.append(train_data)\n        start = start + length\n        num_atoms += train_data.labelled_node_size\n        num_edges += train_data.labelled_edge_size\n        print(\"Number of atoms:\", train_data.labelled_node_size)\n        print(\"Number of edges:\", train_data.labelled_edge_size)\n\n    print(\"Total Number of Atoms: \", num_atoms)\n    print(\"Total Number of Edges: \", num_edges)\n\n    if extra_data is not None:\n        for i in range(len(extra_data)):  # extra data is a list of structure objects\n            start = start0\n            num_atoms = 0\n            num_edges = 0\n\n            for _j in range(num_slices):\n                train_data = createdata_subgraph_cartesian(\n                    extra_data[i],\n                    start,\n                    length,\n                    equivariant_blocks,\n                    out_slices,\n                    construct_kernel,\n                    dtype,\n                    slice_direction,\n                    add_virtual,\n                    two_way,\n                    use_overlap=use_overlap,\n                )\n                torch.save(\n                    train_data,\n                    save_file\n                    + \"_extra_\"\n                    + str(i)\n                    + \"_training_\"\n                    + str(start)\n                    + \"_\"\n                    + str(start + length)\n                    + \".pt\",\n                )\n                data_list.append(train_data)\n                start = start + length\n                num_atoms += train_data.labelled_node_size\n                num_edges += train_data.labelled_edge_size\n                print(\"Number of atoms:\", train_data.labelled_node_size)\n                print(\"Number of edges:\", train_data.labelled_edge_size)\n\n            print(\n                \"Total Number of Atoms for Extra Structure \" + str(i) + \":\", num_atoms\n            )\n            print(\n                \"Total Number of Edges for Extra Structure \" + str(i) + \":\", num_edges\n            )\n\n    dataset = CustomDataset(data_list)\n\n    if dist.is_initialized():\n        sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n        loader = DataLoader(\n            dataset,\n            sampler=sampler,\n            batch_size=1,\n            shuffle=False,\n            collate_fn=custom_collate_fn,\n        )\n    else:\n        loader = DataLoader(\n            dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn\n        )\n\n    print(\"*** Batch properties:\")\n    for batch in loader:\n        print(\"--&gt; Batch: \")\n        print(\"Node Features (x):\", batch.x.size())\n        print(\"Edge Index:\", batch.edge_index.size())\n        print(\"Edge Features (edge_attr):\", batch.edge_attr.size())\n        print(\"Average Node Degree:\", np.mean(np.array(batch.node_degree)))\n        print(\n            \"Average Reduced Node Degree\", np.mean(np.array(batch.reduced_node_degree))\n        )\n\n    return loader\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.batch_data_molecules","title":"batch_data_molecules","text":"<pre><code>batch_data_molecules(structures, device, num_graph=1, batch_size=1, equivariant_blocks=None, out_slices=None, construct_kernel=None, dtype=float64)\n</code></pre> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def batch_data_molecules(\n    structures,\n    device,\n    num_graph=1,\n    batch_size=1,\n    equivariant_blocks=None,\n    out_slices=None,\n    construct_kernel=None,\n    dtype=torch.float64,\n):\n    data_list = []\n\n    for i in range(num_graph):\n        data = create_input_data_molecules(\n            structures[i],\n            equivariant_blocks,\n            out_slices,\n            construct_kernel,\n            device,\n            dtype=dtype,\n        )\n        data_list.append(data)\n\n    dataset = CustomDataset(data_list)\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=custom_collate_fn,\n        num_workers=0,\n    )\n\n    print(\"*** Batch properties:\")\n    for batch in loader:\n        print(\"Node Features (x):\", batch.x.size())\n        print(\"Edge Index:\", batch.edge_index.size())\n        print(\"Edge Features (edge_attr):\", batch.edge_attr.size())\n\n    return loader\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.create_input_data_molecules","title":"create_input_data_molecules","text":"<pre><code>create_input_data_molecules(structure, equivariant_blocks, out_slices, construct_kernel, device, dtype)\n</code></pre> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def create_input_data_molecules(\n    structure, equivariant_blocks, out_slices, construct_kernel, device, dtype\n):\n    # Note: for SO2 network, edge_index has two-way edges, and does not include self-connections\n    edge_index = structure.edge_matrix\n    numbers = torch.tensor([utils.periodic_table[i] for i in structure.atomic_species])\n    coordinates = structure.atomic_structure.get_positions()\n    cell = structure.atomic_structure.get_cell()\n\n    # Make targets:\n\n    # off-diagonal orbital blocks for each edge (bothways)\n    edge_hams = structure.get_orbital_blocks(edge_index)\n    edge_index = torch.tensor(edge_index)\n    H_blocks_edge = [\n        edge_hams[(edge_index[0][i].item(), edge_index[1][i].item())]\n        for i in range(len(edge_index[0]))\n    ]\n    H_blocks_edge = np.array(H_blocks_edge, dtype=object)\n\n    # diagonal orbital blocks (onsite Hamiltonian)\n    onsite_edge_index = np.array([np.arange(len(numbers)), np.arange(len(numbers))])\n    onsite_hams = structure.get_orbital_blocks(onsite_edge_index)\n    onsite = [\n        onsite_hams[(onsite_edge_index[0][i].item(), onsite_edge_index[1][i].item())]\n        for i in range(len(numbers))\n    ]\n    onsite = np.array(onsite, dtype=object)\n\n    # off-diagonal orbital blocks\n    edge_labels = []\n    for i in range(len(edge_index[0])):\n        label = np.zeros(out_slices[-1])\n        for index_target, equivariant_block in enumerate(equivariant_blocks):\n            for N_M_str, block_slice in equivariant_block.items():\n                slice_row = slice(block_slice[0], block_slice[1])\n                slice_col = slice(block_slice[2], block_slice[3])\n                # len_row = block_slice[1] - block_slice[0]\n                # len_col = block_slice[3] - block_slice[2]\n                slice_out = slice(\n                    out_slices[index_target], out_slices[index_target + 1]\n                )\n                condition_number_i, condition_number_j = N_M_str.split()\n\n                if numbers[edge_index[0][i]].item() == int(\n                    condition_number_i\n                ) and numbers[edge_index[1][i]].item() == int(condition_number_j):\n                    label[slice_out] += np.squeeze(\n                        H_blocks_edge[i][slice_row, slice_col].reshape(1, -1)\n                    )\n\n        edge_labels.append(label)\n\n    # diagonal orbital blocks\n    node_labels = []\n    for i in range(len(onsite_edge_index[0])):\n        label = np.zeros(out_slices[-1])\n        for index_target, equivariant_block in enumerate(equivariant_blocks):\n            for N_M_str, block_slice in equivariant_block.items():\n                slice_row = slice(block_slice[0], block_slice[1])\n                slice_col = slice(block_slice[2], block_slice[3])\n                # len_row = block_slice[1] - block_slice[0]\n                # len_col = block_slice[3] - block_slice[2]\n                slice_out = slice(\n                    out_slices[index_target], out_slices[index_target + 1]\n                )\n                condition_number_i, condition_number_j = N_M_str.split()\n                if numbers[onsite_edge_index[0][i]].item() == int(\n                    condition_number_i\n                ) and numbers[onsite_edge_index[1][i]].item() == int(\n                    condition_number_j\n                ):\n                    label[slice_out] += np.squeeze(\n                        onsite[i][slice_row, slice_col].reshape(1, -1)\n                    )\n\n        node_labels.append(label)\n    numbers = numbers.numpy()\n\n    coordinates = torch.tensor(coordinates)\n\n    edge_fea = torch.empty((len(edge_index[0]), 4))\n    for i in range(len(edge_index[0])):\n        distance_vector, distance = find_mic(\n            coordinates[edge_index[1][i]] - coordinates[edge_index[0][i]], cell\n        )\n        edge_fea[i, :] = torch.cat(\n            (torch.tensor([distance]), torch.tensor(distance_vector))\n        )\n\n    edge_fea = torch.tensor(edge_fea, dtype=dtype)\n    x = torch.tensor(numbers)\n\n    edge_labels = torch.tensor(np.array(edge_labels), dtype=dtype, device=device)\n    y = construct_kernel.get_net_out(\n        edge_labels\n    )  # convert Hamiltonian labels from uncoupled space to coupled space (to avoid conversion during training)\n\n    node_labels = torch.tensor(node_labels, dtype=dtype, device=device)\n    node_y = construct_kernel.get_net_out(node_labels)\n\n    return gnnData(x=x, edge_index=edge_index, edge_attr=edge_fea, y=y, node_y=node_y)\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.create_slice_graph","title":"create_slice_graph","text":"<pre><code>create_slice_graph(atom_index, edge_matrix, add_virtual=True, two_way=False)\n</code></pre> <p>Generates required data to locate atoms and edges belonging to the slice sub-structure/graph</p> <p>Note: Virtual atoms are always at the end of the atom index list.</p> <p>Inputs: atom_index: list of atom indices that are part of the slice        edge_matrix: edge indices of the full structure        add_virtual: if True, virtual atoms are added to the slice atom index list and their edges are included in the slice edge index list</p> <p>Outputs: slice_graph: dictionary containing the following keys:         full_atom_index: atom indices of the slice sub-structure/graph, including virtual nodes         full_mapped_edge_index: edge indices of the slice sub-structure/graph, follows the order of the atom index list         full_edge_positions: numbers indicating the positions of selected edge indices within the full edge index list         node_degree: number of edges connected to each node         reduced_node_degree: number of non-virtual edges connected to each node         real_node_size: number of non-virtual atoms in the full atom index list, used to separate the virtual atoms from the labelled atoms         real_edge_size: number of non-virtual edges in the full edge index list, used to separate the virtual edges from the labelled edges</p> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def create_slice_graph(atom_index, edge_matrix, add_virtual=True, two_way=False):\n    \"\"\"\n    Generates required data to locate atoms and edges belonging to the slice sub-structure/graph\n\n    Note: Virtual atoms are always at the end of the atom index list.\n\n    Inputs: atom_index: list of atom indices that are part of the slice\n           edge_matrix: edge indices of the full structure\n           add_virtual: if True, virtual atoms are added to the slice atom index list and their edges are included in the slice edge index list\n\n    Outputs: slice_graph: dictionary containing the following keys:\n            full_atom_index: atom indices of the slice sub-structure/graph, including virtual nodes\n            full_mapped_edge_index: edge indices of the slice sub-structure/graph, follows the order of the atom index list\n            full_edge_positions: numbers indicating the positions of selected edge indices within the full edge index list\n            node_degree: number of edges connected to each node\n            reduced_node_degree: number of non-virtual edges connected to each node\n            real_node_size: number of non-virtual atoms in the full atom index list, used to separate the virtual atoms from the labelled atoms\n            real_edge_size: number of non-virtual edges in the full edge index list, used to separate the virtual edges from the labelled edges\n    \"\"\"\n\n    virtual_atom_index = []  # atom indices of the virtual atoms\n    edge_positions = []  # numbers indicating the positions of selected edge indices within the full edge index list\n\n    mapped_edge_index = []  # edge indices of the slice sub-structure/graph, follows the order of the atom index list  e.g. if atom index list is [25, 26, 40 ...], then atom 25 is atom 0 in the sub-structure/graph\n    node_degree = []  # number of edges connected to each node\n    reduced_node_degree = []  # number of non-virtual edges connected to each node\n\n    slice_graph = {}\n\n    for i in range(len(atom_index)):\n        edge_position = np.squeeze(\n            np.where(edge_matrix[0] == atom_index[i])\n        )  # locate the positions of all edges connected to that particular atom\n        node_degree.append(len(edge_position))\n        count = 0\n        for j in range(len(edge_position)):\n            if edge_matrix[1][edge_position[j]] in atom_index:\n                atom_source_index = atom_index.index(\n                    edge_matrix[0][edge_position[j]]\n                )  # find the positions of the source and target atoms that are part of the slice (to create the edge indices for the data objects)\n                atom_target_index = atom_index.index(edge_matrix[1][edge_position[j]])\n                mapped_edge_index.append([atom_source_index, atom_target_index])\n                edge_positions.append(edge_position[j])\n                count = count + 1\n            else:\n                if (\n                    edge_matrix[1][edge_position[j]] not in virtual_atom_index\n                ):  # if the target atom is not part of the slice, add it to the virtual atom index list. Avoid duplicates\n                    virtual_atom_index.append(edge_matrix[1][edge_position[j]].item())\n\n        reduced_node_degree.append(count)\n\n    if add_virtual is True:\n        full_atom_index = (\n            atom_index + virtual_atom_index\n        )  # add the indices of the virtual atoms to the original slice atom index list\n        virtual_edge_positions = []\n        virtual_mapped_edge_index = []\n\n        for i in range(len(virtual_atom_index)):\n            virtual_edge_position = np.squeeze(\n                np.where(edge_matrix[0] == virtual_atom_index[i])\n            )  # find the virtual edges connected to the virtual atoms\n            for j in range(len(virtual_edge_position)):\n                if edge_matrix[1][virtual_edge_position[j]] in atom_index:\n                    atom_i_index = full_atom_index.index(\n                        edge_matrix[0][virtual_edge_position[j]]\n                    )  # only include one way edges where the source atom is a virtual atom and the target atom is part of the slice\n                    atom_j_index = full_atom_index.index(\n                        edge_matrix[1][virtual_edge_position[j]]\n                    )\n                    virtual_mapped_edge_index.append([atom_i_index, atom_j_index])\n                    virtual_edge_positions.append(virtual_edge_position[j])\n\n        full_mapped_edge_index = (\n            mapped_edge_index + virtual_mapped_edge_index\n        )  # mapped edge indices of the full graph including virtual nodes\n        full_edge_positions = edge_positions + virtual_edge_positions\n\n        if two_way is True:\n            print(\"Using two-way edges for virtual nodes\")\n            for i in range(len(atom_index)):\n                virtual_edge_position = np.squeeze(\n                    np.where(edge_matrix[0] == atom_index[i])\n                )  # find the virtual edges connected to the real atoms (source is now the real atom, target is the virtual atom)\n                for j in range(len(virtual_edge_position)):\n                    if edge_matrix[1][virtual_edge_position[j]] in virtual_atom_index:\n                        atom_i_index = full_atom_index.index(\n                            edge_matrix[0][virtual_edge_position[j]]\n                        )\n                        atom_j_index = full_atom_index.index(\n                            edge_matrix[1][virtual_edge_position[j]]\n                        )\n                        virtual_mapped_edge_index.append([atom_i_index, atom_j_index])\n                        virtual_edge_positions.append(virtual_edge_position[j])\n\n    else:\n        full_atom_index = atom_index\n        full_mapped_edge_index = mapped_edge_index\n        full_edge_positions = edge_positions\n\n    slice_graph[\"full_atom_index\"] = torch.tensor(full_atom_index)\n    slice_graph[\"full_mapped_edge_index\"] = torch.tensor(full_mapped_edge_index).T\n    slice_graph[\"full_edge_positions\"] = torch.tensor(full_edge_positions)\n    slice_graph[\"node_degree\"] = node_degree\n    slice_graph[\"reduced_node_degree\"] = reduced_node_degree\n    slice_graph[\"real_node_size\"] = len(\n        atom_index\n    )  # index of the labelled atoms that are part of the slice\n    slice_graph[\"real_edge_size\"] = len(\n        edge_positions\n    )  # index of the labelled edges that are part of the slice\n\n    return slice_graph\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.createdata_subgraph_cartesian","title":"createdata_subgraph_cartesian","text":"<pre><code>createdata_subgraph_cartesian(structure, start, length, equivariant_blocks, out_slices, construct_kernel, dtype=float64, slice_direction=0, add_virtual=True, two_way=False, use_overlap=False)\n</code></pre> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def createdata_subgraph_cartesian(\n    structure,\n    start,\n    length,\n    equivariant_blocks,\n    out_slices,\n    construct_kernel,\n    dtype=torch.float64,\n    slice_direction=0,\n    add_virtual=True,\n    two_way=False,\n    use_overlap=False,\n):\n    pos = structure.atomic_structure.get_positions()\n    cell = structure.atomic_structure.get_cell()\n    edge_matrix = structure.edge_matrix\n    numbers = structure.atomic_numbers\n\n    # atom_index = []\n\n    # for i in range(len(numbers)):\n    #     if slice_cartesian(pos[i], start, length, slice_direction):\n    #         atom_index.append(i)\n    atom_index = [\n        i\n        for i in range(len(numbers))\n        if slice_cartesian(pos[i], start, length, slice_direction)\n    ]\n\n    slice_graph = create_slice_graph(atom_index, edge_matrix, add_virtual, two_way)\n\n    full_mapped_edge_index = slice_graph[\"full_mapped_edge_index\"]\n    full_edge_positions = slice_graph[\"full_edge_positions\"]\n    full_atom_index = slice_graph[\"full_atom_index\"]\n\n    # find the off-diagonal Hamiltonian blocks of all edges that are part of the graph\n    edge_matrix = torch.tensor(edge_matrix)\n    edge_index = edge_matrix.T[full_edge_positions].numpy()\n    edge_index = edge_index.T\n    offsite_ham = structure.get_orbital_blocks(edge_index)\n    # H_blocks_edge = []\n    # for i in range(len(edge_index[0])):\n    #     H_blocks_edge.append(\n    #         offsite_ham[(edge_index[0][i].item(), edge_index[1][i].item())]\n    #     )\n    H_blocks_edge = [\n        offsite_ham[(edge_index[0][i].item(), edge_index[1][i].item())]\n        for i in range(len(edge_index[0]))\n    ]\n    H_blocks_edge = np.array(H_blocks_edge, dtype=object)\n    edge_labels = flatten_data(\n        H_blocks_edge, edge_index, numbers, equivariant_blocks, out_slices\n    )\n\n    # find the onsite Hamiltonian blocks for all atoms that are part of the graph\n    onsite_edge_index = np.array([np.array(full_atom_index), np.array(full_atom_index)])\n    onsite_ham = structure.get_orbital_blocks(onsite_edge_index)\n    # H_blocks_node = []\n    # for i in range(len(onsite_edge_index[0])):\n    #     H_blocks_node.append(\n    #         onsite_ham[(onsite_edge_index[0][i].item(), onsite_edge_index[1][i].item())]\n    #     )\n    H_blocks_node = [\n        onsite_ham[(onsite_edge_index[0][i].item(), onsite_edge_index[1][i].item())]\n        for i in range(len(onsite_edge_index[0]))\n    ]\n    H_blocks_node = np.array(H_blocks_node, dtype=object)\n    node_labels = flatten_data(\n        H_blocks_node, onsite_edge_index, numbers, equivariant_blocks, out_slices\n    )\n\n    # create edge features, which are the interatomic distances - including periodic boundary conditions\n    edge_fea = torch.empty((len(edge_index[0]), 4))\n    for i in range(len(edge_index[0])):\n        distance_vector, distance = find_mic(\n            pos[edge_index[1][i]] - pos[edge_index[0][i]], cell\n        )\n        edge_fea[i, :] = torch.cat(\n            (torch.tensor([distance]), torch.tensor(distance_vector))\n        )\n    edge_fea = torch.tensor(edge_fea, dtype=dtype)\n\n    # create the node features, which are the atomic numbers of the atoms in the slice\n    atomic_numbers = numbers[full_atom_index]\n    x = torch.tensor(atomic_numbers)\n\n    # create the label data for edges and nodes\n    edge_labels_np = np.array(\n        edge_labels\n    )  # Convert list of numpy arrays to a single numpy ndarray\n    edge_labels = torch.tensor(edge_labels_np, dtype=dtype)\n    node_labels_np = np.array(\n        node_labels\n    )  # Convert list of numpy arrays to a single numpy ndarray\n    node_labels = torch.tensor(node_labels_np, dtype=dtype)\n\n    # convert Hamiltonian labels from uncoupled space to coupled space (to avoid conversion during training)\n    y = construct_kernel.get_net_out(edge_labels)\n    node_y = construct_kernel.get_net_out(node_labels)\n\n    atom_indices = torch.tensor(full_atom_index)\n    atom_coordinates = torch.tensor(pos[atom_indices])\n\n    if use_overlap is True:\n        overlap_matrix = structure.get_orbital_blocks(edge_index, operator=\"overlap\")\n        # S_blocks_edge = []\n        # for i in range(len(edge_index[0])):\n        #     S_blocks_edge.append(\n        #         overlap_matrix[(edge_index[0][i].item(), edge_index[1][i].item())]\n        #     )\n        S_blocks_edge = [\n            overlap_matrix[(edge_index[0][i].item(), edge_index[1][i].item())]\n            for i in range(len(edge_index[0]))\n        ]\n\n        S_blocks_edge = np.array(S_blocks_edge, dtype=object)\n        S_labels = flatten_data(\n            S_blocks_edge, edge_index, numbers, equivariant_blocks, out_slices\n        )\n\n        S_labels_np = np.array(\n            S_labels\n        )  # Convert list of numpy arrays to a single numpy ndarray\n        S_labels = torch.tensor(S_labels_np, dtype=dtype)\n        S_input = construct_kernel.get_net_out(S_labels)\n\n    else:\n        S_input = None\n\n    # create the data object\n    return Data(\n        x=x,\n        edge_index=full_mapped_edge_index,\n        edge_attr=edge_fea,\n        y=y,\n        node_y=node_y,\n        labelled_edge_size=slice_graph[\"real_edge_size\"],\n        labelled_node_size=slice_graph[\"real_node_size\"],\n        node_degree=slice_graph[\"node_degree\"],\n        reduced_node_degree=slice_graph[\"reduced_node_degree\"],\n        atom_indices=atom_indices,\n        atom_coordinates=atom_coordinates,\n        S_input=S_input,\n    )\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.custom_collate_fn","title":"custom_collate_fn","text":"<pre><code>custom_collate_fn(batch)\n</code></pre> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def custom_collate_fn(batch):\n    return Batch.from_data_list(batch)\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.flatten_data","title":"flatten_data","text":"<pre><code>flatten_data(H_blocks, edge_matrix, numbers, equivariant_blocks, out_slices)\n</code></pre> <p>Flattens the Hamiltonian blocks H_blocks into a 1D tensor for each edge in the slice sub-structure/graph</p> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def flatten_data(H_blocks, edge_matrix, numbers, equivariant_blocks, out_slices):\n    \"\"\"\n    Flattens the Hamiltonian blocks H_blocks into a 1D tensor for each edge in the slice sub-structure/graph\n    \"\"\"\n\n    labels = []\n    for i in range(len(edge_matrix[0])):\n        label = np.zeros(out_slices[-1])\n        for index_target, equivariant_block in enumerate(equivariant_blocks):\n            for N_M_str, block_slice in equivariant_block.items():\n                slice_row = slice(block_slice[0], block_slice[1])\n                slice_col = slice(block_slice[2], block_slice[3])\n                # len_row = block_slice[1] - block_slice[0]\n                # len_col = block_slice[3] - block_slice[2]\n                slice_out = slice(\n                    out_slices[index_target], out_slices[index_target + 1]\n                )\n                condition_number_i, condition_number_j = N_M_str.split()\n                if numbers[edge_matrix[0][i]].item() == int(\n                    condition_number_i\n                ) and numbers[edge_matrix[1][i]].item() == int(condition_number_j):\n                    label[slice_out] = (\n                        label[slice_out]\n                        + np.squeeze(H_blocks[i][slice_row, slice_col].reshape(1, -1))\n                    )  # slice_out should match with slice_row x slice_row when flattened\n\n        labels.append(label)\n\n    return labels\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.slice_cartesian","title":"slice_cartesian","text":"<pre><code>slice_cartesian(atom_pos, start, length, slice_direction=0)\n</code></pre> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def slice_cartesian(atom_pos, start, length, slice_direction=0):\n    return bool(\n        atom_pos[slice_direction] &gt;= start\n        and atom_pos[slice_direction] &lt; start + length\n    )\n</code></pre>"},{"location":"reference/augmented_partition/model/data.html#augmented_partition.model.data.split_data_indices","title":"split_data_indices","text":"<pre><code>split_data_indices(num_train, num_validate, num_test, num_total, offset=0)\n</code></pre> <p>Splits the data indices into training, validation, and test sets</p> Source code in <code>augmented_partition/model/data.py</code> <pre><code>def split_data_indices(num_train, num_validate, num_test, num_total, offset=0):\n    \"\"\"\n    Splits the data indices into training, validation, and test sets\n    \"\"\"\n    indices = np.arange(offset, num_total)\n    np.random.shuffle(indices)\n\n    train_indices = indices[:num_train]\n    validate_indices = indices[num_train : num_train + num_validate]\n    test_indices = indices[\n        num_train + num_validate : num_train + num_validate + num_test\n    ]\n\n    return train_indices, validate_indices, test_indices\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html","title":"network","text":""},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.GaussianSmearing","title":"GaussianSmearing","text":"<pre><code>GaussianSmearing(start=-5.0, stop=5.0, num_gaussians=50, basis_width_scalar=1.0)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/model/network.py</code> <pre><code>def __init__(self, start=-5.0, stop=5.0, num_gaussians=50, basis_width_scalar=1.0):\n    super().__init__()\n    self.num_output = num_gaussians\n\n    # will create a set of Gaussian basis functions with centers at each value of offset:\n    offset = torch.linspace(start, stop, num_gaussians)\n\n    self.coeff = -0.5 / (basis_width_scalar * (offset[1] - offset[0])).item() ** 2\n\n    self.register_buffer(\"offset\", offset)\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.GaussianSmearing.coeff","title":"coeff  <code>instance-attribute</code>","text":"<pre><code>coeff = -0.5 / item() ** 2\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.GaussianSmearing.num_output","title":"num_output  <code>instance-attribute</code>","text":"<pre><code>num_output = num_gaussians\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.GaussianSmearing.forward","title":"forward","text":"<pre><code>forward(dist)\n</code></pre> Source code in <code>augmented_partition/model/network.py</code> <pre><code>def forward(self, dist):\n    # the input dist is a tensor of scalar distances with shape (num_edges,)\n    # self.offset is a tensor of shape (num_gaussians,)\n    # the output dist will be a tensor of shape (num_edges, num_gaussians) containing the scalar distance to each\n    # of the \"num_gaussians\" Gaussian centers, for each edge in the input tensor\n\n    # for each distance, find the scalar distance to each Gaussian center:\n    dist = dist.view(-1, 1) - self.offset.view(1, -1)\n\n    # apply the Gaussian function to each distance:\n    return torch.exp(self.coeff * torch.pow(dist, 2))\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net","title":"SO2Net","text":"<pre><code>SO2Net(num_layers, lmax, mmax, mappingReduced, sphere_channels, edge_channels_list, attn_hidden_channels, num_heads, attn_alpha_channels, attn_value_channels, ffn_hidden_channels, irreps_in, irreps_out, gaussian_cutoff=13.0, use_overlap=False)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/model/network.py</code> <pre><code>def __init__(\n    self,\n    num_layers,  # num_MP_layers\n    lmax,\n    mmax,\n    mappingReduced,  # SO3.CoefficientMappingModule(lmax, mmax)\n    sphere_channels,\n    edge_channels_list,  # [sphere_channels, sphere_channels, sphere_channels]\n    attn_hidden_channels,\n    num_heads,\n    attn_alpha_channels,\n    attn_value_channels,\n    ffn_hidden_channels,\n    irreps_in,\n    irreps_out,\n    gaussian_cutoff=13.0,\n    use_overlap=False,\n):\n    super().__init__()\n\n    self.lmax = lmax\n    self.mmax = mmax\n\n    # ffn_activation = (\n    #     \"scaled_silu\"  # activation function used in the feedforward network\n    # )\n    norm_type = \"layer_norm_sh\"  # normalizes l=0 and l&gt;0 coefficients separately\n\n    self.sphere_channels = sphere_channels\n    # attn_hidden_channels = attn_hidden_channels\n    # num_heads = num_heads\n    # attn_alpha_channels = attn_alpha_channels\n    # attn_value_channels = attn_value_channels\n    # ffn_hidden_channels = ffn_hidden_channels\n    # attn_activation = \"scaled_silu\"\n    use_attn_renorm = True\n\n    use_m_share_rad = (\n        True  # (?) share the radial part of the edge embedding for all m values\n    )\n\n    max_num_elements = 100  # maximum number of elements which can exist in the dataset (used for the embedding layer)\n    use_atom_edge_embedding = True\n    self.use_overlap = use_overlap\n    self.output_channels = edge_channels_list[\n        -1\n    ]  # last entry of edge_channels_list is used for the output channels between each layer\n\n    self.distance_expansion = GaussianSmearing(\n        0.0,  # start\n        gaussian_cutoff,  # stop\n        edge_channels_list[0],  # num_gaussians used to expand the distance\n        2.0,  # basis_width_scalar\n    )\n\n    sphere_channels_all = self.output_channels\n    self.sphere_embedding = nn.Embedding(max_num_elements, sphere_channels_all)\n\n    if self.use_overlap is True:\n        self.S_lin = Linear(irreps_in=irreps_out, irreps_out=irreps_in, biases=True)\n\n    self.node_lin = Linear(irreps_in=irreps_in, irreps_out=irreps_out, biases=True)\n    self.edge_lin = Linear(irreps_in=irreps_in, irreps_out=irreps_out, biases=True)\n    self.num_layers = num_layers\n\n    self.SO3_rotation = nn.ModuleList()\n    self.SO3_rotation.append(SO3_Rotation(lmax))\n\n    self.blocks = nn.ModuleList()\n\n    for _i in range(num_layers):\n        block1 = NodeBlockV2(\n            self.sphere_channels,\n            attn_hidden_channels,\n            num_heads,\n            attn_alpha_channels,\n            attn_value_channels,\n            ffn_hidden_channels,\n            self.sphere_channels,\n            lmax,\n            mmax,\n            self.SO3_rotation,\n            mappingReduced,\n            max_num_elements,\n            edge_channels_list,\n            use_atom_edge_embedding,\n            use_m_share_rad,\n            # attn_activation,\n            use_attn_renorm,\n            # ffn_activation,\n            norm_type,\n        )\n\n        self.blocks.append(block1)\n\n        block2 = EdgeBlockV2(\n            self.sphere_channels,\n            attn_hidden_channels,\n            num_heads,\n            attn_alpha_channels,\n            attn_value_channels,\n            ffn_hidden_channels,\n            self.sphere_channels,\n            lmax,\n            mmax,\n            self.SO3_rotation,\n            mappingReduced,\n            max_num_elements,\n            edge_channels_list,\n            use_atom_edge_embedding,\n            use_m_share_rad,\n            # attn_activation,\n            # use_attn_renorm,\n            # ffn_activation,\n            norm_type,\n        )\n\n        self.blocks.append(block2)\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.SO3_rotation","title":"SO3_rotation  <code>instance-attribute</code>","text":"<pre><code>SO3_rotation = ModuleList()\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.S_lin","title":"S_lin  <code>instance-attribute</code>","text":"<pre><code>S_lin = Linear(irreps_in=irreps_out, irreps_out=irreps_in, biases=True)\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.blocks","title":"blocks  <code>instance-attribute</code>","text":"<pre><code>blocks = ModuleList()\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.distance_expansion","title":"distance_expansion  <code>instance-attribute</code>","text":"<pre><code>distance_expansion = GaussianSmearing(0.0, gaussian_cutoff, edge_channels_list[0], 2.0)\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.edge_lin","title":"edge_lin  <code>instance-attribute</code>","text":"<pre><code>edge_lin = Linear(irreps_in=irreps_in, irreps_out=irreps_out, biases=True)\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.node_lin","title":"node_lin  <code>instance-attribute</code>","text":"<pre><code>node_lin = Linear(irreps_in=irreps_in, irreps_out=irreps_out, biases=True)\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.num_layers","title":"num_layers  <code>instance-attribute</code>","text":"<pre><code>num_layers = num_layers\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.output_channels","title":"output_channels  <code>instance-attribute</code>","text":"<pre><code>output_channels = edge_channels_list[-1]\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.sphere_channels","title":"sphere_channels  <code>instance-attribute</code>","text":"<pre><code>sphere_channels = sphere_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.sphere_embedding","title":"sphere_embedding  <code>instance-attribute</code>","text":"<pre><code>sphere_embedding = Embedding(max_num_elements, sphere_channels_all)\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.use_overlap","title":"use_overlap  <code>instance-attribute</code>","text":"<pre><code>use_overlap = use_overlap\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.SO2Net.forward","title":"forward","text":"<pre><code>forward(batch)\n</code></pre> Source code in <code>augmented_partition/model/network.py</code> <pre><code>def forward(self, batch):\n    device = batch.y.device\n    dtype = batch.y.dtype\n\n    # note: the batch size dimension multiplies the # nodes and # edges\n    atomic_numbers = batch.x  # shape = (num_nodes) = [3]\n    edge_distance = batch.edge_attr[:, 0]  # shape = (num_edges) = [6]\n    edge_distance_vec = batch.edge_attr[\n        :, [2, 3, 1]\n    ]  # shape = (num_edges, 3) = [6, 3]\n    edge_index = batch.edge_index  # shape = (2, num_edges) = [2, 6]\n\n    num_subgraph_nodes = len(atomic_numbers)\n    num_subgraph_edges = len(edge_distance)\n\n    # Initialise the node embedding with atomic_numbers\n    # length of angular momentum coefficients = (lmax+1)^2 = (4+1)^2 = 25 = 1(l=0) + 3(l=1) + 5(l=2) + 7(l=3) + 9(l=4)\n    # node embedding = (num atoms, num coefficients, sphere_channels) = (3, 25, 64)\n    # edge embedding = (num edges, num coefficients, sphere_channels) = (6, 25, 64)\n    node_embedding = SO3_Embedding(\n        num_subgraph_nodes, self.lmax, self.sphere_channels, device, dtype\n    )  # [number of atoms, number of coefficients, number of channels]\n    edge_embedding = SO3_Embedding(\n        num_subgraph_edges, self.lmax, self.sphere_channels, device, dtype\n    )  # [number of edges, number of coefficients, number of channels]\n\n    node_element_embedding = self.sphere_embedding(atomic_numbers)\n    edge_distance_embedding = self.distance_expansion(edge_distance)\n\n    # Initialize the l = 0, m = 0 coefficients of each embedding:\n    offset_res = 0\n    node_embedding.embedding[:, offset_res, :] = node_element_embedding\n\n    if self.use_overlap is True:\n        print(\"Using overlap matrix as edge features\")\n        S_tensor = self.S_lin(batch.S_input)\n\n        for l_idx in range(self.lmax + 1):\n            start = l_idx**2 * self.sphere_channels\n            end = (\n                l_idx**2 * self.sphere_channels\n                + (2 * l_idx + 1) * self.sphere_channels\n            )\n            edge_embedding.embedding[\n                :, l_idx**2 : (l_idx**2 + 2 * l_idx + 1), :\n            ] = (\n                S_tensor[:, start:end]\n                .reshape(S_tensor.shape[0], self.sphere_channels, 2 * l_idx + 1)\n                .transpose(-1, -2)\n            )\n\n    else:\n        edge_embedding.embedding[:, offset_res, :] = edge_distance_embedding\n\n    # Create 3D rotation matrices for each of the edges\n    edge_rot_mat = init_edge_rot_mat(\n        edge_distance_vec\n    )  # shape = (num_edges, 3, 3) = [6, 3, 3]\n    self.SO3_rotation[0].set_wigner(\n        edge_rot_mat\n    )  # set the rotation matrices for each of the edges in the edge list\n\n    # Process the graph through the layers\n    for i in range(self.num_layers):\n        node_embedding = self.blocks[2 * i](\n            node_embedding,  # SO3_Embedding\n            atomic_numbers,\n            edge_distance_embedding,\n            edge_index,\n            edge_embedding,\n        )\n\n        edge_embedding = self.blocks[2 * i + 1](\n            node_embedding,  # SO3_Embedding\n            atomic_numbers,\n            edge_distance_embedding,\n            edge_index,\n            edge_embedding,\n        )\n\n    node_output = convert_to_irreps(\n        node_embedding, self.output_channels, self.lmax, self.node_lin\n    )\n    edge_output = convert_to_irreps(\n        edge_embedding, self.output_channels, self.lmax, self.edge_lin\n    )\n\n    return node_output, edge_output\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.convert_to_irreps","title":"convert_to_irreps","text":"<pre><code>convert_to_irreps(inp, output_channels, lmax, lin_node)\n</code></pre> <p>Converts the output irreps to the coupled space irrep representation needed to reconstruct the Hamiltonian using the linear layer from e3nn library e.g. map 64x0e+64x1e+64x2e+64x3e+64x4e to 1x0e+1x1e+1x1e+1x0e+1x1e+1x2e+..+1x1e+1x2e+1x3e+1x4e</p> Source code in <code>augmented_partition/model/network.py</code> <pre><code>def convert_to_irreps(inp, output_channels, lmax, lin_node):\n    \"\"\"\n    Converts the output irreps to the coupled space irrep representation needed to reconstruct the Hamiltonian using the linear layer from e3nn library\n    e.g. map 64x0e+64x1e+64x2e+64x3e+64x4e to 1x0e+1x1e+1x1e+1x0e+1x1e+1x2e+..+1x1e+1x2e+1x3e+1x4e\n\n    \"\"\"\n\n    # prepare sorted_output:\n    test_input = inp.embedding.transpose(\n        -1, -2\n    )  # rearrange from l major order into feature major order so that e.g. 64 x 1e can be extracted correctly after flattening the columns belonging to l = 1\n    feature_size = test_input.shape[0]\n    sorted_output = torch.zeros(feature_size, output_channels * ((lmax + 1) ** 2))\n    device = inp.embedding.device\n\n    for l_idx in range(lmax + 1):\n        start = l_idx**2 * output_channels\n        end = l_idx**2 * output_channels + output_channels * (2 * l_idx + 1)\n        sorted_output[:, start:end] = torch.squeeze(\n            test_input[:, :, l_idx**2 : l_idx**2 + (2 * l_idx + 1)].reshape(\n                feature_size, 1, -1\n            )\n        )\n\n    # convert:\n    return lin_node(sorted_output.to(device))\n</code></pre>"},{"location":"reference/augmented_partition/model/network.html#augmented_partition.model.network.init_edge_rot_mat","title":"init_edge_rot_mat","text":"<pre><code>init_edge_rot_mat(edge_distance_vec)\n</code></pre> <p>Takes the edge distance vectors and returns the 3D rotation matrix for each edge</p> Source code in <code>augmented_partition/model/network.py</code> <pre><code>def init_edge_rot_mat(edge_distance_vec):\n    \"\"\"\n    Takes the edge distance vectors and returns the 3D rotation matrix for each edge\n    \"\"\"\n    edge_vec_0 = edge_distance_vec\n    edge_vec_0_distance = torch.sqrt(torch.sum(edge_vec_0**2, dim=1))\n\n    # Make sure the atoms are far enough apart\n    if torch.min(edge_vec_0_distance) &lt; 0.0001:\n        print(f\"Error edge_vec_0_distance: {torch.min(edge_vec_0_distance)}\")\n\n    norm_x = edge_vec_0 / (edge_vec_0_distance.view(-1, 1))\n    edge_vec_2 = torch.rand_like(edge_vec_0) - 0.5\n    edge_vec_2 = edge_vec_2 / (torch.sqrt(torch.sum(edge_vec_2**2, dim=1)).view(-1, 1))\n    # Create two rotated copys of the random vectors in case the random vector is aligned with norm_x\n    # With two 90 degree rotated vectors, at least one should not be aligned with norm_x\n    edge_vec_2b = edge_vec_2.clone()\n    edge_vec_2b[:, 0] = -edge_vec_2[:, 1]\n    edge_vec_2b[:, 1] = edge_vec_2[:, 0]\n    edge_vec_2c = edge_vec_2.clone()\n    edge_vec_2c[:, 1] = -edge_vec_2[:, 2]\n    edge_vec_2c[:, 2] = edge_vec_2[:, 1]\n    vec_dot_b = torch.abs(torch.sum(edge_vec_2b * norm_x, dim=1)).view(-1, 1)\n    vec_dot_c = torch.abs(torch.sum(edge_vec_2c * norm_x, dim=1)).view(-1, 1)\n\n    vec_dot = torch.abs(torch.sum(edge_vec_2 * norm_x, dim=1)).view(-1, 1)\n    edge_vec_2 = torch.where(torch.gt(vec_dot, vec_dot_b), edge_vec_2b, edge_vec_2)\n    vec_dot = torch.abs(torch.sum(edge_vec_2 * norm_x, dim=1)).view(-1, 1)\n    edge_vec_2 = torch.where(torch.gt(vec_dot, vec_dot_c), edge_vec_2c, edge_vec_2)\n\n    vec_dot = torch.abs(torch.sum(edge_vec_2 * norm_x, dim=1))\n\n    # Check the vectors aren't aligned\n    assert torch.max(vec_dot) &lt; 0.99\n\n    norm_z = torch.cross(norm_x, edge_vec_2, dim=1)\n    norm_z = norm_z / (torch.sqrt(torch.sum(norm_z**2, dim=1, keepdim=True)))\n    norm_z = norm_z / (torch.sqrt(torch.sum(norm_z**2, dim=1)).view(-1, 1))\n    norm_y = torch.cross(norm_x, norm_z, dim=1)\n    norm_y = norm_y / (torch.sqrt(torch.sum(norm_y**2, dim=1, keepdim=True)))\n\n    # Construct the 3D rotation matrix\n    norm_x = norm_x.view(-1, 3, 1)\n    norm_y = -norm_y.view(-1, 3, 1)\n    norm_z = norm_z.view(-1, 3, 1)\n\n    edge_rot_mat_inv = torch.cat([norm_z, norm_x, norm_y], dim=2)\n    edge_rot_mat = torch.transpose(edge_rot_mat_inv, 1, 2)\n\n    return edge_rot_mat.detach()\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html","title":"structure","text":""},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure","title":"Structure","text":"<pre><code>Structure(xyz_file, hamiltonian_file, overlap_file, pbc, orbital_basis, dataset='custom', database_props=None, self_interaction=True, bothways=False, make_soap=False, save_matrices=False, rcut=4.0, cell=None, use_overlap=False)\n</code></pre> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def __init__(\n    self,\n    xyz_file,\n    hamiltonian_file,\n    overlap_file,\n    pbc,\n    orbital_basis,\n    dataset=\"custom\",\n    database_props=None,\n    self_interaction=True,\n    bothways=False,\n    make_soap=False,\n    save_matrices=False,\n    rcut=4.0,\n    cell=None,\n    use_overlap=False,\n):\n    # input quantities\n    self.xyz_file = xyz_file  # XYZ file containing atomic positions\n    self.hamiltonian_file = (\n        hamiltonian_file  # File containing the Hamiltonian matrix\n    )\n    self.overlap_file = overlap_file  # File containing the overlap matrix\n    self.database_props = database_props  # SchNet database\n    self.periodic_cell = None  # Periodic cell size\n\n    self.hamiltonian = None  # Hamiltonian matrix\n    self.overlap = None  # Overlap matrix\n    self.neighbour_list = None  # Neighbor list for atomic structure\n    self.edge_matrix = None  # Edge matrix for atomic structure\n    self.num_orbitals_per_atom = None  # Number of orbitals per atom\n    self.num_unique_orbitals = None  # Number of unique orbitals in the system\n    self.soap_features = None  # SOAP descriptor features\n    self.basis = orbital_basis  # Orbital basis for electronic structure\n    self.rotate_dic = None  # dictionary of rotation matrices for each edge\n    self.atomic_species = None  # Atomic species in the structure\n    self.atomic_numbers = None  # Atomic numbers in the structure\n\n    self.use_overlap = use_overlap\n\n    # parameters:\n    self.rcut = rcut  # cutoff radius for neighbor list\n\n    if dataset == \"schnet\":\n        if database_props is None:\n            raise ValueError(\n                \"Database properties must be provided for SchNet dataset.\"\n            )\n\n        # initialize atomic structure\n        self.init_atomic_structure_schnet(\n            self.database_props, pbc, self_interaction, bothways\n        )\n\n        # initialize electronic structure\n        self.init_electronic_structure_schnet(self.database_props)\n\n    else:\n        # initialize atomic structure\n        self.init_atomic_structure(\n            self.xyz_file, pbc, self_interaction, bothways, cell\n        )\n\n        # initialize SOAP features\n        if make_soap:\n            self.make_soap_features(pbc)\n\n        # initialize electronic structure\n        self.init_electronic_structure(\n            self.hamiltonian_file, self.overlap_file, save_matrices\n        )\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.atomic_numbers","title":"atomic_numbers  <code>instance-attribute</code>","text":"<pre><code>atomic_numbers = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.atomic_species","title":"atomic_species  <code>instance-attribute</code>","text":"<pre><code>atomic_species = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.basis","title":"basis  <code>instance-attribute</code>","text":"<pre><code>basis = orbital_basis\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.database_props","title":"database_props  <code>instance-attribute</code>","text":"<pre><code>database_props = database_props\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.edge_matrix","title":"edge_matrix  <code>instance-attribute</code>","text":"<pre><code>edge_matrix = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.hamiltonian","title":"hamiltonian  <code>instance-attribute</code>","text":"<pre><code>hamiltonian = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.hamiltonian_file","title":"hamiltonian_file  <code>instance-attribute</code>","text":"<pre><code>hamiltonian_file = hamiltonian_file\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.neighbour_list","title":"neighbour_list  <code>instance-attribute</code>","text":"<pre><code>neighbour_list = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.num_orbitals_per_atom","title":"num_orbitals_per_atom  <code>instance-attribute</code>","text":"<pre><code>num_orbitals_per_atom = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.num_unique_orbitals","title":"num_unique_orbitals  <code>instance-attribute</code>","text":"<pre><code>num_unique_orbitals = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.overlap","title":"overlap  <code>instance-attribute</code>","text":"<pre><code>overlap = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.overlap_file","title":"overlap_file  <code>instance-attribute</code>","text":"<pre><code>overlap_file = overlap_file\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.periodic_cell","title":"periodic_cell  <code>instance-attribute</code>","text":"<pre><code>periodic_cell = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.rcut","title":"rcut  <code>instance-attribute</code>","text":"<pre><code>rcut = rcut\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.rotate_dic","title":"rotate_dic  <code>instance-attribute</code>","text":"<pre><code>rotate_dic = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.soap_features","title":"soap_features  <code>instance-attribute</code>","text":"<pre><code>soap_features = None\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.use_overlap","title":"use_overlap  <code>instance-attribute</code>","text":"<pre><code>use_overlap = use_overlap\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.xyz_file","title":"xyz_file  <code>instance-attribute</code>","text":"<pre><code>xyz_file = xyz_file\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.ORCA_to_CP2K","title":"ORCA_to_CP2K","text":"<pre><code>ORCA_to_CP2K(hamiltonian)\n</code></pre> <p>Convert the ORCA order to CP2K order (only p and d orbitals implemented)</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def ORCA_to_CP2K(self, hamiltonian):\n    \"\"\"\n    Convert the ORCA order to CP2K order (only p and d orbitals implemented)\n    \"\"\"\n\n    # iterate over atoms in structure:\n    for i in range(len(self.atomic_structure)):\n        species = self.atomic_structure.get_chemical_symbols()[i]\n        starting_index = int(np.sum(self.num_orbitals_per_atom[:i]))\n        orbital_shell = orbital_type_dict[self.basis][species]\n        num_s_orbitals = orbital_shell.count(0)\n        num_p_orbitals = orbital_shell.count(1)\n        num_d_orbitals = orbital_shell.count(2)\n\n        p_permutation = [2, 0, 1]  # ORCA \u2192 CP2K for p orbitals\n        d_permutation = [4, 2, 0, 1, 3]  # ORCA \u2192 CP2K for d orbitals\n\n        for p in range(num_p_orbitals):\n            start = starting_index + num_s_orbitals + 3 * p\n            # permute rows\n            hamiltonian[start : start + 3, :] = hamiltonian[start : start + 3, :][\n                p_permutation, :\n            ]\n            # permute columns\n            hamiltonian[:, start : start + 3] = hamiltonian[:, start : start + 3][\n                :, p_permutation\n            ]\n\n        for d in range(num_d_orbitals):\n            start = starting_index + num_s_orbitals + 3 * num_p_orbitals + 5 * d\n            # permute rows\n            hamiltonian[start : start + 5, :] = hamiltonian[start : start + 5, :][\n                d_permutation, :\n            ]\n            # permute columns\n            hamiltonian[:, start : start + 5] = hamiltonian[:, start : start + 5][\n                :, d_permutation\n            ]\n\n    return hamiltonian\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.csr_to_dict","title":"csr_to_dict","text":"<pre><code>csr_to_dict(csr_matrix)\n</code></pre> <p>Convert a CSR matrix to a dictionary format - ONLY FOR SCHNET</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def csr_to_dict(self, csr_matrix):\n    \"\"\"\n    Convert a CSR matrix to a dictionary format - ONLY FOR SCHNET\n    \"\"\"\n\n    # Extract CSR components\n    indptr = csr_matrix.indptr\n    indices = csr_matrix.indices\n    data = csr_matrix.data\n\n    # Initialize dictionary to store (row, col) -&gt; value mappings\n    dict_matrix = {}\n\n    # Populate the dictionary\n    for row in range(len(indptr) - 1):\n        start_idx = indptr[row]\n        end_idx = indptr[row + 1]\n        for idx in range(start_idx, end_idx):\n            col = indices[idx]\n            value = data[idx]\n            # Note: the SCHNET hamiltonians are zero-indexed so we add 1\n            dict_matrix[(row + 1, col + 1)] = value\n            # dict_matrix[(row, col)] = value\n\n    return dict_matrix\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.get_max_interaction_radius","title":"get_max_interaction_radius","text":"<pre><code>get_max_interaction_radius(eps)\n</code></pre> <p>Return the maximum distance between two atoms, such that the Hamiltonian matrix has at least one element with a magnitude greater than eps. Also saves the interaction distances to a file and plots a histogram of them. Require rcut to be overestimated.</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def get_max_interaction_radius(self, eps):\n    \"\"\"\n    Return the maximum distance between two atoms, such that the Hamiltonian matrix has at\n    least one element with a magnitude greater than eps. Also saves the interaction distances\n    to a file and plots a histogram of them.\n    Require rcut to be overestimated.\n    \"\"\"\n\n    cell = self.atomic_structure.get_cell()\n    interaction_distance_list = []\n\n    # iterate over all the edges in the edge matrix\n    # for i, edge in enumerate(self.edge_matrix.T):\n    for i, edge in enumerate(self.edge_matrix.T):\n        print(i + 1, \"/\", len(self.edge_matrix.T))\n\n        # edge is a 1D array with two elements: [atom_i_index, atom_j_index]\n        atom_i_index = edge[0]\n        atom_j_index = edge[1]\n        orbital_block = self.get_orbital_blocks([[atom_i_index], [atom_j_index]])\n\n        # check if any element in the orbital block has a magnitude greater than eps\n        for key in orbital_block:\n            if np.max(np.abs(orbital_block[key])) &gt; eps:\n                atom_i_pos = self.atomic_structure.get_positions()[atom_i_index]\n                atom_j_pos = self.atomic_structure.get_positions()[atom_j_index]\n                distance = find_mic(atom_i_pos - atom_j_pos, cell)\n                interaction_distance_list.append(distance[1])\n\n    # save the interaction distances to a file\n    with Path(\"interaction_distances.txt\").open(\"w\") as f:\n        for item in interaction_distance_list:\n            f.write(f\"{item}\\n\")\n\n    print(\"Max interaction distance: \", max(interaction_distance_list))\n\n    # plot a histogram of the interaction distances\n    fig, ax = plt.subplots()\n    ax.hist(interaction_distance_list, bins=50)\n    ax.set_xlabel(\"Distance between atoms (A)\")\n    ax.set_ylabel(\"Frequency\")\n    plt.savefig(\"interaction_distances.png\", dpi=300)\n\n    return max(interaction_distance_list)\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.get_orbital_blocks","title":"get_orbital_blocks","text":"<pre><code>get_orbital_blocks(edge_idx, operator='hamiltonian')\n</code></pre> <p>Given the edges between two atoms (as a tuple), extract and return the corresponding orbital blocks from the hamiltonian matrix. (add overlap)</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def get_orbital_blocks(self, edge_idx, operator=\"hamiltonian\"):\n    \"\"\"\n    Given the edges between two atoms (as a tuple), extract and return the corresponding orbital blocks\n    from the hamiltonian matrix. (add overlap)\n    \"\"\"\n\n    orbital_blocks = {}\n\n    try:\n        # iterates over all the edges specified in the input edge_idx list\n        for i in range(len(edge_idx[0])):\n            # atom pair\n            atom_i_index = edge_idx[0][i]\n            atom_j_index = edge_idx[1][i]\n            key_str = (atom_i_index, atom_j_index)\n\n            # initialize size of the orbital block using the # orbitals of the two atoms\n            starting_i, num_orbitals_i = self.map_atom_to_orbital(atom_i_index)\n            starting_j, num_orbitals_j = self.map_atom_to_orbital(atom_j_index)\n            mat = np.zeros(shape=(num_orbitals_i, num_orbitals_j), dtype=float)\n\n            # fill in the orbital block from the hamiltonian matrix\n            for alpha in range(num_orbitals_i):\n                for beta in range(num_orbitals_j):\n                    orb_tuple = (starting_i + alpha, starting_j + beta)\n                    if operator == \"hamiltonian\" and orb_tuple in self.hamiltonian:\n                        # extract the hamiltonian value from the csr matrix if it exists (is nonzero)\n                        mat[alpha, beta] = self.hamiltonian[\n                            (starting_i + alpha, starting_j + beta)\n                        ]\n\n                    elif operator == \"overlap\" and orb_tuple in self.overlap:\n                        # extract the overlap value from the csr matrix if it exists (is nonzero)\n                        mat[alpha, beta] = self.overlap[\n                            (starting_i + alpha, starting_j + beta)\n                        ]\n\n                        # mat[alpha,beta] = self.hamiltonian[(starting_i+alpha,starting_j+beta)]\n\n            orbital_blocks[key_str] = mat\n\n    except TypeError as e:\n        print(f\"TypeError occurred: {e}\")\n        print(\n            \"!! The hamiltonian and overlap files were probably not loaded into the Structure. !!\"\n        )\n\n    return orbital_blocks\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.imagesc_dict","title":"imagesc_dict","text":"<pre><code>imagesc_dict(dict_matrix, log=True)\n</code></pre> <p>Plot the Hamiltonian matrix as an imagesc plot.</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def imagesc_dict(self, dict_matrix, log=True):\n    \"\"\"\n    Plot the Hamiltonian matrix as an imagesc plot.\n    \"\"\"\n\n    # Extract all row and column indices\n    rows, cols = zip(*dict_matrix.keys())\n    n_rows = max(rows) + 1\n    n_cols = max(cols) + 1\n    full_matrix = np.zeros((n_rows, n_cols))\n\n    # Populate the full matrix with the data from the sparse matrix\n    for (i, j), value in dict_matrix.items():\n        if log:\n            full_matrix[i, j] = np.log(np.abs(value))\n        else:\n            full_matrix[i, j] = value\n\n    # Plot the matrix using matplotlib\n    plt.figure()\n    plt.imshow(full_matrix, cmap=\"viridis\")\n    plt.colorbar()\n    plt.title(\"CSR Matrix Visualization\")\n    plt.xlabel(\"Column Index\")\n    plt.ylabel(\"Row Index\")\n    plt.savefig(\"hamiltonian_matrix.png\", dpi=300)\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.init_atomic_structure","title":"init_atomic_structure","text":"<pre><code>init_atomic_structure(xyz_file, pbc, self_interaction, bothways, cell)\n</code></pre> <p>Initialize the atomic structure from an XYZ file.</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def init_atomic_structure(self, xyz_file, pbc, self_interaction, bothways, cell):\n    \"\"\"\n    Initialize the atomic structure from an XYZ file.\n    \"\"\"\n\n    # atomic positions\n    self.atomic_structure = read(xyz_file)\n\n    # set the elements in the atomic structure:\n    self.atomic_species = self.atomic_structure.get_chemical_symbols()\n    self.atomic_numbers = torch.tensor(\n        [utils.periodic_table[i] for i in self.atomic_species]\n    )\n\n    # lattice vectors (periodic box size)\n    if pbc:\n        print(\"Periodic boundary conditions are set.\")\n\n        if cell is not None:\n            # a, b, c = lattice_vector_components\n            self.atomic_structure.set_cell(cell)\n            self.atomic_structure.set_pbc([pbc, pbc, pbc])\n            self.periodic_cell = cell\n\n        else:\n            last_three_values = list(self.atomic_structure.info.keys())[-3:]\n            lattice_vector_components = [\n                float(value.strip(\",\")) for value in last_three_values\n            ]\n            a, b, c = lattice_vector_components\n            self.atomic_structure.set_cell([a, b, c])\n            self.atomic_structure.set_pbc([pbc, pbc, pbc])\n            self.periodic_cell = np.array([a, b, c])\n\n    # neighbor list\n    array_rcut = np.ones(len(self.atomic_structure)) * self.rcut\n    self.neighbour_list = NeighborList(\n        array_rcut, skin=0, self_interaction=self_interaction, bothways=bothways\n    )\n    self.neighbour_list.update(self.atomic_structure)\n\n    # adjacency matrix\n    matrix = self.neighbour_list.get_connectivity_matrix(sparse=True)\n    matrix = matrix.tocoo()\n    edge_matrix_np = np.array([matrix.row, matrix.col], dtype=np.int64)\n    self.edge_matrix = edge_matrix_np\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.init_atomic_structure_schnet","title":"init_atomic_structure_schnet","text":"<pre><code>init_atomic_structure_schnet(database_props, pbc, self_interaction, bothways)\n</code></pre> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def init_atomic_structure_schnet(\n    self, database_props, pbc, self_interaction, bothways\n):\n    # Extract the xyz coordinates and atomic numbers from the database properties\n    positions = np.array(database_props[\"_positions\"], dtype=np.float64)\n    atomic_numbers = np.array(database_props[\"_atomic_numbers\"], dtype=int)\n\n    # Create an ASE Atoms object\n    self.atomic_structure = Atoms(\n        numbers=atomic_numbers, positions=positions, pbc=pbc\n    )\n    self.atomic_species = self.atomic_structure.get_chemical_symbols()\n\n    # neighbor list\n    array_rcut = np.ones(len(self.atomic_structure)) * self.rcut\n    self.neighbour_list = NeighborList(\n        array_rcut, skin=0, self_interaction=self_interaction, bothways=bothways\n    )\n    self.neighbour_list.update(self.atomic_structure)\n\n    # adjacency matrix\n    matrix = self.neighbour_list.get_connectivity_matrix(sparse=True)\n    matrix = matrix.tocoo()\n    edge_matrix_np = np.array([matrix.row, matrix.col], dtype=np.int64)\n    # edge_matrix = torch.tensor(edge_matrix_np, dtype=torch.long)\n    self.edge_matrix = edge_matrix_np\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.init_electronic_structure","title":"init_electronic_structure","text":"<pre><code>init_electronic_structure(hamiltonian_file, overlap_file, save_matrices)\n</code></pre> <p>Initialize the electronic structure from the Hamiltonian and overlap matrices.</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def init_electronic_structure(self, hamiltonian_file, overlap_file, save_matrices):\n    \"\"\"\n    Initialize the electronic structure from the Hamiltonian and overlap matrices.\n    \"\"\"\n\n    hamiltonian_pickle = \"hamiltonian.pkl\"\n    hamiltonian_path = Path(hamiltonian_pickle)\n    # overlap_pickle = \"overlap.pkl\"\n\n    # set up the Hamiltonian and overlap matrices (load from saved pickle if they exist)\n    if hamiltonian_path.exists() and save_matrices is True:\n        print(\"Unpickling hamiltonian matrix...\")\n        with hamiltonian_path.open(\"rb\") as f:\n            self.hamiltonian = pickle.load(f)\n    else:\n        self.hamiltonian = self.read_sparse_matrix_csr(hamiltonian_file)\n\n        if self.use_overlap is True:\n            self.overlap = self.read_sparse_matrix_csr(overlap_file)\n\n    if save_matrices:\n        with hamiltonian_path.open(\"wb\") as f:\n            pickle.dump(self.hamiltonian, f)\n\n    # if os.path.exists(overlap_pickle):\n    #     print(\"Unpickling overlap matrix...\")\n    #     with open(overlap_pickle, \"rb\") as f:\n    #         self.overlap = pickle.load(f)\n    # else:\n    #     self.overlap = self.read_sparse_matrix_csr(overlap_file)\n    #     with open(overlap_pickle, \"wb\") as f:\n    #         pickle.dump(self.overlap, f)\n\n    # initialize atomic orbital data\n    self.num_orbitals_per_atom = [\n        np.sum(2 * np.array(orbital_type_dict[self.basis][species]) + 1)\n        for species in self.atomic_structure.get_chemical_symbols()\n    ]\n\n    unique_atomic_species = set(self.atomic_structure.get_chemical_symbols())\n    self.num_unique_orbitals = np.sum(\n        [\n            np.sum(2 * np.array(orbital_type_dict[self.basis][species]) + 1)\n            for species in unique_atomic_species\n        ]\n    )\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.init_electronic_structure_schnet","title":"init_electronic_structure_schnet","text":"<pre><code>init_electronic_structure_schnet(database_props)\n</code></pre> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def init_electronic_structure_schnet(self, database_props):\n    # initialize atomic orbital data\n    self.num_orbitals_per_atom = [\n        np.sum(2 * np.array(orbital_type_dict[self.basis][species]) + 1)\n        for species in self.atomic_structure.get_chemical_symbols()\n    ]\n    unique_atomic_species = set(self.atomic_structure.get_chemical_symbols())\n    self.num_unique_orbitals = np.sum(\n        [\n            np.sum(2 * np.array(orbital_type_dict[self.basis][species]) + 1)\n            for species in unique_atomic_species\n        ]\n    )\n\n    hamiltonian = database_props[\"hamiltonian\"]\n    # overlap = database_props[\"overlap\"]\n\n    # convert complex spherical harmonics to real spherical harmonics by permuting the order of p-orbitals\n    hamiltonian = self.ORCA_to_CP2K(hamiltonian)\n\n    hamiltonian_csr = csr_matrix(hamiltonian)\n    # overlap_csr = csr_matrix(overlap)\n\n    # check if hamiltonian_csr is symmetric\n    assert (hamiltonian_csr != hamiltonian_csr.T).nnz == 0\n\n    self.hamiltonian = self.csr_to_dict(hamiltonian_csr)\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.make_soap_features","title":"make_soap_features","text":"<pre><code>make_soap_features(pbc)\n</code></pre> <p>Make SOAP features for the atomic structure.</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def make_soap_features(self, pbc):\n    \"\"\"\n    Make SOAP features for the atomic structure.\n    \"\"\"\n\n    # Set up the SOAP descriptor\n    species = self.atomic_structure.get_chemical_symbols()\n    soap = SOAP(\n        species=species,\n        r_cut=7.0,\n        n_max=5,\n        l_max=5,\n        rbf=\"polynomial\",\n        periodic=pbc,\n        sparse=False,\n    )\n\n    # Get SOAP features\n    self.soap_features = soap.create(self.atomic_structure)\n    print(\"size of SOAP feature matrix: \", np.shape(self.soap_features))\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.map_atom_to_orbital","title":"map_atom_to_orbital","text":"<pre><code>map_atom_to_orbital(atom_index)\n</code></pre> <p>Map the atom index to the starting orbital index and the number of orbitals</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def map_atom_to_orbital(self, atom_index):\n    \"\"\"\n    Map the atom index to the starting orbital index and the number of orbitals\n    \"\"\"\n\n    starting_index = int(\n        np.sum(self.num_orbitals_per_atom[:atom_index]) + 1\n    )  # index where this atom's orbitals start in H and S\n    num_orbitals = self.num_orbitals_per_atom[\n        atom_index\n    ]  # number of orbitals for this atom\n\n    return starting_index, num_orbitals\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.partition_graph","title":"partition_graph","text":"<pre><code>partition_graph(n_clusters, write_xyz=False)\n</code></pre> <p>KMEANS: Partition the graph into <code>n_clusters</code> using K-means clustering.</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def partition_graph(self, n_clusters, write_xyz=False):\n    \"\"\"\n    KMEANS: Partition the graph into `n_clusters` using K-means clustering.\n    \"\"\"\n    # Create a NetworkX graph from the edge matrix\n    G = nx.Graph()\n    G.add_edges_from(self.edge_matrix.T)\n\n    # Convert the graph to an adjacency matrix\n    adj_matrix = nx.to_numpy_array(G)\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n    labels = kmeans.fit_predict(adj_matrix)\n\n    # Group nodes by their cluster\n    partitions = {i: np.where(labels == i)[0] for i in range(n_clusters)}\n\n    if write_xyz:\n        for _i, (cluster, subgraph_nodes) in enumerate(partitions.items()):\n            filename = \"cluster_\" + str(cluster) + \".xyz\"\n            utils.write_xyz_file(\n                filename,\n                self.atomic_structure.get_chemical_symbols(),\n                self.atomic_structure.get_positions(),\n                subgraph_nodes,\n            )\n\n    return partitions\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.read_matrix","title":"read_matrix","text":"<pre><code>read_matrix(file_path)\n</code></pre> <p>Read a matrix file and return the matrix in a dictionary format.</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def read_matrix(self, file_path):\n    \"\"\"\n    Read a matrix file and return the matrix in a dictionary format.\n    \"\"\"\n    matrix_data = {}\n\n    with Path(file_path).open() as f:\n        lines = f.readlines()\n        for line in lines:\n            data_str = line.strip().split()\n            if len(data_str) &gt;= 3:\n                indices = (int(data_str[0]), int(data_str[1]))\n                value = float(data_str[2])\n                matrix_data[indices] = value\n                # Assuming the matrix is symmetric, also add the transpose value\n                matrix_data[(indices[1], indices[0])] = value\n\n    return matrix_data\n</code></pre>"},{"location":"reference/augmented_partition/model/structure.html#augmented_partition.model.structure.Structure.read_sparse_matrix_csr","title":"read_sparse_matrix_csr","text":"<pre><code>read_sparse_matrix_csr(file_path)\n</code></pre> <p>Read a sparse matrix in CSR format from a file and return the matrix in a dictionary format.</p> Source code in <code>augmented_partition/model/structure.py</code> <pre><code>def read_sparse_matrix_csr(self, file_path):\n    \"\"\"\n    Read a sparse matrix in CSR format from a file and return the matrix in a dictionary format.\n    \"\"\"\n\n    # indptr = []\n    indices = []\n    data = []\n\n    print(\"reading file: \", file_path)\n    with Path(file_path).open() as f:\n        lines = f.readlines()\n        for line in lines:\n            data_str = line.strip().split()\n            if len(data_str) &gt;= 3:\n                indices.append([int(data_str[0]), int(data_str[1])])\n                data.append(float(data_str[2]))\n    csr_matrix = {}\n    for i in range(len(indices)):\n        csr_matrix[(indices[i][0], indices[i][1])] = data[i]\n        csr_matrix[(indices[i][1], indices[i][0])] = data[i]\n\n    return csr_matrix\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html","title":"training","text":""},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.CombinedLoss","title":"CombinedLoss","text":"<pre><code>CombinedLoss(eps=1e-16)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def __init__(self, eps=1e-16):\n    super().__init__()\n    self.mse = nn.MSELoss()\n    self.mae = nn.L1Loss()\n    self.eps = eps\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.CombinedLoss.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.CombinedLoss.mae","title":"mae  <code>instance-attribute</code>","text":"<pre><code>mae = L1Loss()\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.CombinedLoss.mse","title":"mse  <code>instance-attribute</code>","text":"<pre><code>mse = MSELoss()\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.CombinedLoss.forward","title":"forward","text":"<pre><code>forward(yhat, y)\n</code></pre> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def forward(self, yhat, y):\n    return torch.sqrt(self.mse(yhat, y) + self.eps) + self.mae(yhat, y)\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.RMSELoss","title":"RMSELoss","text":"<pre><code>RMSELoss(eps=1e-06)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def __init__(self, eps=1e-6):\n    super().__init__()\n    self.mse = nn.MSELoss()\n    self.eps = eps\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.RMSELoss.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.RMSELoss.mse","title":"mse  <code>instance-attribute</code>","text":"<pre><code>mse = MSELoss()\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.RMSELoss.forward","title":"forward","text":"<pre><code>forward(yhat, y)\n</code></pre> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def forward(self, yhat, y):\n    return torch.sqrt(self.mse(yhat, y) + self.eps)\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.compute_eigenvalues","title":"compute_eigenvalues","text":"<pre><code>compute_eigenvalues(base_path_lower, S_path, base_path_upper=None, symmetrize=False, save_file='model')\n</code></pre> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def compute_eigenvalues(\n    base_path_lower, S_path, base_path_upper=None, symmetrize=False, save_file=\"model\"\n):\n    # Load the data\n    S = np.loadtxt(S_path)\n\n    S_row_ind = S[:, 0].astype(np.int32) - 1\n    S_col_ind = S[:, 1].astype(np.int32) - 1\n    S_data = S[:, 2]\n\n    S_matrix = sp.coo_matrix((S_data, (S_row_ind, S_col_ind)))\n\n    H_lower_diagonal_name = base_path_lower\n\n    H = np.loadtxt(H_lower_diagonal_name)\n\n    H_row_ind = H[:, 0].astype(np.int32) - 1\n    H_col_ind = H[:, 1].astype(np.int32) - 1\n    H_data = H[:, 2]\n\n    H_matrix = sp.coo_matrix((H_data, (H_row_ind, H_col_ind)))\n    H_matrix = H_matrix.toarray()\n    tmp = H_matrix.conj().T.copy()\n    # set diagonal to zero\n    np.fill_diagonal(tmp, 0)\n    H_lower_matrix = H_matrix + tmp\n\n    assert np.allclose(H_lower_matrix, H_lower_matrix.conj().T)\n\n    if symmetrize:\n        assert base_path_upper is not None\n        H_upper_diagonal_name = base_path_upper\n\n        H = np.loadtxt(H_upper_diagonal_name)\n\n        H_row_ind = H[:, 0].astype(np.int32) - 1\n        H_col_ind = H[:, 1].astype(np.int32) - 1\n        H_data = H[:, 2]\n\n        H_matrix = sp.coo_matrix((H_data, (H_row_ind, H_col_ind)))\n        H_matrix = H_matrix.toarray()\n        tmp = H_matrix.conj().T.copy()\n        # set diagonal to zero\n        np.fill_diagonal(tmp, 0)\n        H_upper_matrix = H_matrix + tmp\n\n        assert np.allclose(H_upper_matrix, H_upper_matrix.conj().T)\n\n        H_full_matrix = (H_lower_matrix + H_upper_matrix) / 2\n\n    else:\n        H_full_matrix = H_lower_matrix\n\n    S_matrix = S_matrix.toarray()\n    tmp = S_matrix.conj().T.copy()\n    # set diagonal to zero\n    np.fill_diagonal(tmp, 0)\n    S_matrix = S_matrix + tmp\n\n    assert np.allclose(H_full_matrix, H_full_matrix.conj().T)\n    assert np.allclose(S_matrix, S_matrix.conj().T)\n\n    start = time.time()\n    w, v = scipy.linalg.eigh(H_full_matrix, S_matrix, lower=True)\n    end = time.time()\n\n    print(\"Time: \", end - start)\n\n    # save the eigenvalues and eigenvectors\n    np.save(save_file + \"eigenvalues.npy\", w)\n    np.save(save_file + \"eigenvectors.npy\", v)\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.create_test_info","title":"create_test_info","text":"<pre><code>create_test_info(model, test_batch, construct_kernel, equivariant_blocks, atom_orbitals, out_slices, device, save_file='model_in_training.pth')\n</code></pre> <p>Evaluate the model on the test set and return the mean absolute error for the node and edge predictions after reconstructing the Hamiltonian matrices from the predictions.</p> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def create_test_info(\n    model,\n    test_batch,\n    construct_kernel,\n    equivariant_blocks,\n    atom_orbitals,\n    out_slices,\n    device,\n    save_file=\"model_in_training.pth\",\n):\n    \"\"\"\n    Evaluate the model on the test set and return the mean absolute error for the node and edge predictions after reconstructing the Hamiltonian matrices from the predictions.\n\n    \"\"\"\n    test_batch_dev = test_batch.to(device)\n    test_node, test_edge = model(test_batch_dev)\n\n    test_info = {}\n\n    # test_node = test_node\n    # test_edge = test_edge\n\n    if torch.is_tensor(test_batch_dev.labelled_node_size):\n        labelled_node_size = test_batch_dev.labelled_node_size.item()\n        labelled_edge_size = test_batch_dev.labelled_edge_size.item()  # when test batch is loaded from dataloader, it is a tensor instead of a number\n\n    else:\n        labelled_node_size = test_batch_dev.labelled_node_size\n        labelled_edge_size = test_batch_dev.labelled_edge_size\n\n    flattened_node_labels = construct_kernel.get_H(\n        test_batch_dev.node_y[0:labelled_node_size]\n    )  # convert into flattened Hamiltonian form\n    flattened_node_pred = construct_kernel.get_H(test_node[0:labelled_node_size])\n\n    onsite_edge_index = torch.cat(\n        (\n            torch.arange(labelled_node_size).unsqueeze(0),\n            torch.arange(labelled_node_size).unsqueeze(0),\n        ),\n        0,\n    )\n    numbers = test_batch_dev.x[0 : test_batch_dev.labelled_node_size]\n\n    node_label = utils.unflatten(\n        flattened_node_labels,\n        numbers,\n        onsite_edge_index,\n        equivariant_blocks,\n        atom_orbitals,\n        out_slices,\n    )\n    node_pred = utils.unflatten(\n        flattened_node_pred,\n        numbers,\n        onsite_edge_index,\n        equivariant_blocks,\n        atom_orbitals,\n        out_slices,\n    )\n\n    H_block_node_labels = [matrix.flatten() for matrix in node_label.values()]\n    node_label_tensor = torch.cat(H_block_node_labels)\n\n    H_block_node_pred = [matrix.flatten() for matrix in node_pred.values()]\n    node_pred_tensor = torch.cat(H_block_node_pred)\n\n    test_info[\"flattened_node_labels\"] = flattened_node_labels\n    test_info[\"flattened_node_pred\"] = flattened_node_pred\n    test_info[\"node_label\"] = node_label_tensor\n    test_info[\"node_pred\"] = node_pred_tensor\n\n    flattened_edge_labels = construct_kernel.get_H(\n        test_batch_dev.y[0:labelled_edge_size]\n    )\n    flattened_edge_pred = construct_kernel.get_H(test_edge[0:labelled_edge_size])\n\n    edge_label = utils.unflatten(\n        flattened_edge_labels,\n        numbers,\n        test_batch_dev.edge_index[:, 0:labelled_edge_size],\n        equivariant_blocks,\n        atom_orbitals,\n        out_slices,\n    )\n    edge_pred = utils.unflatten(\n        flattened_edge_pred,\n        numbers,\n        test_batch_dev.edge_index[:, 0:labelled_edge_size],\n        equivariant_blocks,\n        atom_orbitals,\n        out_slices,\n    )\n\n    H_block_edge_labels = [matrix.flatten() for matrix in edge_label.values()]\n    edge_label_tensor = torch.cat(H_block_edge_labels)\n\n    H_block_edge_pred = [matrix.flatten() for matrix in edge_pred.values()]\n    edge_pred_tensor = torch.cat(H_block_edge_pred)\n\n    test_info[\"flattened_edge_labels\"] = flattened_edge_labels\n    test_info[\"flattened_edge_pred\"] = flattened_edge_pred\n    test_info[\"edge_label\"] = edge_label_tensor\n    test_info[\"edge_pred\"] = edge_pred_tensor\n\n    torch.save(test_info, save_file + \"_test_info.pt\")\n\n    MAE_node = torch.mean(torch.abs(node_label_tensor - node_pred_tensor))\n    MAE_edge = torch.mean(torch.abs(edge_label_tensor - edge_pred_tensor))\n\n    pred_tensor = torch.cat([node_pred_tensor, edge_pred_tensor])\n    label_tensor = torch.cat([node_label_tensor, edge_label_tensor])\n    MAEloss_total = torch.mean(torch.abs(pred_tensor - label_tensor))\n\n    print(\"Mean Absolute Node Error in mHartree: \", MAE_node * 1000)\n    print(\n        \"Node Standard Deviation: \",\n        torch.std(torch.abs(node_label_tensor - node_pred_tensor)) * 1000,\n    )\n    print(\"Mean Absolute Edge Error in mHartree: \", MAE_edge * 1000)\n    print(\n        \"Edge Standard Deviation: \",\n        torch.std(torch.abs(edge_label_tensor - edge_pred_tensor)) * 1000,\n    )\n    print(\"Mean Total Error:\", MAEloss_total * 1000)\n\n    return MAE_node, MAE_edge\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.evaluate_batch","title":"evaluate_batch","text":"<pre><code>evaluate_batch(model, test_data_loader, construct_kernel, equivariant_blocks, atom_orbitals, out_slices, device)\n</code></pre> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def evaluate_batch(\n    model,\n    test_data_loader,\n    construct_kernel,\n    equivariant_blocks,\n    atom_orbitals,\n    out_slices,\n    device,\n):\n    total_length = 0\n    total_node_loss = 0\n    total_edge_loss = 0\n    total_loss = 0\n\n    for batch in test_data_loader:\n        model.eval()\n        if dist.is_available() and dist.is_initialized():\n            # find_unused_parameters=True handles the cases where some parameters dont recieve gradients, such as the directed ones\n            model = nn.parallel.DistributedDataParallel(\n                model,\n                device_ids=[device],\n                output_device=device,\n                find_unused_parameters=True,\n            )\n\n        with torch.no_grad():\n            test_batch = batch.to(device)\n\n            # Forward pass\n            test_node, test_edge = model(test_batch)\n            # print(\"--&gt; Memory allocated: \" + str(torch.cuda.memory_allocated(device)/1e9) + \"GB\")\n            # torch.cuda.synchronize()\n            test_node = test_node.cpu()\n            test_edge = test_edge.cpu()\n\n            # if test_batch.labelled_node_size.item() exists, use it, otherwise use the total number of nodes\n            if hasattr(test_batch, \"labelled_node_size\"):\n                labelled_node_size = test_batch.labelled_node_size.item()\n                labelled_edge_size = test_batch.labelled_edge_size.item()\n            else:\n                batch_size = len(test_batch)\n                labelled_node_size = test_batch[0].num_nodes * batch_size\n                labelled_edge_size = test_batch[0].num_edges * batch_size\n\n            arange_tensor = torch.arange(labelled_node_size).unsqueeze(0)\n            onsite_edges = torch.cat((arange_tensor, arange_tensor), 0)\n\n            # Process node predictions\n            flattened_node_labels = construct_kernel.get_H(\n                test_batch.node_y[0:labelled_node_size].cpu()\n            )\n            flattened_node_pred = construct_kernel.get_H(\n                test_node[:labelled_node_size].cpu()\n            )\n\n            node_label = utils.unflatten(\n                flattened_node_labels,\n                test_batch.x[0:labelled_node_size],\n                onsite_edges,\n                equivariant_blocks,\n                atom_orbitals,\n                out_slices,\n            )\n\n            node_pred = utils.unflatten(\n                flattened_node_pred,\n                test_batch.x[0:labelled_node_size],\n                onsite_edges,\n                equivariant_blocks,\n                atom_orbitals,\n                out_slices,\n            )\n\n            H_block_node_labels = [matrix.flatten() for matrix in node_label.values()]\n            node_label_tensor = torch.cat(H_block_node_labels)\n            H_block_node_pred = [matrix.flatten() for matrix in node_pred.values()]\n            node_pred_tensor = torch.cat(H_block_node_pred)\n\n            # Process edge predictions\n            flattened_edge_labels = construct_kernel.get_H(\n                test_batch.y[0:labelled_edge_size].cpu()\n            )\n            flattened_edge_pred = construct_kernel.get_H(\n                test_edge[0:labelled_edge_size].cpu()\n            )\n\n            edge_label = utils.unflatten(\n                flattened_edge_labels,\n                test_batch.x[0:labelled_node_size],\n                test_batch.edge_index[:, 0:labelled_edge_size],\n                equivariant_blocks,\n                atom_orbitals,\n                out_slices,\n            )\n\n            edge_pred = utils.unflatten(\n                flattened_edge_pred,\n                test_batch.x[0:labelled_node_size],\n                test_batch.edge_index[:, 0:labelled_edge_size],\n                equivariant_blocks,\n                atom_orbitals,\n                out_slices,\n            )\n\n            H_block_edge_labels = [matrix.flatten() for matrix in edge_label.values()]\n            edge_label_tensor = torch.cat(H_block_edge_labels)\n            H_block_edge_pred = [matrix.flatten() for matrix in edge_pred.values()]\n            edge_pred_tensor = torch.cat(H_block_edge_pred)\n\n            # Compute the MAE\n            pred_tensor = torch.cat([node_pred_tensor, edge_pred_tensor])\n            label_tensor = torch.cat([node_label_tensor, edge_label_tensor])\n\n            node_MAE = torch.mean(torch.abs(node_pred_tensor - node_label_tensor))\n            edge_MAE = torch.mean(torch.abs(edge_pred_tensor - edge_label_tensor))\n            MAEloss = torch.mean(torch.abs(pred_tensor - label_tensor))\n\n            print(\n                \"Mean Absolute Node Error in mHartree: \",\n                torch.mean(torch.abs(node_pred_tensor - node_label_tensor)) * 1e3,\n            )\n            print(\n                \"Mean Absolute Edge Error in mHartree: \",\n                torch.mean(torch.abs(edge_pred_tensor - edge_label_tensor)) * 1e3,\n            )\n            print(\"Mean Absolute Error in mHartree: \", MAEloss * 1e3)\n\n            model.train()  # Set the model back to training mode\n\n            total_length += len(test_batch)\n            print(len(test_batch), \" samples in the batch\")\n\n            total_node_loss += node_MAE * len(test_batch)\n            total_edge_loss += edge_MAE * len(test_batch)\n            total_loss += MAEloss * len(test_batch)\n\n    # Average the losses over all batches\n    averaged_total_loss = total_loss / total_length\n    averaged_node_loss = total_node_loss / total_length\n    averaged_edge_loss = total_edge_loss / total_length\n\n    print(\"Total Node Loss: \", averaged_node_loss * 1e3)\n    print(\"Total Edge Loss: \", averaged_edge_loss * 1e3)\n    print(\"Total Loss: \", averaged_total_loss * 1e3)\n    print(\"Evaluation completed\")\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(model, test_batch, construct_kernel, equivariant_blocks, atom_orbitals, out_slices, device, save_file='model_in_training.pth', reconstruct_ham=True, compute_total_loss=True, plot=True, lower_triangular=False, H_filter=False, S_filter_value=None)\n</code></pre> <p>Evaluate the model on the test set and return the mean absolute error for the node and edge predictions. Also reconstructs the Hamiltonian matrices from the predictions.</p> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def evaluate_model(\n    model,\n    test_batch,\n    construct_kernel,\n    equivariant_blocks,\n    atom_orbitals,\n    out_slices,\n    device,\n    save_file=\"model_in_training.pth\",\n    reconstruct_ham=True,\n    compute_total_loss=True,\n    plot=True,\n    lower_triangular=False,\n    H_filter=False,\n    S_filter_value=None,\n):\n    \"\"\"\n    Evaluate the model on the test set and return the mean absolute error for the node and edge predictions. Also reconstructs the Hamiltonian matrices from the predictions.\n\n    \"\"\"\n    # MPI Initialization\n    comm = MPI.COMM_WORLD\n    rank = comm.Get_rank()\n    size = comm.Get_size()\n\n    test_batch = test_batch.to(device)\n    test_node, test_edge = model(test_batch)\n    global_edge_index = test_batch.edge_index\n\n    # test_node = test_node.cpu()\n    # test_edge = test_edge.cpu()\n\n    if torch.is_tensor(test_batch.labelled_node_size):\n        labelled_node_size = test_batch.labelled_node_size.item()\n        labelled_edge_size = test_batch.labelled_edge_size.item()  # when test batch is loaded from dataloader, it is a tensor instead of a number\n\n    else:\n        labelled_node_size = test_batch.labelled_node_size\n        labelled_edge_size = test_batch.labelled_edge_size\n\n    labelled_edge_index = global_edge_index[0:labelled_edge_size]\n\n    # \ud83d\udd39 Distribute work across ranks\n    node_indices_split = np.array_split(np.arange(labelled_node_size), size)\n    edge_indices_split = np.array_split(np.arange(labelled_edge_size), size)\n\n    local_node_indices = node_indices_split[rank]  # Nodes assigned to this rank\n    local_edge_indices = edge_indices_split[rank]  # Edges assigned to this rank\n\n    if S_filter_value is not None and test_batch.S_input is not None:\n        print(\"Filtering edges based on S matrix\")\n\n        local_S_matrix = test_batch.S_input[\n            local_edge_indices\n        ]  # filter out edges where S is too small\n\n        S_means = torch.mean(torch.abs(local_S_matrix), dim=1)\n        S_mask = S_means &gt; S_filter_value\n        print(local_edge_indices.shape)\n        local_edge_indices = local_edge_indices[S_mask.cpu().numpy()]\n\n        print(local_edge_indices.shape)\n\n    # \ud83d\udd39 Distribute labelled_atom_index and labelled_edge_index\n    local_labelled_atom_index = local_node_indices\n    local_labelled_edge_index = labelled_edge_index[\n        :, local_edge_indices\n    ]  # Shape (2, num_local_edges)\n    numbers = test_batch.x[0:labelled_node_size]\n\n    # Extract local predicted data for this rank\n    local_test_node = test_node[local_node_indices]\n    local_test_edge = test_edge[local_edge_indices]\n\n    if H_filter is True:\n        print(\"Filtering elements based on non-zero elements of label H matrix\")\n\n        local_label_node = test_batch.node_y[local_node_indices]\n        local_label_edge = test_batch.y[local_edge_indices]\n\n        local_test_node[torch.abs(local_label_node) &lt; 1e-6] = 0\n        local_test_edge[torch.abs(local_label_edge) &lt; 1e-6] = 0\n\n    # \ud83d\udd39 Compute local flattened Hamiltonians\n    local_flattened_node_pred = construct_kernel.get_H(local_test_node)\n    local_flattened_edge_pred = construct_kernel.get_H(local_test_edge)\n    onsite_edge_index = torch.cat(\n        (\n            torch.tensor(local_labelled_atom_index).unsqueeze(0),\n            torch.tensor(local_labelled_atom_index).unsqueeze(0),\n        ),\n        0,\n    )\n    # \ud83d\udd39 Compute local dictionaries using unflatten\n    local_node_pred_dic = utils.unflatten(\n        local_flattened_node_pred,\n        numbers,\n        onsite_edge_index,\n        equivariant_blocks,\n        atom_orbitals,\n        out_slices,\n    )\n\n    local_edge_pred_dic = utils.unflatten(\n        local_flattened_edge_pred,\n        numbers,\n        local_labelled_edge_index,\n        equivariant_blocks,\n        atom_orbitals,\n        out_slices,\n    )\n\n    if compute_total_loss is True:\n        # Extract local predicted data for this rank\n        local_label_node = test_batch.node_y[local_node_indices]\n        local_label_edge = test_batch.y[local_edge_indices]\n\n        # \ud83d\udd39 Compute local flattened Hamiltonians\n        local_flattened_node_labels = construct_kernel.get_H(local_label_node)\n        local_flattened_edge_labels = construct_kernel.get_H(local_label_edge)\n\n        # \ud83d\udd39 Compute local dictionaries using unflatten\n        local_node_label_dic = utils.unflatten(\n            local_flattened_node_labels,\n            numbers,\n            onsite_edge_index,\n            equivariant_blocks,\n            atom_orbitals,\n            out_slices,\n        )\n\n        local_edge_label_dic = utils.unflatten(\n            local_flattened_edge_labels,\n            numbers,\n            local_labelled_edge_index,  # Use local_labelled_edge_index\n            equivariant_blocks,\n            atom_orbitals,\n            out_slices,\n        )\n\n        H_block_node_labels = [\n            matrix.flatten() for matrix in local_node_label_dic.values()\n        ]\n        node_label_tensor = torch.cat(H_block_node_labels)\n\n        H_block_node_pred = [\n            matrix.flatten() for matrix in local_node_pred_dic.values()\n        ]\n        node_pred_tensor = torch.cat(H_block_node_pred)\n\n        H_block_edge_labels = [\n            matrix.flatten() for matrix in local_edge_label_dic.values()\n        ]\n        edge_label_tensor = torch.cat(H_block_edge_labels)\n\n        H_block_edge_pred = [\n            matrix.flatten() for matrix in local_edge_pred_dic.values()\n        ]\n        edge_pred_tensor = torch.cat(H_block_edge_pred)\n\n        MAE_node = torch.mean(torch.abs(node_label_tensor - node_pred_tensor))\n        MAE_edge = torch.mean(torch.abs(edge_label_tensor - edge_pred_tensor))\n\n        print(\"Mean Absolute Local Node Error in mHartree: \", MAE_node * 1e3)\n        print(\"Mean Absolute Local Edge Error in mHartree: \", MAE_edge * 1e3)\n\n        all_node_labels = comm.gather(node_label_tensor, root=0)\n        all_node_preds = comm.gather(node_pred_tensor, root=0)\n        all_edge_labels = comm.gather(edge_label_tensor, root=0)\n        all_edge_preds = comm.gather(edge_pred_tensor, root=0)\n\n        if rank == 0:\n            all_pred_tensor = torch.cat(\n                [torch.cat(all_node_preds), torch.cat(all_edge_preds)]\n            )\n            all_label_tensor = torch.cat(\n                [torch.cat(all_node_labels), torch.cat(all_edge_labels)]\n            )\n\n            node_error = torch.mean(\n                torch.abs(torch.cat(all_node_labels) - torch.cat(all_node_preds))\n            )\n            edge_error = torch.mean(\n                torch.abs(torch.cat(all_edge_labels) - torch.cat(all_edge_preds))\n            )\n            total_error = torch.mean(torch.abs(all_label_tensor - all_pred_tensor))\n\n            print(\"Mean Absolute Node Error in mHartree: \", node_error * 1e3)\n            print(\"Mean Absolute Edge Error in mHartree: \", edge_error * 1e3)\n            print(\"Mean Absolute Error in mHartree: \", total_error * 1e3)\n\n            if plot is True:\n                print(\"Plotting\")\n                plt.figure(figsize=(4, 3))\n                plt.scatter(\n                    torch.cat(all_edge_labels).detach().numpy(),\n                    torch.cat(all_edge_preds).detach().numpy(),\n                    s=1,\n                    alpha=0.5,\n                    edgecolor=\"none\",\n                    color=\"crimson\",\n                    label=\"Edge\",\n                )\n                plt.scatter(\n                    torch.cat(all_node_labels).detach().numpy(),\n                    torch.cat(all_node_preds).detach().numpy(),\n                    s=1,\n                    alpha=0.5,\n                    edgecolor=\"none\",\n                    color=\"blue\",\n                    label=\"Node\",\n                )\n                plt.plot(\n                    torch.cat(all_node_labels).detach().numpy(),\n                    torch.cat(all_node_labels).detach().numpy(),\n                    c=\"k\",\n                    linestyle=\"dashed\",\n                    linewidth=0.1,\n                    alpha=0.3,\n                )\n                plt.xlabel(\"Real $H_{ij}$\")\n                plt.ylabel(\"Predicted  $H_{ij}$\")\n                plt.legend()\n                # plt.text(0.5, 0.1, 'Node loss = '+str(MAE_node.item())+', Edge loss = '+str(MAE_edge.item()), fontsize=5, transform=plt.gca().transAxes)\n                plt.savefig(save_file + \".png\", dpi=300, bbox_inches=\"tight\")\n                plt.close()\n\n    if reconstruct_ham is True:\n        pred_dic = local_node_pred_dic.copy()\n        pred_dic.update(local_edge_pred_dic)\n        reconstruct_hamiltonian(\n            pred_dic,\n            numbers,\n            comm,\n            rank,\n            atom_orbitals,\n            save_file=save_file,\n            lower_triangular=lower_triangular,\n        )\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.evaluate_slice","title":"evaluate_slice","text":"<pre><code>evaluate_slice(model, data_loader, construct_kernel, equivariant_blocks, atom_orbitals, out_slices, device, save_file='./')\n</code></pre> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def evaluate_slice(\n    model,\n    data_loader,\n    construct_kernel,\n    equivariant_blocks,\n    atom_orbitals,\n    out_slices,\n    device,\n    save_file=\"./\",\n):\n    model.eval()\n    all_node_labels = []\n    all_node_preds = []\n    all_edge_labels = []\n    all_edge_preds = []\n\n    # currently only testing on a single rank with 1 batch, need to fix for multiple ranks and batches\n    # all examples are set up with 1 batch\n    assert len(data_loader) == 1\n\n    if dist.is_available() and dist.is_initialized():\n        # find_unused_parameters=True handles the cases where some parameters dont recieve gradients, such as the directed ones\n        model = nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[device],\n            output_device=device,\n            find_unused_parameters=True,\n        )\n\n    with torch.no_grad():\n        MAEloss_total = 0.0\n\n        for i, test_batch in enumerate(data_loader):\n            print(f\"Loading batch {i}/{len(data_loader)}...\")\n            test_batch_dev = test_batch.to(device)\n\n            # Forward pass\n            test_node, test_edge = model(test_batch_dev)\n            # print(\"--&gt; Memory allocated: \" + str(torch.cuda.memory_allocated(device)/1e9) + \"GB\")\n            # torch.cuda.synchronize()\n            test_node = test_node.cpu()\n            test_edge = test_edge.cpu()\n\n            # if test_batch.labelled_node_size.item() exists, use it, otherwise use the total number of nodes\n            if hasattr(test_batch_dev, \"labelled_node_size\"):\n                labelled_node_size = test_batch_dev.labelled_node_size.item()\n                labelled_edge_size = test_batch_dev.labelled_edge_size.item()\n            else:\n                batch_size = len(test_batch_dev)\n                labelled_node_size = test_batch_dev[0].num_nodes * batch_size\n                labelled_edge_size = test_batch_dev[0].num_edges * batch_size\n\n            arange_tensor = torch.arange(labelled_node_size).unsqueeze(0)\n            onsite_edges = torch.cat((arange_tensor, arange_tensor), 0)\n\n            # Process node predictions\n            flattened_node_labels = construct_kernel.get_H(\n                test_batch_dev.node_y[0:labelled_node_size].cpu()\n            )\n            flattened_node_pred = construct_kernel.get_H(\n                test_node[:labelled_node_size].cpu()\n            )\n\n            node_label = utils.unflatten(\n                flattened_node_labels,\n                test_batch_dev.x[0:labelled_node_size],\n                onsite_edges,\n                equivariant_blocks,\n                atom_orbitals,\n                out_slices,\n            )\n\n            node_pred = utils.unflatten(\n                flattened_node_pred,\n                test_batch_dev.x[0:labelled_node_size],\n                onsite_edges,\n                equivariant_blocks,\n                atom_orbitals,\n                out_slices,\n            )\n\n            H_block_node_labels = [matrix.flatten() for matrix in node_label.values()]\n            node_label_tensor = torch.cat(H_block_node_labels)\n            H_block_node_pred = [matrix.flatten() for matrix in node_pred.values()]\n            node_pred_tensor = torch.cat(H_block_node_pred)\n\n            # Process edge predictions\n            flattened_edge_labels = construct_kernel.get_H(\n                test_batch_dev.y[0:labelled_edge_size].cpu()\n            )\n            flattened_edge_pred = construct_kernel.get_H(\n                test_edge[0:labelled_edge_size].cpu()\n            )\n\n            edge_label = utils.unflatten(\n                flattened_edge_labels,\n                test_batch_dev.x[0:labelled_node_size],\n                test_batch_dev.edge_index[:, 0:labelled_edge_size],\n                equivariant_blocks,\n                atom_orbitals,\n                out_slices,\n            )\n\n            edge_pred = utils.unflatten(\n                flattened_edge_pred,\n                test_batch_dev.x[0:labelled_node_size],\n                test_batch_dev.edge_index[:, 0:labelled_edge_size],\n                equivariant_blocks,\n                atom_orbitals,\n                out_slices,\n            )\n\n            H_block_edge_labels = [matrix.flatten() for matrix in edge_label.values()]\n            edge_label_tensor = torch.cat(H_block_edge_labels)\n            H_block_edge_pred = [matrix.flatten() for matrix in edge_pred.values()]\n            edge_pred_tensor = torch.cat(H_block_edge_pred)\n\n            # Compute the MAE\n            pred_tensor = torch.cat([node_pred_tensor, edge_pred_tensor])\n            label_tensor = torch.cat([node_label_tensor, edge_label_tensor])\n            MAEloss_total += torch.mean(torch.abs(pred_tensor - label_tensor))\n\n            print(\n                \"Mean Absolute Node Error in mHartree: \",\n                torch.mean(torch.abs(node_pred_tensor - node_label_tensor)) * 1e3,\n            )\n            print(\n                \"Mean Absolute Edge Error in mHartree: \",\n                torch.mean(torch.abs(edge_pred_tensor - edge_label_tensor)) * 1e3,\n            )\n            print(\"Mean Absolute Error in mHartree: \", MAEloss_total * 1e3)\n\n            # Collect results for plotting\n            all_node_labels.append(node_label_tensor)\n            all_node_preds.append(node_pred_tensor)\n            all_edge_labels.append(edge_label_tensor)\n            all_edge_preds.append(edge_pred_tensor)\n\n            # local_rank = (\n            #     dist.get_rank() if dist.is_available() and dist.is_initialized() else 0\n            # )\n            # with open(save_file + '_MAE_rank_' + str(local_rank) + '_batch_' + str(i) + '_size_' + str(len(data_loader)) + '.txt', 'w') as f:\n            #     f.write(f\"Mean Absolute Node Error in mHartree: {torch.mean(torch.abs(node_pred_tensor - node_label_tensor)) * 1e3}\\n\")\n            #     f.write(f\"Mean Absolute Edge Error in mHartree: {torch.mean(torch.abs(edge_pred_tensor - edge_label_tensor)) * 1e3}\\n\")\n            #     f.write(f\"Mean Absolute Error in mHartree: {MAEloss_total * 1e3}\\n\")\n\n            # Clear cache after processing each batch\n            del (\n                test_node,\n                test_edge,\n                test_batch,\n                node_label,\n                node_pred,\n                edge_label,\n                edge_pred,\n            )  # , node_label_tensor, node_pred_tensor, edge_label_tensor, edge_pred_tensor\n            del (\n                flattened_node_labels,\n                flattened_node_pred,\n                flattened_edge_labels,\n                flattened_edge_pred,\n                H_block_node_labels,\n                H_block_node_pred,\n                H_block_edge_labels,\n                H_block_edge_pred,\n            )\n            del pred_tensor, label_tensor\n            # torch.cuda.empty_cache()\n            gc.collect()  # Python garbage collection\n            # torch.cuda.synchronize()\n            # print(\"--&gt; Memory allocated (after gc): \" + str(torch.cuda.memory_allocated(device)/1e9) + \"GB\")\n\n        # average the loss over all batches\n        MAEloss_total = MAEloss_total / len(data_loader)\n\n    # Concatenate all results\n    all_node_labels = torch.cat(all_node_labels)\n    all_node_preds = torch.cat(all_node_preds)\n    all_edge_labels = torch.cat(all_edge_labels)\n    all_edge_preds = torch.cat(all_edge_preds)\n\n    # * Note: testing is always done on a single rank with 1 batch so far, need to modify this for multiple ranks and batches\n    # collect the loss from all ranks:\n    if dist.is_available() and dist.is_initialized():\n        dist.all_reduce(\n            torch.tensor(MAEloss_total, device=device), op=dist.ReduceOp.SUM\n        )\n\n    # local_rank = dist.get_rank() if dist.is_available() and dist.is_initialized() else 0\n\n    # downsample for plotting:\n    downsample = 100\n    all_node_labels = all_node_labels[::downsample]\n    all_node_preds = all_node_preds[::downsample]\n    all_edge_labels = all_edge_labels[::downsample]\n    all_edge_preds = all_edge_preds[::downsample]\n\n    # Plotting\n    plt.figure(figsize=(4, 3))\n    plt.scatter(\n        all_edge_labels.cpu().numpy(),\n        all_edge_preds.cpu().numpy(),\n        s=1,\n        alpha=0.5,\n        edgecolor=\"none\",\n        color=\"crimson\",\n        label=\"Edge\",\n    )\n    plt.scatter(\n        all_node_labels.cpu().numpy(),\n        all_node_preds.cpu().numpy(),\n        s=1,\n        alpha=0.5,\n        edgecolor=\"none\",\n        color=\"blue\",\n        label=\"Node\",\n    )\n    plt.plot(\n        all_node_labels.cpu().numpy(),\n        all_node_labels.cpu().numpy(),\n        c=\"k\",\n        linestyle=\"dashed\",\n        linewidth=0.1,\n        alpha=0.3,\n    )\n    plt.xlabel(r\"$(H_{ij})_{\\alpha \\beta}^{GT}$\")\n    plt.ylabel(r\"$(H_{ij})_{\\alpha \\beta}^{pred}$\")\n    plt.legend()\n    plt.savefig(save_file + \"_prediction.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.get_loss_flattened","title":"get_loss_flattened","text":"<pre><code>get_loss_flattened(node_output, edge_output, batch, criterion)\n</code></pre> <p>Process a batch of data (forward pass + loss) for labels and targets in the uncoupled basis</p> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def get_loss_flattened(node_output, edge_output, batch, criterion):\n    \"\"\"\n    Process a batch of data (forward pass + loss) for labels and targets in the uncoupled basis\n    \"\"\"\n\n    if hasattr(batch, \"labelled_node_size\"):\n        labelled_node_size = batch.labelled_node_size.item()\n        labelled_edge_size = batch.labelled_edge_size.item()\n    else:\n        batch_size = len(batch)\n        labelled_node_size = batch[0].num_nodes * batch_size\n        labelled_edge_size = batch[0].num_edges * batch_size\n\n    # Compute the loss\n    loss_node = criterion(\n        node_output[0:labelled_node_size], batch.node_y[0:labelled_node_size]\n    )  # node_y is the node label\n    loss_edge = criterion(\n        edge_output[0:labelled_edge_size], batch.y[0:labelled_edge_size]\n    )  # y is the edge label\n    output = torch.cat(\n        [node_output[0:labelled_node_size], edge_output[0:labelled_edge_size]], dim=0\n    )\n    labels = torch.cat(\n        [batch.node_y[0:labelled_node_size], batch.y[0:labelled_edge_size]], dim=0\n    )\n    loss = criterion(output, labels)\n\n    return loss_node, loss_edge, loss\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.get_loss_unflattened","title":"get_loss_unflattened","text":"<pre><code>get_loss_unflattened(node_output, edge_output, batch, criterion, construct_kernel, equivariant_blocks, atom_orbitals, out_slices)\n</code></pre> <p>Process a batch of data (forward pass + loss) for labels and targets in the coupled basis</p> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def get_loss_unflattened(\n    node_output,\n    edge_output,\n    batch,\n    criterion,\n    construct_kernel,\n    equivariant_blocks,\n    atom_orbitals,\n    out_slices,\n):\n    \"\"\"\n    Process a batch of data (forward pass + loss) for labels and targets in the coupled basis\n    \"\"\"\n\n    if hasattr(batch, \"labelled_node_size\"):\n        labelled_node_size = batch.labelled_node_size.item()\n        labelled_edge_size = batch.labelled_edge_size.item()\n    else:\n        batch_size = len(batch)\n        labelled_node_size = batch[0].num_nodes * batch_size\n        labelled_edge_size = batch[0].num_edges * batch_size\n\n    arange_tensor = torch.arange(labelled_node_size).unsqueeze(0)\n    torch_cat_tensor = torch.cat(\n        (arange_tensor, arange_tensor), 0\n    )  # edge_index for self-loop (nodes)\n\n    # Process node predictions\n    flattened_node_labels = construct_kernel.get_H(batch.node_y[0:labelled_node_size])\n    flattened_node_pred = construct_kernel.get_H(node_output[:labelled_node_size])\n\n    node_label = utils.unflatten(\n        flattened_node_labels,\n        batch.x[0:labelled_node_size],\n        torch_cat_tensor,\n        equivariant_blocks,\n        atom_orbitals,\n        out_slices,\n    )\n\n    node_pred = utils.unflatten(\n        flattened_node_pred,\n        batch.x[0:labelled_node_size],\n        torch_cat_tensor,\n        equivariant_blocks,\n        atom_orbitals,\n        out_slices,\n    )\n\n    node_label_tensor = torch.cat([matrix.flatten() for matrix in node_label.values()])\n    node_pred_tensor = torch.cat([matrix.flatten() for matrix in node_pred.values()])\n\n    # Process edge predictions\n    flattened_edge_labels = construct_kernel.get_H(batch.y[0:labelled_edge_size])\n    flattened_edge_pred = construct_kernel.get_H(edge_output[0:labelled_edge_size])\n\n    edge_label = utils.unflatten(\n        flattened_edge_labels,\n        batch.x[0:labelled_node_size],\n        batch.edge_index[:, 0:labelled_edge_size],\n        equivariant_blocks,\n        atom_orbitals,\n        out_slices,\n    )\n\n    edge_pred = utils.unflatten(\n        flattened_edge_pred,\n        batch.x[0:labelled_node_size],\n        batch.edge_index[:, 0:labelled_edge_size],\n        equivariant_blocks,\n        atom_orbitals,\n        out_slices,\n    )\n\n    edge_label_tensor = torch.cat([matrix.flatten() for matrix in edge_label.values()])\n    edge_pred_tensor = torch.cat([matrix.flatten() for matrix in edge_pred.values()])\n\n    # Compute the loss\n    loss_node = criterion(node_pred_tensor, node_label_tensor)\n    loss_edge = criterion(edge_pred_tensor, edge_label_tensor)\n    pred_tensor = torch.cat([node_pred_tensor, edge_pred_tensor])\n    label_tensor = torch.cat([node_label_tensor, edge_label_tensor])\n    loss = criterion(pred_tensor, label_tensor)\n\n    return loss_node, loss_edge, loss\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.plot_eigenvalue_comparison","title":"plot_eigenvalue_comparison","text":"<pre><code>plot_eigenvalue_comparison(reference_path, test_path, save_file='model')\n</code></pre> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def plot_eigenvalue_comparison(reference_path, test_path, save_file=\"model\"):\n    plt.rcParams.update({\"font.size\": 14})\n    w = np.load(test_path)\n\n    w_ref = np.load(reference_path)\n\n    print(np.linalg.norm(w - w_ref, ord=2) / np.linalg.norm(w_ref, ord=2))\n    print(np.linalg.norm(w - w_ref, ord=1) / np.linalg.norm(w_ref, ord=1))\n\n    plt.figure(figsize=(6, 4))\n    plt.scatter(\n        np.arange(len(w)), w, s=1.2, alpha=0.2, c=\"tomato\"\n    )  # make dot size smaller\n    plt.scatter(0, -10, s=10, c=\"tomato\", label=r\"$\\mathbf{H}_{ij}^{pred}$\")\n    plt.scatter(\n        np.arange(len(w_ref)), w_ref, s=1.2, alpha=0.2, c=\"mediumslateblue\"\n    )  # make dot size smaller\n    plt.scatter(0, -10, s=10, c=\"mediumslateblue\", label=r\"$\\mathbf{H}_{ij}^{GT}$\")\n\n    # y Eigenvale\n    plt.xlabel(\"Index\")\n    plt.ylabel(r\"Eigenvalue ($\\mathbf{H})\\;[E_h]$\")\n    plt.ylim(-2.1, 1.1)\n\n    plt.legend(frameon=False, loc=\"lower right\", title=r\"[$\\alpha$=0.2]\")\n    plt.savefig(\n        save_file + \"_comparison_eigenvalue\" + \"_zoom.png\", dpi=700, bbox_inches=\"tight\"\n    )\n    plt.close(\"all\")\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.reconstruct_hamiltonian","title":"reconstruct_hamiltonian","text":"<pre><code>reconstruct_hamiltonian(local_pred_dic, numbers, comm, rank, atom_orbitals, save_file='model_in_training.pth', lower_triangular=False)\n</code></pre> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def reconstruct_hamiltonian(\n    local_pred_dic,\n    numbers,\n    comm,\n    rank,\n    atom_orbitals,\n    save_file=\"model_in_training.pth\",\n    lower_triangular=False,\n):\n    local_keys = local_pred_dic.keys()\n    filtered_local_keys = []\n\n    if lower_triangular is True:\n        filtered_local_keys = [tuple(key) for key in local_keys if key[0] &gt;= key[1]]\n        # for key in local_keys:\n        #     if key[0] &gt;= key[1]:\n        #         filtered_local_keys.append(tuple(key))\n\n    else:\n        filtered_local_keys = [tuple(key) for key in local_keys if key[0] &lt;= key[1]]\n        # for key in local_keys:\n        #     if key[0] &lt;= key[1]:  # remove all duplicate offsite blocks\n        #         filtered_local_keys.append(tuple(key))\n\n    print(\"filtering done\")\n\n    local_positions = []\n    local_values = []\n\n    # Start timing\n    start_time = time.time()\n\n    # Convert global_atomic_numbers to NumPy array\n    global_atomic_numbers = np.array(numbers.tolist())\n\n    # Compute the number of orbitals for each atom type\n    num_orbitals_per_atom = np.array(\n        [\n            np.sum(2 * np.array(atom_orbitals[str(atom)]) + 1)\n            for atom in global_atomic_numbers\n        ]\n    )\n\n    # Compute starting indices (Hamiltonian indices start from 1)\n    starting_indices = (\n        np.cumsum(num_orbitals_per_atom) + 1\n    )  # switch from 0-based to 1-based indexing\n    starting_indices = np.insert(starting_indices, 0, 1)[\n        :-1\n    ]  # add 1 at the beginning and remove last element\n\n    # Extract atom indices from keys\n    keys_array = np.array(filtered_local_keys)  # Shape: (N, 2)\n    atom_i_indices = keys_array[:, 0]\n    atom_j_indices = keys_array[:, 1]\n\n    starting_i = starting_indices[atom_i_indices]\n    starting_j = starting_indices[atom_j_indices]\n\n    H_blocks = [local_pred_dic[tuple(k)] for k in filtered_local_keys]\n\n    # Process each block separately due to varying sizes\n    local_positions = []\n    local_values = []\n\n    for H_block, s_i, s_j in zip(H_blocks, starting_i, starting_j):\n        row_idx, col_idx = np.indices(H_block.shape)\n\n        global_i = s_i + row_idx\n        global_j = s_j + col_idx\n\n        # Apply triangular condition\n        mask = global_i &gt;= global_j if lower_triangular else global_i &lt;= global_j\n\n        H_block_host = H_block.detach().numpy()\n        mask &amp;= H_block_host != 0\n\n        # Collect valid positions and values\n        local_positions.append(np.column_stack((global_i[mask], global_j[mask])))\n        local_values.append(H_block_host[mask])\n\n    # Flatten into single arrays\n    local_positions = np.concatenate(local_positions, axis=0)\n    local_values = np.concatenate(local_values, axis=0)\n\n    # Step 2: Gather results at the root rank\n    all_positions = comm.gather(local_positions, root=0)\n    all_values = comm.gather(local_values, root=0)\n\n    # Step 3: Root rank processes and writes\n    if rank == 0:\n        # Combine results from all ranks\n        combined_positions = np.concatenate(all_positions, axis=0)\n        combined_values = np.concatenate(all_values, axis=0)\n\n        # Sort by positions\n        paired = zip(combined_positions, combined_values)\n        sorted_pairs = sorted(paired, key=lambda pair: pair[0][0])\n        positions_sorted, values_sorted = zip(*sorted_pairs)\n\n        # Write to the output file\n        with Path(save_file).open(\"w\") as file:\n            for (i, j), value in zip(positions_sorted, values_sorted):\n                file.write(f\"       {i}        {j}  {value:.8e}\\n\")\n\n        print(f\"Hamiltonian matrix written to {save_file}\")\n\n    # End timing\n    end_time = time.time()\n\n    # Print total time taken\n    reconstruct_time = end_time - start_time\n    # if rank == 0:\n    print(f\"Reconstruct_time: {reconstruct_time:.2f} seconds\")\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.save_training_state","title":"save_training_state","text":"<pre><code>save_training_state(model, optimizer, track_loss_edge, track_loss_node, track_validation_edge, track_validation_node, save_file)\n</code></pre> <p>Save the training state of the model and optimizer</p> Source code in <code>augmented_partition/model/training.py</code> <pre><code>@env.only_rank_zero\ndef save_training_state(\n    model,\n    optimizer,\n    track_loss_edge,\n    track_loss_node,\n    track_validation_edge,\n    track_validation_node,\n    save_file,\n):\n    \"\"\"\n    Save the training state of the model and optimizer\n    \"\"\"\n    torch.save(\n        {\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n        },\n        save_file + \".pt\",\n    )\n    torch.save(model.state_dict(), save_file + \"_state_dic.pt\")\n\n    with Path(save_file + \"_training_loss.txt\").open(\"w\") as f:\n        for edge, node in zip(track_loss_edge, track_loss_node):\n            f.write(f\"{edge:.8f}\\t{node:.8f}\\n\")\n\n    with Path(save_file + \"_validation_loss.txt\").open(\"w\") as f:\n        for edge, node in zip(track_validation_edge, track_validation_node):\n            f.write(f\"{edge:.8f}\\t{node:.8f}\\n\")\n\n    plt.figure(figsize=(4, 3))\n    plt.plot(track_loss_node, label=\"node\")\n    plt.plot(track_loss_edge, label=\"edge\")\n    plt.plot(track_validation_node, label=\"validation node\")\n    plt.plot(track_validation_edge, label=\"validation edge\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.yscale(\"log\")\n    plt.legend()\n    plt.savefig(save_file + \"_loss.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n</code></pre>"},{"location":"reference/augmented_partition/model/training.html#augmented_partition.model.training.train_and_validate_model_subgraph","title":"train_and_validate_model_subgraph","text":"<pre><code>train_and_validate_model_subgraph(model, optimizer, training_loader, validation_loader, num_epochs=5000, loss_tol=0.0001, patience=500, decay=0.5, threshold=0.001, min_lr=1e-05, save_file='model.pth', schedule=True, unflatten=False, construct_kernel=None, equivariant_blocks=None, atom_orbitals=None, out_slices=None, criterion='mse')\n</code></pre> Source code in <code>augmented_partition/model/training.py</code> <pre><code>def train_and_validate_model_subgraph(\n    model,\n    optimizer,\n    training_loader,\n    validation_loader,\n    num_epochs=5000,\n    loss_tol=0.0001,\n    patience=500,\n    decay=0.5,\n    threshold=1e-3,\n    min_lr=1e-5,\n    save_file=\"model.pth\",\n    schedule=True,\n    # dtype=torch.float32,\n    unflatten=False,\n    construct_kernel=None,\n    equivariant_blocks=None,\n    atom_orbitals=None,\n    out_slices=None,\n    criterion=\"mse\",\n):\n    device = next(model.parameters()).device\n\n    if criterion == \"mse\":\n        criterion = nn.MSELoss(reduction=\"mean\")\n    elif criterion == \"mae\":\n        criterion = nn.L1Loss(reduction=\"mean\")\n    elif criterion == \"rmse\":\n        criterion = RMSELoss()\n    elif criterion == \"combined\":\n        criterion = CombinedLoss()\n\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=decay, patience=patience, threshold=threshold\n    )\n\n    if dist.is_available() and dist.is_initialized():\n        # find_unused_parameters=True handles the cases where some parameters dont recieve gradients, such as the one-way edges\n        model = nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[device],\n            output_device=device,\n            find_unused_parameters=True,\n        )\n\n    track_loss_node = []\n    track_loss_edge = []\n    track_validation_node = []\n    track_validation_edge = []\n    track_training_loss = []  # node + edge\n    track_validation_loss = []  # node + edge\n\n    model.train()  # Set the model to training mode\n    for epoch in range(num_epochs):\n        # model.train()\n        epoch_start_time = time.time()\n\n        for batch in training_loader:\n            batch_start_time = time.time()\n\n            optimizer.zero_grad()\n\n            # Forward pass\n            batch_dev = batch.to(device)\n            node_output, edge_output = model(batch_dev)\n            forward_pass_time = time.time()\n\n            # Loss computation\n            if unflatten:\n                loss_node, loss_edge, mse_loss = get_loss_unflattened(\n                    node_output,\n                    edge_output,\n                    batch_dev,\n                    criterion,\n                    construct_kernel,\n                    equivariant_blocks,\n                    atom_orbitals,\n                    out_slices,\n                )\n            else:\n                loss_node, loss_edge, mse_loss = get_loss_flattened(\n                    node_output, edge_output, batch_dev, criterion\n                )\n\n            # Backward pass\n            loss = mse_loss\n            # loss = loss_node+loss_edge\n            loss.backward()\n            backward_pass_time = time.time()\n\n            # Parameter update\n            optimizer.step()\n\n            batch_end_time = time.time()\n            forward_pass_duration = forward_pass_time - batch_start_time\n            backward_pass_duration = backward_pass_time - forward_pass_time\n            batch_duration = batch_end_time - batch_start_time\n\n        epoch_end_time = time.time()\n        epoch_duration = epoch_end_time - epoch_start_time\n        track_loss_node.append(loss_node.cpu().detach().numpy())\n        track_loss_edge.append(loss_edge.cpu().detach().numpy())\n        track_training_loss.append(loss.cpu().detach().numpy())\n\n        @env.only_rank_zero\n        def print_train_info(\n            epoch,\n            epoch_duration,\n            forward_pass_duration,\n            backward_pass_duration,\n            batch_duration,\n            loss,\n        ):\n            print(f\"Epoch {epoch} - Time: {epoch_duration:.4f} seconds\")\n            print(f\"--&gt; Forward Pass Time: {forward_pass_duration:.4f} seconds\")\n            print(f\"--&gt; Backward Pass Time: {backward_pass_duration:.4f} seconds\")\n            print(f\"--&gt; Total Batch process time: {batch_duration:.4f} seconds\")\n            # print(\"--&gt; Memory allocated: \" + str(torch.cuda.memory_allocated(device)/1e9) + \"GB\")\n            # print(f\"--&gt; Memory info: {torch.cuda.mem_get_info(device)}\")\n            print(\"Epoch: \" + str(epoch) + \" loss: \" + str(loss))\n\n        print_train_info(\n            epoch,\n            epoch_duration,\n            forward_pass_duration,\n            backward_pass_duration,\n            batch_duration,\n            loss,\n        )\n\n        # Validate the model\n        model.eval()\n        validation_loss = 0.0\n        with torch.no_grad():\n            for batch in validation_loader:\n                batch_dev = batch.to(device)\n\n                # Forward pass\n                node_output, edge_output = model(batch_dev)\n\n                # Loss computation\n                if unflatten:\n                    loss_node, loss_edge, loss = get_loss_unflattened(\n                        node_output,\n                        edge_output,\n                        batch_dev,\n                        criterion,\n                        construct_kernel,\n                        equivariant_blocks,\n                        atom_orbitals,\n                        out_slices,\n                    )\n                else:\n                    loss_node, loss_edge, loss = get_loss_flattened(\n                        node_output, edge_output, batch_dev, criterion\n                    )\n                    # loss_node, loss_edge, loss = get_loss_transformed(node_output, edge_output, batch, criterion)\n\n                validation_loss += loss.cpu().detach().numpy()\n\n        track_validation_node.append(loss_node.cpu().detach().numpy())\n        track_validation_edge.append(loss_edge.cpu().detach().numpy())\n        track_validation_loss.append(loss.cpu().detach().numpy())\n\n        @env.only_rank_zero\n        def print_val_info(validation_loss, loss_node, loss_edge):\n            print(\"Validation loss: \", validation_loss)\n            print(\"Validation node loss: \", loss_node.cpu().detach().numpy())\n            print(\"Validation edge loss: \", loss_edge.cpu().detach().numpy())\n\n        print_val_info(validation_loss, loss_node, loss_edge)\n\n        # save the model and the current training status every 100 epochs\n        if epoch % 100 == 0:\n            save_training_state(\n                model,\n                optimizer,\n                track_loss_edge,\n                track_loss_node,\n                track_validation_edge,\n                track_validation_node,\n                save_file,\n            )\n\n        if schedule is True:\n            scheduler.step(validation_loss)\n            current_lr = optimizer.param_groups[0][\"lr\"]\n\n            print(f\"Current Learning Rate: {current_lr:.8f}\")\n            if current_lr &lt;= min_lr:\n                print(\n                    \"Learning rate has reached the minimum threshold. Stopping training.\"\n                )\n                break\n\n        if loss &lt; loss_tol:\n            print(\"Loss has reached the minimum threshold. Stopping training.\")\n            break\n\n    print(\"Final loss: \", loss)\n    save_training_state(\n        model,\n        optimizer,\n        track_loss_edge,\n        track_loss_node,\n        track_validation_edge,\n        track_validation_node,\n        save_file,\n    )\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html","title":"transformer_block","text":""},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.EdgeBlockV2","title":"EdgeBlockV2","text":"<pre><code>EdgeBlockV2(sphere_channels, attn_hidden_channels, num_heads, attn_alpha_channels, attn_value_channels, ffn_hidden_channels, output_channels, lmax, mmax, SO3_rotation, mappingReduced, max_num_elements, edge_channels_list, use_atom_edge_embedding=True, use_m_share_rad=False, norm_type='rms_norm_sh', hidden_update=False)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def __init__(\n    self,\n    sphere_channels,\n    attn_hidden_channels,\n    num_heads,\n    attn_alpha_channels,\n    attn_value_channels,\n    ffn_hidden_channels,\n    output_channels,\n    lmax,\n    mmax,\n    SO3_rotation,\n    mappingReduced,\n    max_num_elements,\n    edge_channels_list,\n    use_atom_edge_embedding=True,\n    use_m_share_rad=False,\n    # attn_activation=\"silu\",\n    # use_attn_renorm=True,\n    # ffn_activation=\"silu\",\n    norm_type=\"rms_norm_sh\",\n    hidden_update=False,\n):\n    super().__init__()\n\n    assert sphere_channels == output_channels, (\n        \"SO2EdgeUpdate only supports sphere_channels == output_channels\"\n    )\n\n    self.norm_1 = get_normalization_layer(\n        norm_type, lmax=lmax, num_channels=sphere_channels\n    )\n    self.norm_2 = get_normalization_layer(\n        norm_type, lmax=lmax, num_channels=sphere_channels\n    )\n\n    if hidden_update is True:\n        self.ga = SO2HiddenUpdate(\n            sphere_channels=sphere_channels,\n            hidden_channels=attn_hidden_channels,\n            num_heads=num_heads,\n            attn_alpha_channels=attn_alpha_channels,\n            attn_value_channels=attn_value_channels,\n            output_channels=output_channels,\n            lmax=lmax,\n            mmax=mmax,\n            SO3_rotation=SO3_rotation,\n            mappingReduced=mappingReduced,\n            max_num_elements=max_num_elements,\n            edge_channels_list=edge_channels_list,\n            use_atom_edge_embedding=use_atom_edge_embedding,\n            use_m_share_rad=use_m_share_rad,\n            # activation=attn_activation,\n            # use_attn_renorm=use_attn_renorm,\n        )\n\n    else:\n        self.ga = SO2EdgeUpdate(\n            sphere_channels=sphere_channels,\n            hidden_channels=attn_hidden_channels,\n            num_heads=num_heads,\n            attn_alpha_channels=attn_alpha_channels,\n            attn_value_channels=attn_value_channels,\n            output_channels=output_channels,\n            lmax=lmax,\n            mmax=mmax,\n            SO3_rotation=SO3_rotation,\n            mappingReduced=mappingReduced,\n            max_num_elements=max_num_elements,\n            edge_channels_list=edge_channels_list,\n            use_atom_edge_embedding=use_atom_edge_embedding,\n            use_m_share_rad=use_m_share_rad,\n            # activation=attn_activation,\n            # use_attn_renorm=use_attn_renorm,\n        )\n\n    self.ffn = FeedForwardNetwork(\n        sphere_channels=sphere_channels,\n        hidden_channels=ffn_hidden_channels,\n        output_channels=output_channels,\n        lmax=lmax,\n        mmax=mmax,\n        # activation=ffn_activation,\n    )\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.EdgeBlockV2.ffn","title":"ffn  <code>instance-attribute</code>","text":"<pre><code>ffn = FeedForwardNetwork(sphere_channels=sphere_channels, hidden_channels=ffn_hidden_channels, output_channels=output_channels, lmax=lmax, mmax=mmax)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.EdgeBlockV2.ga","title":"ga  <code>instance-attribute</code>","text":"<pre><code>ga = SO2HiddenUpdate(sphere_channels=sphere_channels, hidden_channels=attn_hidden_channels, num_heads=num_heads, attn_alpha_channels=attn_alpha_channels, attn_value_channels=attn_value_channels, output_channels=output_channels, lmax=lmax, mmax=mmax, SO3_rotation=SO3_rotation, mappingReduced=mappingReduced, max_num_elements=max_num_elements, edge_channels_list=edge_channels_list, use_atom_edge_embedding=use_atom_edge_embedding, use_m_share_rad=use_m_share_rad)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.EdgeBlockV2.norm_1","title":"norm_1  <code>instance-attribute</code>","text":"<pre><code>norm_1 = get_normalization_layer(norm_type, lmax=lmax, num_channels=sphere_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.EdgeBlockV2.norm_2","title":"norm_2  <code>instance-attribute</code>","text":"<pre><code>norm_2 = get_normalization_layer(norm_type, lmax=lmax, num_channels=sphere_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.EdgeBlockV2.forward","title":"forward","text":"<pre><code>forward(x, atomic_numbers, edge_distance, edge_index, edge_fea)\n</code></pre> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def forward(\n    self,\n    x,  # SO3_Embedding\n    atomic_numbers,\n    edge_distance,\n    edge_index,\n    edge_fea,\n):\n    output_embedding = edge_fea\n    x_res = output_embedding.embedding\n\n    # Normalize the input embedding\n    output_embedding.embedding = self.norm_1(output_embedding.embedding)\n\n    # Perform the SO2EdgeUpdate\n\n    output_embedding = self.ga(\n        x,  # use the node embedding from the previous block\n        atomic_numbers,\n        edge_distance,\n        edge_index,\n        output_embedding,\n    )  # put the output_embedding in place of edge_embedding\n\n    # print(torch.mean(abs(output_embedding.embedding)/torch.mean(abs(x_res))))\n\n    # Add the residual connection and update the output embedding\n    output_embedding.embedding = output_embedding.embedding + x_res\n    x_res = output_embedding.embedding\n\n    # Normalize the output embedding\n    output_embedding.embedding = self.norm_2(output_embedding.embedding)\n\n    # Pass through the feedforward network\n    output_embedding = self.ffn(output_embedding)\n\n    # Add the residual connection\n    output_embedding.embedding = output_embedding.embedding + x_res\n\n    return output_embedding\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork","title":"FeedForwardNetwork","text":"<pre><code>FeedForwardNetwork(sphere_channels, hidden_channels, output_channels, lmax, mmax)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>FeedForwardNetwork: Perform feedforward network with gate activation</p> <p>Args:     sphere_channels (int):      Number of spherical channels     hidden_channels (int):      Number of hidden channels used during feedforward network     output_channels (int):      Number of output channels</p> <pre><code>lmax (int):                 Max degree (l)\nmmax (int):                 Max order (m)\n\nactivation (str):           Type of activation function\n</code></pre> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def __init__(\n    self,\n    sphere_channels,\n    hidden_channels,\n    output_channels,\n    lmax,\n    mmax,\n    # activation=\"scaled_silu\",\n):\n    super().__init__()\n    self.sphere_channels = sphere_channels\n    self.hidden_channels = hidden_channels\n    self.output_channels = output_channels\n    self.lmax = lmax\n    self.mmax = mmax\n\n    self.so3_linear_1 = SO3_LinearV2(\n        self.sphere_channels, self.hidden_channels, lmax=self.lmax\n    )\n    self.so3_linear_2 = SO3_LinearV2(\n        self.hidden_channels, self.output_channels, lmax=self.lmax\n    )\n\n    self.gating_linear = torch.nn.Linear(\n        self.sphere_channels, self.lmax * self.hidden_channels\n    )\n    self.gate_act = GateActivation(self.lmax, self.lmax, self.hidden_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork.gate_act","title":"gate_act  <code>instance-attribute</code>","text":"<pre><code>gate_act = GateActivation(lmax, lmax, hidden_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork.gating_linear","title":"gating_linear  <code>instance-attribute</code>","text":"<pre><code>gating_linear = Linear(sphere_channels, lmax * hidden_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork.hidden_channels","title":"hidden_channels  <code>instance-attribute</code>","text":"<pre><code>hidden_channels = hidden_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork.output_channels","title":"output_channels  <code>instance-attribute</code>","text":"<pre><code>output_channels = output_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork.so3_linear_1","title":"so3_linear_1  <code>instance-attribute</code>","text":"<pre><code>so3_linear_1 = SO3_LinearV2(sphere_channels, hidden_channels, lmax=lmax)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork.so3_linear_2","title":"so3_linear_2  <code>instance-attribute</code>","text":"<pre><code>so3_linear_2 = SO3_LinearV2(hidden_channels, output_channels, lmax=lmax)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork.sphere_channels","title":"sphere_channels  <code>instance-attribute</code>","text":"<pre><code>sphere_channels = sphere_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.FeedForwardNetwork.forward","title":"forward","text":"<pre><code>forward(input_embedding)\n</code></pre> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def forward(self, input_embedding):\n    gating_scalars = None\n\n    if self.gating_linear is not None:\n        gating_scalars = self.gating_linear(\n            input_embedding.embedding.narrow(1, 0, 1)\n        )\n\n    input_embedding = self.so3_linear_1(input_embedding)\n\n    input_embedding.embedding = self.gate_act(\n        gating_scalars, input_embedding.embedding\n    )\n\n    return self.so3_linear_2(input_embedding)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.NodeBlockV2","title":"NodeBlockV2","text":"<pre><code>NodeBlockV2(sphere_channels, attn_hidden_channels, num_heads, attn_alpha_channels, attn_value_channels, ffn_hidden_channels, output_channels, lmax, mmax, SO3_rotation, mappingReduced, max_num_elements, edge_channels_list, use_atom_edge_embedding=True, use_m_share_rad=False, use_attn_renorm=True, norm_type='rms_norm_sh', local_update=False)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def __init__(\n    self,\n    sphere_channels,\n    attn_hidden_channels,\n    num_heads,\n    attn_alpha_channels,\n    attn_value_channels,\n    ffn_hidden_channels,\n    output_channels,\n    lmax,\n    mmax,\n    SO3_rotation,\n    mappingReduced,\n    max_num_elements,\n    edge_channels_list,\n    use_atom_edge_embedding=True,\n    use_m_share_rad=False,\n    # attn_activation=\"silu\",\n    use_attn_renorm=True,\n    # ffn_activation=\"silu\",\n    norm_type=\"rms_norm_sh\",\n    local_update=False,\n):\n    super().__init__()\n\n    assert sphere_channels == output_channels, (\n        \"NodeBlockV2 only supports sphere_channels == output_channels\"\n    )\n\n    self.norm_1 = get_normalization_layer(\n        norm_type, lmax=lmax, num_channels=sphere_channels\n    )\n    self.norm_2 = get_normalization_layer(\n        norm_type, lmax=lmax, num_channels=sphere_channels\n    )\n\n    if local_update is True:\n        self.ga = SO2NodeUpdate_local(\n            sphere_channels=sphere_channels,\n            hidden_channels=attn_hidden_channels,\n            num_heads=num_heads,\n            attn_alpha_channels=attn_alpha_channels,\n            attn_value_channels=attn_value_channels,\n            output_channels=output_channels,\n            lmax=lmax,\n            mmax=mmax,\n            SO3_rotation=SO3_rotation,\n            mappingReduced=mappingReduced,\n            max_num_elements=max_num_elements,\n            edge_channels_list=edge_channels_list,\n            use_atom_edge_embedding=use_atom_edge_embedding,\n            use_m_share_rad=use_m_share_rad,\n            # activation=attn_activation,\n            use_attn_renorm=use_attn_renorm,\n        )\n\n    else:\n        self.ga = SO2NodeUpdate(\n            sphere_channels=sphere_channels,\n            hidden_channels=attn_hidden_channels,\n            num_heads=num_heads,\n            attn_alpha_channels=attn_alpha_channels,\n            attn_value_channels=attn_value_channels,\n            output_channels=output_channels,\n            lmax=lmax,\n            mmax=mmax,\n            SO3_rotation=SO3_rotation,\n            mappingReduced=mappingReduced,\n            max_num_elements=max_num_elements,\n            edge_channels_list=edge_channels_list,\n            use_atom_edge_embedding=use_atom_edge_embedding,\n            use_m_share_rad=use_m_share_rad,\n            # activation=attn_activation,\n            use_attn_renorm=use_attn_renorm,\n        )\n\n    self.ffn = FeedForwardNetwork(\n        sphere_channels=sphere_channels,\n        hidden_channels=ffn_hidden_channels,\n        output_channels=output_channels,\n        lmax=lmax,\n        mmax=mmax,\n        # activation=ffn_activation,\n    )\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.NodeBlockV2.ffn","title":"ffn  <code>instance-attribute</code>","text":"<pre><code>ffn = FeedForwardNetwork(sphere_channels=sphere_channels, hidden_channels=ffn_hidden_channels, output_channels=output_channels, lmax=lmax, mmax=mmax)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.NodeBlockV2.ga","title":"ga  <code>instance-attribute</code>","text":"<pre><code>ga = SO2NodeUpdate_local(sphere_channels=sphere_channels, hidden_channels=attn_hidden_channels, num_heads=num_heads, attn_alpha_channels=attn_alpha_channels, attn_value_channels=attn_value_channels, output_channels=output_channels, lmax=lmax, mmax=mmax, SO3_rotation=SO3_rotation, mappingReduced=mappingReduced, max_num_elements=max_num_elements, edge_channels_list=edge_channels_list, use_atom_edge_embedding=use_atom_edge_embedding, use_m_share_rad=use_m_share_rad, use_attn_renorm=use_attn_renorm)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.NodeBlockV2.norm_1","title":"norm_1  <code>instance-attribute</code>","text":"<pre><code>norm_1 = get_normalization_layer(norm_type, lmax=lmax, num_channels=sphere_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.NodeBlockV2.norm_2","title":"norm_2  <code>instance-attribute</code>","text":"<pre><code>norm_2 = get_normalization_layer(norm_type, lmax=lmax, num_channels=sphere_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.NodeBlockV2.forward","title":"forward","text":"<pre><code>forward(x, atomic_numbers, edge_distance, edge_index, edge_fea)\n</code></pre> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def forward(\n    self,\n    x,  # node embedding\n    atomic_numbers,\n    edge_distance,  # edge distance embedding (initial edge features)\n    edge_index,\n    edge_fea,  # edge embedding\n):\n    output_embedding = x\n    x_res = output_embedding.embedding\n\n    # Normalize the input embedding\n    output_embedding.embedding = self.norm_1(output_embedding.embedding)\n\n    # Perform the SO2NodeUpdate\n    output_embedding = self.ga(\n        output_embedding, atomic_numbers, edge_distance, edge_index, edge_fea\n    )\n\n    # Add the residual connection and update the output embedding\n    output_embedding.embedding = output_embedding.embedding + x_res\n    x_res = output_embedding.embedding\n\n    # Normalize the output embedding\n    output_embedding.embedding = self.norm_2(output_embedding.embedding)\n\n    # Pass through the feedforward network\n    output_embedding = self.ffn(output_embedding)\n\n    # Add the residual connection\n    output_embedding.embedding = output_embedding.embedding + x_res\n\n    return output_embedding\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate","title":"SO2EdgeUpdate","text":"<pre><code>SO2EdgeUpdate(sphere_channels, hidden_channels, num_heads, attn_alpha_channels, attn_value_channels, output_channels, lmax, mmax, SO3_rotation, mappingReduced, max_num_elements, edge_channels_list, use_atom_edge_embedding=True, use_m_share_rad=True)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def __init__(\n    self,\n    sphere_channels,\n    hidden_channels,\n    num_heads,\n    attn_alpha_channels,\n    attn_value_channels,\n    output_channels,\n    lmax,\n    mmax,\n    SO3_rotation,\n    mappingReduced,\n    max_num_elements,\n    edge_channels_list,\n    use_atom_edge_embedding=True,\n    use_m_share_rad=True,\n    # activation=\"scaled_silu\",\n    # use_attn_renorm=True,\n):\n    super().__init__()\n\n    self.sphere_channels = sphere_channels\n    self.hidden_channels = hidden_channels\n    self.num_heads = num_heads\n    self.attn_alpha_channels = attn_alpha_channels\n    self.attn_value_channels = attn_value_channels\n    self.output_channels = output_channels\n    self.lmax = lmax\n    self.mmax = mmax\n\n    self.SO3_rotation = SO3_rotation\n    self.mappingReduced = mappingReduced\n\n    # Create edge scalar (invariant to rotations) features\n    # Embedding function of the atomic numbers\n    self.max_num_elements = max_num_elements\n    self.edge_channels_list = copy.deepcopy(edge_channels_list)\n    self.use_atom_edge_embedding = use_atom_edge_embedding\n    self.use_m_share_rad = use_m_share_rad\n\n    if self.use_atom_edge_embedding:\n        self.source_embedding = nn.Embedding(\n            self.max_num_elements, self.edge_channels_list[-1]\n        )\n        self.target_embedding = nn.Embedding(\n            self.max_num_elements, self.edge_channels_list[-1]\n        )\n        nn.init.uniform_(self.source_embedding.weight.data, -0.001, 0.001)\n        nn.init.uniform_(self.target_embedding.weight.data, -0.001, 0.001)\n        self.edge_channels_list[0] = (\n            self.edge_channels_list[0] + 2 * self.edge_channels_list[-1]\n        )\n    else:\n        self.source_embedding, self.target_embedding = None, None\n\n    # Create SO(2) convolution blocks\n    extra_m0_output_channels = None\n    extra_m0_output_channels = self.num_heads * self.attn_alpha_channels\n    extra_m0_output_channels = (\n        extra_m0_output_channels + self.lmax * self.hidden_channels\n    )\n\n    # radial function (scale all m components within a type-L vector of one channel with the same weight)\n    if self.use_m_share_rad:\n        self.edge_channels_list = [\n            *self.edge_channels_list,\n            3 * self.sphere_channels * (self.lmax + 1),\n        ]\n        self.rad_func = RadialFunction(self.edge_channels_list)\n\n        expand_index = torch.zeros([(self.lmax + 1) ** 2]).long()\n        for l_idx in range(self.lmax + 1):\n            start_idx = l_idx**2\n            length = 2 * l_idx + 1\n            expand_index[start_idx : (start_idx + length)] = l_idx\n        self.register_buffer(\"expand_index\", expand_index)\n\n    self.so2_conv_1 = SO2_Convolution(\n        3 * self.sphere_channels,\n        self.hidden_channels,\n        self.lmax,\n        self.mmax,\n        self.mappingReduced,\n        internal_weights=(bool(self.use_m_share_rad)),\n        edge_channels_list=(\n            self.edge_channels_list if not self.use_m_share_rad else None\n        ),\n        extra_m0_output_channels=extra_m0_output_channels,  # for attention weights and/or gate activation\n    )\n\n    self.gate_act = GateActivation(\n        lmax=self.lmax, mmax=self.mmax, num_channels=self.hidden_channels\n    )\n\n    self.proj = SO3_LinearV2(\n        self.hidden_channels, self.output_channels, lmax=self.lmax\n    )\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.SO3_rotation","title":"SO3_rotation  <code>instance-attribute</code>","text":"<pre><code>SO3_rotation = SO3_rotation\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.attn_alpha_channels","title":"attn_alpha_channels  <code>instance-attribute</code>","text":"<pre><code>attn_alpha_channels = attn_alpha_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.attn_value_channels","title":"attn_value_channels  <code>instance-attribute</code>","text":"<pre><code>attn_value_channels = attn_value_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.edge_channels_list","title":"edge_channels_list  <code>instance-attribute</code>","text":"<pre><code>edge_channels_list = deepcopy(edge_channels_list)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.gate_act","title":"gate_act  <code>instance-attribute</code>","text":"<pre><code>gate_act = GateActivation(lmax=lmax, mmax=mmax, num_channels=hidden_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.hidden_channels","title":"hidden_channels  <code>instance-attribute</code>","text":"<pre><code>hidden_channels = hidden_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.mappingReduced","title":"mappingReduced  <code>instance-attribute</code>","text":"<pre><code>mappingReduced = mappingReduced\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.max_num_elements","title":"max_num_elements  <code>instance-attribute</code>","text":"<pre><code>max_num_elements = max_num_elements\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.num_heads","title":"num_heads  <code>instance-attribute</code>","text":"<pre><code>num_heads = num_heads\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.output_channels","title":"output_channels  <code>instance-attribute</code>","text":"<pre><code>output_channels = output_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.proj","title":"proj  <code>instance-attribute</code>","text":"<pre><code>proj = SO3_LinearV2(hidden_channels, output_channels, lmax=lmax)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.rad_func","title":"rad_func  <code>instance-attribute</code>","text":"<pre><code>rad_func = RadialFunction(edge_channels_list)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.so2_conv_1","title":"so2_conv_1  <code>instance-attribute</code>","text":"<pre><code>so2_conv_1 = SO2_Convolution(3 * sphere_channels, hidden_channels, lmax, mmax, mappingReduced, internal_weights=bool(use_m_share_rad), edge_channels_list=edge_channels_list if not use_m_share_rad else None, extra_m0_output_channels=extra_m0_output_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.source_embedding","title":"source_embedding  <code>instance-attribute</code>","text":"<pre><code>source_embedding = Embedding(max_num_elements, edge_channels_list[-1])\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.sphere_channels","title":"sphere_channels  <code>instance-attribute</code>","text":"<pre><code>sphere_channels = sphere_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.target_embedding","title":"target_embedding  <code>instance-attribute</code>","text":"<pre><code>target_embedding = Embedding(max_num_elements, edge_channels_list[-1])\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.use_atom_edge_embedding","title":"use_atom_edge_embedding  <code>instance-attribute</code>","text":"<pre><code>use_atom_edge_embedding = use_atom_edge_embedding\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.use_m_share_rad","title":"use_m_share_rad  <code>instance-attribute</code>","text":"<pre><code>use_m_share_rad = use_m_share_rad\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2EdgeUpdate.forward","title":"forward","text":"<pre><code>forward(x, atomic_numbers, edge_distance, edge_index, edge_fea)\n</code></pre> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def forward(self, x, atomic_numbers, edge_distance, edge_index, edge_fea):\n    # Compute edge scalar features (invariant to rotations)\n    # Uses atomic numbers and edge distance as inputs\n    if self.use_atom_edge_embedding:\n        source_element = atomic_numbers[edge_index[0]]  # Source atom atomic number\n        target_element = atomic_numbers[edge_index[1]]  # Target atom atomic number\n        source_embedding = self.source_embedding(source_element)\n        target_embedding = self.target_embedding(target_element)\n        x_edge = torch.cat(\n            (edge_distance, source_embedding, target_embedding), dim=1\n        )\n    else:\n        x_edge = edge_distance\n\n    x_source = x.clone()\n    x_target = x.clone()\n    x_source._expand_edge(\n        edge_index[0, :]\n    )  # first dimension is the number of edges\n    x_target._expand_edge(edge_index[1, :])\n\n    x_message_data = torch.cat(\n        (x_source.embedding, x_target.embedding, edge_fea.embedding), dim=2\n    )  # concatenate source and target node embeddings along channel dimension\n    x_message = SO3_Embedding(\n        0,\n        x_target.lmax,\n        x_target.num_channels * 3,\n        device=x_target.device,\n        dtype=x_target.dtype,\n    )\n    x_message.set_embedding(x_message_data)\n    x_message.set_lmax_mmax(self.lmax, self.mmax)\n\n    # radial function (linear layers + layer normalization + SiLU)\n    if self.use_m_share_rad:\n        x_edge_weight = self.rad_func(x_edge)\n        x_edge_weight = x_edge_weight.reshape(\n            -1, (self.lmax + 1), 3 * self.sphere_channels\n        )\n        x_edge_weight = torch.index_select(\n            x_edge_weight, dim=1, index=self.expand_index\n        )  # [E, (L_max + 1) ** 2, C]\n        x_message.embedding = x_message.embedding * x_edge_weight\n\n    # Rotate the irreps to align with the edge\n    x_message._rotate(self.SO3_rotation, self.lmax, self.mmax)\n\n    # First SO(2)-convolution\n    x_message, x_0_extra = self.so2_conv_1(x_message, x_edge)\n\n    # Activation (Gate activation)\n    x_alpha_num_channels = self.num_heads * self.attn_alpha_channels\n    x_0_gating = x_0_extra.narrow(\n        1, x_alpha_num_channels, x_0_extra.shape[1] - x_alpha_num_channels\n    )  # for activation\n    x_message.embedding = self.gate_act(x_0_gating, x_message.embedding)\n\n    # Rotate back the irreps\n    x_message._rotate_inv(self.SO3_rotation)\n\n    # Project\n    return self.proj(x_message)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate","title":"SO2HiddenUpdate","text":"<pre><code>SO2HiddenUpdate(sphere_channels, hidden_channels, num_heads, attn_alpha_channels, attn_value_channels, output_channels, lmax, mmax, SO3_rotation, mappingReduced, max_num_elements, edge_channels_list, use_atom_edge_embedding=True, use_m_share_rad=False)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def __init__(\n    self,\n    sphere_channels,\n    hidden_channels,\n    num_heads,\n    attn_alpha_channels,\n    attn_value_channels,\n    output_channels,\n    lmax,\n    mmax,\n    SO3_rotation,\n    mappingReduced,\n    max_num_elements,\n    edge_channels_list,\n    use_atom_edge_embedding=True,\n    use_m_share_rad=False,\n    # activation=\"scaled_silu\",\n    # use_attn_renorm=True,\n):\n    super().__init__()\n\n    self.sphere_channels = sphere_channels\n    self.hidden_channels = hidden_channels\n    self.num_heads = num_heads\n    self.attn_alpha_channels = attn_alpha_channels\n    self.attn_value_channels = attn_value_channels\n    self.output_channels = output_channels\n    self.lmax = lmax\n    self.mmax = mmax\n\n    self.SO3_rotation = SO3_rotation\n    self.mappingReduced = mappingReduced\n\n    # Create edge scalar (invariant to rotations) features\n    # Embedding function of the atomic numbers\n    self.max_num_elements = max_num_elements\n    self.edge_channels_list = copy.deepcopy(edge_channels_list)\n    self.use_atom_edge_embedding = use_atom_edge_embedding\n    self.use_m_share_rad = use_m_share_rad\n\n    if self.use_atom_edge_embedding:\n        self.source_embedding = nn.Embedding(\n            self.max_num_elements, self.edge_channels_list[-1]\n        )\n        self.target_embedding = nn.Embedding(\n            self.max_num_elements, self.edge_channels_list[-1]\n        )\n        nn.init.uniform_(self.source_embedding.weight.data, -0.001, 0.001)\n        nn.init.uniform_(self.target_embedding.weight.data, -0.001, 0.001)\n        self.edge_channels_list[0] = (\n            self.edge_channels_list[0] + 2 * self.edge_channels_list[-1]\n        )\n    else:\n        self.source_embedding, self.target_embedding = None, None\n\n    # self.use_attn_renorm = use_attn_renorm\n\n    # Create SO(2) convolution blocks\n    extra_m0_output_channels = None\n    extra_m0_output_channels = self.num_heads * self.attn_alpha_channels\n    extra_m0_output_channels = (\n        extra_m0_output_channels + self.lmax * self.hidden_channels\n    )\n\n    if self.use_m_share_rad:\n        self.edge_channels_list = [\n            *self.edge_channels_list,\n            2 * self.sphere_channels * (self.lmax + 1),\n        ]\n        self.rad_func = RadialFunction(self.edge_channels_list)\n\n        expand_index = torch.zeros([(self.lmax + 1) ** 2]).long()\n        for l_idx in range(self.lmax + 1):\n            start_idx = l_idx**2\n            length = 2 * l_idx + 1\n            expand_index[start_idx : (start_idx + length)] = l_idx\n        self.register_buffer(\"expand_index\", expand_index)\n\n    self.so2_conv_1 = SO2_Convolution(\n        2 * self.sphere_channels,\n        self.hidden_channels,\n        self.lmax,\n        self.mmax,\n        self.mappingReduced,\n        internal_weights=(bool(self.use_m_share_rad)),\n        edge_channels_list=(\n            self.edge_channels_list if not self.use_m_share_rad else None\n        ),\n        extra_m0_output_channels=extra_m0_output_channels,  # for attention weights and/or gate activation\n    )\n\n    self.gate_act = GateActivation(\n        lmax=self.lmax, mmax=self.mmax, num_channels=self.hidden_channels\n    )\n\n    self.proj = SO3_LinearV2(\n        self.hidden_channels, self.output_channels, lmax=self.lmax\n    )\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.SO3_rotation","title":"SO3_rotation  <code>instance-attribute</code>","text":"<pre><code>SO3_rotation = SO3_rotation\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.attn_alpha_channels","title":"attn_alpha_channels  <code>instance-attribute</code>","text":"<pre><code>attn_alpha_channels = attn_alpha_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.attn_value_channels","title":"attn_value_channels  <code>instance-attribute</code>","text":"<pre><code>attn_value_channels = attn_value_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.edge_channels_list","title":"edge_channels_list  <code>instance-attribute</code>","text":"<pre><code>edge_channels_list = deepcopy(edge_channels_list)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.gate_act","title":"gate_act  <code>instance-attribute</code>","text":"<pre><code>gate_act = GateActivation(lmax=lmax, mmax=mmax, num_channels=hidden_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.hidden_channels","title":"hidden_channels  <code>instance-attribute</code>","text":"<pre><code>hidden_channels = hidden_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.mappingReduced","title":"mappingReduced  <code>instance-attribute</code>","text":"<pre><code>mappingReduced = mappingReduced\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.max_num_elements","title":"max_num_elements  <code>instance-attribute</code>","text":"<pre><code>max_num_elements = max_num_elements\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.num_heads","title":"num_heads  <code>instance-attribute</code>","text":"<pre><code>num_heads = num_heads\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.output_channels","title":"output_channels  <code>instance-attribute</code>","text":"<pre><code>output_channels = output_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.proj","title":"proj  <code>instance-attribute</code>","text":"<pre><code>proj = SO3_LinearV2(hidden_channels, output_channels, lmax=lmax)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.rad_func","title":"rad_func  <code>instance-attribute</code>","text":"<pre><code>rad_func = RadialFunction(edge_channels_list)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.so2_conv_1","title":"so2_conv_1  <code>instance-attribute</code>","text":"<pre><code>so2_conv_1 = SO2_Convolution(2 * sphere_channels, hidden_channels, lmax, mmax, mappingReduced, internal_weights=bool(use_m_share_rad), edge_channels_list=edge_channels_list if not use_m_share_rad else None, extra_m0_output_channels=extra_m0_output_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.source_embedding","title":"source_embedding  <code>instance-attribute</code>","text":"<pre><code>source_embedding = Embedding(max_num_elements, edge_channels_list[-1])\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.sphere_channels","title":"sphere_channels  <code>instance-attribute</code>","text":"<pre><code>sphere_channels = sphere_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.target_embedding","title":"target_embedding  <code>instance-attribute</code>","text":"<pre><code>target_embedding = Embedding(max_num_elements, edge_channels_list[-1])\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.use_atom_edge_embedding","title":"use_atom_edge_embedding  <code>instance-attribute</code>","text":"<pre><code>use_atom_edge_embedding = use_atom_edge_embedding\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.use_m_share_rad","title":"use_m_share_rad  <code>instance-attribute</code>","text":"<pre><code>use_m_share_rad = use_m_share_rad\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2HiddenUpdate.forward","title":"forward","text":"<pre><code>forward(x, atomic_numbers, edge_distance, edge_index, hidden_fea)\n</code></pre> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def forward(self, x, atomic_numbers, edge_distance, edge_index, hidden_fea):\n    # Compute edge scalar features (invariant to rotations)\n    # Uses atomic numbers and edge distance as inputs\n    if self.use_atom_edge_embedding:\n        source_element = atomic_numbers[edge_index[0]]  # Source atom atomic number\n        target_element = atomic_numbers[edge_index[1]]  # Target atom atomic number\n        source_embedding = self.source_embedding(source_element)\n        target_embedding = self.target_embedding(target_element)\n        x_edge = torch.cat(\n            (edge_distance, source_embedding, target_embedding), dim=1\n        )\n    else:\n        x_edge = edge_distance\n\n    # x_source = x.clone()\n    x_target = x.clone()\n    # x_source._expand_edge(edge_index[0, :]) #first dimension is the number of edges\n    x_target._expand_edge(edge_index[1, :])\n\n    # x_message_data = torch.cat((x_source.embedding, x_target.embedding, edge_fea.embedding), dim=2) #concatenate source and target node embeddings along channel dimension\n\n    x_message_data = torch.cat(\n        (x_target.embedding, hidden_fea.embedding), dim=2\n    )  # concatenate hidden and target node embeddings and perform convolution to produce new hidden embedding\n\n    x_message = SO3_Embedding(\n        0,\n        x_target.lmax,\n        x_target.num_channels * 2,\n        device=x_target.device,\n        dtype=x_target.dtype,\n    )\n    x_message.set_embedding(x_message_data)\n    x_message.set_lmax_mmax(self.lmax, self.mmax)\n\n    # radial function (scale all m components within a type-L vector of one channel with the same weight)\n    if self.use_m_share_rad:\n        x_edge_weight = self.rad_func(x_edge)\n        x_edge_weight = x_edge_weight.reshape(\n            -1, (self.lmax + 1), 2 * self.sphere_channels\n        )\n        x_edge_weight = torch.index_select(\n            x_edge_weight, dim=1, index=self.expand_index\n        )  # [E, (L_max + 1) ** 2, C]\n        x_message.embedding = x_message.embedding * x_edge_weight\n\n    # Rotate the irreps to align with the edge\n    x_message._rotate(self.SO3_rotation, self.lmax, self.mmax)\n\n    # First SO(2)-convolution\n    x_message, x_0_extra = self.so2_conv_1(x_message, x_edge)\n\n    # Activation (Gate activation)\n    x_alpha_num_channels = self.num_heads * self.attn_alpha_channels\n    x_0_gating = x_0_extra.narrow(\n        1, x_alpha_num_channels, x_0_extra.shape[1] - x_alpha_num_channels\n    )  # for activation\n    x_message.embedding = self.gate_act(x_0_gating, x_message.embedding)\n\n    # Rotate back the irreps\n    x_message._rotate_inv(self.SO3_rotation)\n\n    # Project\n    return self.proj(x_message)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate","title":"SO2NodeUpdate","text":"<pre><code>SO2NodeUpdate(sphere_channels, hidden_channels, num_heads, attn_alpha_channels, attn_value_channels, output_channels, lmax, mmax, SO3_rotation, mappingReduced, max_num_elements, edge_channels_list, use_atom_edge_embedding=True, use_m_share_rad=False, use_attn_renorm=True)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def __init__(\n    self,\n    sphere_channels,\n    hidden_channels,\n    num_heads,\n    attn_alpha_channels,\n    attn_value_channels,\n    output_channels,\n    lmax,\n    mmax,\n    SO3_rotation,\n    mappingReduced,\n    max_num_elements,\n    edge_channels_list,\n    use_atom_edge_embedding=True,\n    use_m_share_rad=False,\n    # activation=\"scaled_silu\",\n    use_attn_renorm=True,\n):\n    super().__init__()\n\n    self.sphere_channels = sphere_channels\n    self.hidden_channels = hidden_channels\n    self.num_heads = num_heads\n    self.attn_alpha_channels = attn_alpha_channels\n    self.attn_value_channels = attn_value_channels\n    self.output_channels = output_channels\n    self.lmax = lmax\n    self.mmax = mmax\n\n    self.SO3_rotation = SO3_rotation\n    self.mappingReduced = mappingReduced\n\n    # Create edge scalar (invariant to rotations) features\n    # Embedding function of the atomic numbers\n    self.max_num_elements = max_num_elements\n    self.edge_channels_list = copy.deepcopy(edge_channels_list)\n    self.use_atom_edge_embedding = use_atom_edge_embedding\n    self.use_m_share_rad = use_m_share_rad\n\n    if self.use_atom_edge_embedding:\n        self.source_embedding = nn.Embedding(\n            self.max_num_elements, self.edge_channels_list[-1]\n        )\n        self.target_embedding = nn.Embedding(\n            self.max_num_elements, self.edge_channels_list[-1]\n        )\n        nn.init.uniform_(self.source_embedding.weight.data, -0.001, 0.001)\n        nn.init.uniform_(self.target_embedding.weight.data, -0.001, 0.001)\n        self.edge_channels_list[0] = (\n            self.edge_channels_list[0] + 2 * self.edge_channels_list[-1]\n        )\n    else:\n        self.source_embedding, self.target_embedding = None, None\n\n    self.use_attn_renorm = use_attn_renorm\n\n    # Create SO(2) convolution blocks\n    extra_m0_output_channels = None\n    extra_m0_output_channels = self.num_heads * self.attn_alpha_channels\n    extra_m0_output_channels = (\n        extra_m0_output_channels + self.lmax * self.hidden_channels\n    )  # for gate activation\n\n    if self.use_m_share_rad:\n        self.edge_channels_list = [\n            *self.edge_channels_list,\n            3 * self.sphere_channels * (self.lmax + 1),\n        ]\n        self.rad_func = RadialFunction(self.edge_channels_list)\n        expand_index = torch.zeros([(self.lmax + 1) ** 2]).long()\n        for l_idx in range(self.lmax + 1):\n            start_idx = l_idx**2\n            length = 2 * l_idx + 1\n            expand_index[start_idx : (start_idx + length)] = l_idx\n        self.register_buffer(\"expand_index\", expand_index)\n\n    self.so2_conv_1 = SO2_Convolution(\n        3 * self.sphere_channels,\n        self.hidden_channels,\n        self.lmax,\n        self.mmax,\n        self.mappingReduced,\n        internal_weights=True,\n        edge_channels_list=(\n            self.edge_channels_list if not self.use_m_share_rad else None\n        ),\n        extra_m0_output_channels=extra_m0_output_channels,  # for attention weights and/or gate activation\n    )\n\n    if self.use_attn_renorm:\n        self.alpha_norm = torch.nn.LayerNorm(self.attn_alpha_channels)\n    else:\n        self.alpha_norm = torch.nn.Identity()\n    self.alpha_act = SmoothLeakyReLU()\n    self.alpha_dot = torch.nn.Parameter(\n        torch.randn(self.num_heads, self.attn_alpha_channels)\n    )\n    # torch_geometric.nn.inits.glorot(self.alpha_dot) # Following GATv2\n    std = 1.0 / math.sqrt(self.attn_alpha_channels)\n    torch.nn.init.uniform_(self.alpha_dot, -std, std)\n\n    self.gate_act = GateActivation(\n        lmax=self.lmax, mmax=self.mmax, num_channels=self.hidden_channels\n    )\n\n    self.so2_conv_2 = SO2_Convolution(\n        self.hidden_channels,\n        self.num_heads * self.attn_value_channels,\n        self.lmax,\n        self.mmax,\n        self.mappingReduced,\n        internal_weights=True,\n        edge_channels_list=None,\n        extra_m0_output_channels=None,\n    )\n\n    self.proj = SO3_LinearV2(\n        self.num_heads * self.attn_value_channels,\n        self.output_channels,\n        lmax=self.lmax,\n    )\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.SO3_rotation","title":"SO3_rotation  <code>instance-attribute</code>","text":"<pre><code>SO3_rotation = SO3_rotation\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.alpha_act","title":"alpha_act  <code>instance-attribute</code>","text":"<pre><code>alpha_act = SmoothLeakyReLU()\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.alpha_dot","title":"alpha_dot  <code>instance-attribute</code>","text":"<pre><code>alpha_dot = Parameter(randn(num_heads, attn_alpha_channels))\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.alpha_norm","title":"alpha_norm  <code>instance-attribute</code>","text":"<pre><code>alpha_norm = LayerNorm(attn_alpha_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.attn_alpha_channels","title":"attn_alpha_channels  <code>instance-attribute</code>","text":"<pre><code>attn_alpha_channels = attn_alpha_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.attn_value_channels","title":"attn_value_channels  <code>instance-attribute</code>","text":"<pre><code>attn_value_channels = attn_value_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.edge_channels_list","title":"edge_channels_list  <code>instance-attribute</code>","text":"<pre><code>edge_channels_list = deepcopy(edge_channels_list)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.gate_act","title":"gate_act  <code>instance-attribute</code>","text":"<pre><code>gate_act = GateActivation(lmax=lmax, mmax=mmax, num_channels=hidden_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.hidden_channels","title":"hidden_channels  <code>instance-attribute</code>","text":"<pre><code>hidden_channels = hidden_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.mappingReduced","title":"mappingReduced  <code>instance-attribute</code>","text":"<pre><code>mappingReduced = mappingReduced\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.max_num_elements","title":"max_num_elements  <code>instance-attribute</code>","text":"<pre><code>max_num_elements = max_num_elements\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.num_heads","title":"num_heads  <code>instance-attribute</code>","text":"<pre><code>num_heads = num_heads\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.output_channels","title":"output_channels  <code>instance-attribute</code>","text":"<pre><code>output_channels = output_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.proj","title":"proj  <code>instance-attribute</code>","text":"<pre><code>proj = SO3_LinearV2(num_heads * attn_value_channels, output_channels, lmax=lmax)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.rad_func","title":"rad_func  <code>instance-attribute</code>","text":"<pre><code>rad_func = RadialFunction(edge_channels_list)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.so2_conv_1","title":"so2_conv_1  <code>instance-attribute</code>","text":"<pre><code>so2_conv_1 = SO2_Convolution(3 * sphere_channels, hidden_channels, lmax, mmax, mappingReduced, internal_weights=True, edge_channels_list=edge_channels_list if not use_m_share_rad else None, extra_m0_output_channels=extra_m0_output_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.so2_conv_2","title":"so2_conv_2  <code>instance-attribute</code>","text":"<pre><code>so2_conv_2 = SO2_Convolution(hidden_channels, num_heads * attn_value_channels, lmax, mmax, mappingReduced, internal_weights=True, edge_channels_list=None, extra_m0_output_channels=None)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.source_embedding","title":"source_embedding  <code>instance-attribute</code>","text":"<pre><code>source_embedding = Embedding(max_num_elements, edge_channels_list[-1])\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.sphere_channels","title":"sphere_channels  <code>instance-attribute</code>","text":"<pre><code>sphere_channels = sphere_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.target_embedding","title":"target_embedding  <code>instance-attribute</code>","text":"<pre><code>target_embedding = Embedding(max_num_elements, edge_channels_list[-1])\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.use_atom_edge_embedding","title":"use_atom_edge_embedding  <code>instance-attribute</code>","text":"<pre><code>use_atom_edge_embedding = use_atom_edge_embedding\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.use_attn_renorm","title":"use_attn_renorm  <code>instance-attribute</code>","text":"<pre><code>use_attn_renorm = use_attn_renorm\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.use_m_share_rad","title":"use_m_share_rad  <code>instance-attribute</code>","text":"<pre><code>use_m_share_rad = use_m_share_rad\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate.forward","title":"forward","text":"<pre><code>forward(x, atomic_numbers, edge_distance, edge_index, edge_fea)\n</code></pre> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def forward(self, x, atomic_numbers, edge_distance, edge_index, edge_fea):\n    # Compute edge scalar features (invariant to rotations)\n    # Uses atomic numbers and edge distance as inputs\n    if self.use_atom_edge_embedding:\n        source_element = atomic_numbers[edge_index[0]]  # Source atom atomic number\n        target_element = atomic_numbers[edge_index[1]]  # Target atom atomic number\n        source_embedding = self.source_embedding(source_element)\n        target_embedding = self.target_embedding(target_element)\n        x_edge = torch.cat(\n            (edge_distance, source_embedding, target_embedding), dim=1\n        )  # shape of [#edges, 3 * #channels]\n    else:\n        x_edge = edge_distance\n\n    x_source = x.clone()\n    x_target = x.clone()\n    x_source._expand_edge(\n        edge_index[0, :]\n    )  # first dimension is the number of edges\n    x_target._expand_edge(edge_index[1, :])\n\n    # to form the message, concatenate the embeddings of the source node, target node, and the edge between them\n    x_message_data = torch.cat(\n        (x_source.embedding, x_target.embedding, edge_fea.embedding), dim=2\n    )\n    x_message = SO3_Embedding(\n        0,\n        x_target.lmax,\n        x_target.num_channels * 3,\n        device=x_target.device,\n        dtype=x_target.dtype,\n    )\n    x_message.set_embedding(\n        x_message_data\n    )  # shape of [#edges, #channels, 3 * #channels]\n    x_message.set_lmax_mmax(self.lmax, self.mmax)\n\n    # radial function (linear layers + layer normalization + SiLU)\n    if (\n        self.use_m_share_rad\n    ):  # either applied here (currently done) or inside the SO2_Convolution\n        x_edge_weight = self.rad_func(x_edge)\n        x_edge_weight = x_edge_weight.reshape(\n            -1, (self.lmax + 1), 3 * self.sphere_channels\n        )  # 3x for the 3 concatenated features\n        x_edge_weight = torch.index_select(\n            x_edge_weight, dim=1, index=self.expand_index\n        )  # [E, (L_max + 1) ** 2, C]\n        x_message.embedding = x_message.embedding * x_edge_weight\n\n    # Rotate the irreps to align with the edge\n    x_message._rotate(self.SO3_rotation, self.lmax, self.mmax)\n\n    # First SO(2)-convolution\n    x_message, x_0_extra = self.so2_conv_1(x_message, x_edge)\n\n    # Activation (Gate activation)\n    x_alpha_num_channels = self.num_heads * self.attn_alpha_channels\n    x_0_gating = x_0_extra.narrow(\n        1, x_alpha_num_channels, x_0_extra.shape[1] - x_alpha_num_channels\n    )  # for activation\n    x_0_alpha = x_0_extra.narrow(\n        1, 0, x_alpha_num_channels\n    )  # for attention weights, shape [E, num_heads * attn_alpha_channels]\n    x_message.embedding = self.gate_act(x_0_gating, x_message.embedding)\n\n    # Second SO(2)-convolution\n    x_message = self.so2_conv_2(x_message, x_edge)\n\n    # Attention weights\n    x_0_alpha = x_0_alpha.reshape(\n        -1, self.num_heads, self.attn_alpha_channels\n    )  # shape of [E, num_heads, attn_alpha_channels]\n    x_0_alpha = self.alpha_norm(x_0_alpha)\n    x_0_alpha = self.alpha_act(x_0_alpha)\n    alpha = torch.einsum(\"bik, ik -&gt; bi\", x_0_alpha, self.alpha_dot)\n\n    alpha = torch_geometric.utils.softmax(alpha, edge_index[1])\n    alpha = alpha.reshape(\n        alpha.shape[0], 1, self.num_heads, 1\n    )  # shape of [E, 1, num_heads, 1]\n\n    # Attention weights * non-linear messages\n    attn = x_message.embedding  # shape of [E, (lmax+1)^2, # hidden channels]\n    attn = attn.reshape(\n        attn.shape[0], attn.shape[1], self.num_heads, self.attn_value_channels\n    )  # shape of [E, #channels, num_heads, attn_value_channels]\n    attn = attn * alpha\n    attn = attn.reshape(\n        attn.shape[0], attn.shape[1], self.num_heads * self.attn_value_channels\n    )\n    x_message.embedding = attn\n\n    # Rotate back the irreps\n    x_message._rotate_inv(self.SO3_rotation)\n\n    # Aggregate incoming neighboring messages for each target node\n    x_message._reduce_edge(edge_index[1], len(x.embedding))\n\n    # Project\n    return self.proj(x_message)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local","title":"SO2NodeUpdate_local","text":"<pre><code>SO2NodeUpdate_local(sphere_channels, hidden_channels, num_heads, attn_alpha_channels, attn_value_channels, output_channels, lmax, mmax, SO3_rotation, mappingReduced, max_num_elements, edge_channels_list, use_atom_edge_embedding=True, use_m_share_rad=False, use_attn_renorm=True)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def __init__(\n    self,\n    sphere_channels,\n    hidden_channels,\n    num_heads,\n    attn_alpha_channels,\n    attn_value_channels,\n    output_channels,\n    lmax,\n    mmax,\n    SO3_rotation,\n    mappingReduced,\n    max_num_elements,\n    edge_channels_list,\n    use_atom_edge_embedding=True,\n    use_m_share_rad=False,\n    # activation=\"scaled_silu\",\n    # use_s2_act_attn=False,\n    use_attn_renorm=True,\n):\n    super().__init__()\n\n    self.sphere_channels = sphere_channels\n    self.hidden_channels = hidden_channels\n    self.num_heads = num_heads\n    self.attn_alpha_channels = attn_alpha_channels\n    self.attn_value_channels = attn_value_channels\n    self.output_channels = output_channels\n    self.lmax = lmax\n    self.mmax = mmax\n\n    self.SO3_rotation = SO3_rotation\n    self.mappingReduced = mappingReduced\n\n    # Create edge scalar (invariant to rotations) features\n    # Embedding function of the atomic numbers\n    self.max_num_elements = max_num_elements\n    self.edge_channels_list = copy.deepcopy(edge_channels_list)\n    self.use_atom_edge_embedding = use_atom_edge_embedding\n    self.use_m_share_rad = use_m_share_rad\n\n    if self.use_atom_edge_embedding:\n        self.source_embedding = nn.Embedding(\n            self.max_num_elements, self.edge_channels_list[-1]\n        )\n        self.target_embedding = nn.Embedding(\n            self.max_num_elements, self.edge_channels_list[-1]\n        )\n        nn.init.uniform_(self.source_embedding.weight.data, -0.001, 0.001)\n        nn.init.uniform_(self.target_embedding.weight.data, -0.001, 0.001)\n        self.edge_channels_list[0] = (\n            self.edge_channels_list[0] + 2 * self.edge_channels_list[-1]\n        )\n    else:\n        self.source_embedding, self.target_embedding = None, None\n\n    self.use_attn_renorm = use_attn_renorm\n\n    # Create SO(2) convolution blocks\n    extra_m0_output_channels = None\n    extra_m0_output_channels = self.num_heads * self.attn_alpha_channels\n    extra_m0_output_channels = (\n        extra_m0_output_channels + self.lmax * self.hidden_channels\n    )\n\n    if self.use_m_share_rad:\n        self.edge_channels_list = [\n            *self.edge_channels_list,\n            2 * self.sphere_channels * (self.lmax + 1),\n        ]\n        self.rad_func = RadialFunction(self.edge_channels_list)\n        expand_index = torch.zeros([(self.lmax + 1) ** 2]).long()\n        for l_idx in range(self.lmax + 1):\n            start_idx = l_idx**2\n            length = 2 * l_idx + 1\n            expand_index[start_idx : (start_idx + length)] = l_idx\n        self.register_buffer(\"expand_index\", expand_index)\n\n    self.so2_conv_1 = SO2_Convolution(\n        2 * self.sphere_channels,\n        self.hidden_channels,\n        self.lmax,\n        self.mmax,\n        self.mappingReduced,\n        internal_weights=(bool(self.use_m_share_rad)),\n        edge_channels_list=(\n            self.edge_channels_list if not self.use_m_share_rad else None\n        ),\n        extra_m0_output_channels=extra_m0_output_channels,  # for attention weights and/or gate activation\n    )\n\n    if self.use_attn_renorm:\n        self.alpha_norm = torch.nn.LayerNorm(self.attn_alpha_channels)\n    else:\n        self.alpha_norm = torch.nn.Identity()\n    self.alpha_act = SmoothLeakyReLU()\n    self.alpha_dot = torch.nn.Parameter(\n        torch.randn(self.num_heads, self.attn_alpha_channels)\n    )\n    # torch_geometric.nn.inits.glorot(self.alpha_dot) # Following GATv2\n    std = 1.0 / math.sqrt(self.attn_alpha_channels)\n    torch.nn.init.uniform_(self.alpha_dot, -std, std)\n\n    self.gate_act = GateActivation(\n        lmax=self.lmax, mmax=self.mmax, num_channels=self.hidden_channels\n    )\n\n    self.so2_conv_2 = SO2_Convolution(\n        self.hidden_channels,\n        self.num_heads * self.attn_value_channels,\n        self.lmax,\n        self.mmax,\n        self.mappingReduced,\n        internal_weights=True,\n        edge_channels_list=None,\n        extra_m0_output_channels=None,\n    )\n\n    self.proj = SO3_LinearV2(\n        self.num_heads * self.attn_value_channels,\n        self.output_channels,\n        lmax=self.lmax,\n    )\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.SO3_rotation","title":"SO3_rotation  <code>instance-attribute</code>","text":"<pre><code>SO3_rotation = SO3_rotation\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.alpha_act","title":"alpha_act  <code>instance-attribute</code>","text":"<pre><code>alpha_act = SmoothLeakyReLU()\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.alpha_dot","title":"alpha_dot  <code>instance-attribute</code>","text":"<pre><code>alpha_dot = Parameter(randn(num_heads, attn_alpha_channels))\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.alpha_norm","title":"alpha_norm  <code>instance-attribute</code>","text":"<pre><code>alpha_norm = LayerNorm(attn_alpha_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.attn_alpha_channels","title":"attn_alpha_channels  <code>instance-attribute</code>","text":"<pre><code>attn_alpha_channels = attn_alpha_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.attn_value_channels","title":"attn_value_channels  <code>instance-attribute</code>","text":"<pre><code>attn_value_channels = attn_value_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.edge_channels_list","title":"edge_channels_list  <code>instance-attribute</code>","text":"<pre><code>edge_channels_list = deepcopy(edge_channels_list)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.gate_act","title":"gate_act  <code>instance-attribute</code>","text":"<pre><code>gate_act = GateActivation(lmax=lmax, mmax=mmax, num_channels=hidden_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.hidden_channels","title":"hidden_channels  <code>instance-attribute</code>","text":"<pre><code>hidden_channels = hidden_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.lmax","title":"lmax  <code>instance-attribute</code>","text":"<pre><code>lmax = lmax\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.mappingReduced","title":"mappingReduced  <code>instance-attribute</code>","text":"<pre><code>mappingReduced = mappingReduced\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.max_num_elements","title":"max_num_elements  <code>instance-attribute</code>","text":"<pre><code>max_num_elements = max_num_elements\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.mmax","title":"mmax  <code>instance-attribute</code>","text":"<pre><code>mmax = mmax\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.num_heads","title":"num_heads  <code>instance-attribute</code>","text":"<pre><code>num_heads = num_heads\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.output_channels","title":"output_channels  <code>instance-attribute</code>","text":"<pre><code>output_channels = output_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.proj","title":"proj  <code>instance-attribute</code>","text":"<pre><code>proj = SO3_LinearV2(num_heads * attn_value_channels, output_channels, lmax=lmax)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.rad_func","title":"rad_func  <code>instance-attribute</code>","text":"<pre><code>rad_func = RadialFunction(edge_channels_list)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.so2_conv_1","title":"so2_conv_1  <code>instance-attribute</code>","text":"<pre><code>so2_conv_1 = SO2_Convolution(2 * sphere_channels, hidden_channels, lmax, mmax, mappingReduced, internal_weights=bool(use_m_share_rad), edge_channels_list=edge_channels_list if not use_m_share_rad else None, extra_m0_output_channels=extra_m0_output_channels)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.so2_conv_2","title":"so2_conv_2  <code>instance-attribute</code>","text":"<pre><code>so2_conv_2 = SO2_Convolution(hidden_channels, num_heads * attn_value_channels, lmax, mmax, mappingReduced, internal_weights=True, edge_channels_list=None, extra_m0_output_channels=None)\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.source_embedding","title":"source_embedding  <code>instance-attribute</code>","text":"<pre><code>source_embedding = Embedding(max_num_elements, edge_channels_list[-1])\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.sphere_channels","title":"sphere_channels  <code>instance-attribute</code>","text":"<pre><code>sphere_channels = sphere_channels\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.target_embedding","title":"target_embedding  <code>instance-attribute</code>","text":"<pre><code>target_embedding = Embedding(max_num_elements, edge_channels_list[-1])\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.use_atom_edge_embedding","title":"use_atom_edge_embedding  <code>instance-attribute</code>","text":"<pre><code>use_atom_edge_embedding = use_atom_edge_embedding\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.use_attn_renorm","title":"use_attn_renorm  <code>instance-attribute</code>","text":"<pre><code>use_attn_renorm = use_attn_renorm\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.use_m_share_rad","title":"use_m_share_rad  <code>instance-attribute</code>","text":"<pre><code>use_m_share_rad = use_m_share_rad\n</code></pre>"},{"location":"reference/augmented_partition/model/transformer_block.html#augmented_partition.model.transformer_block.SO2NodeUpdate_local.forward","title":"forward","text":"<pre><code>forward(x, atomic_numbers, edge_distance, edge_index, hidden_fea)\n</code></pre> Source code in <code>augmented_partition/model/transformer_block.py</code> <pre><code>def forward(self, x, atomic_numbers, edge_distance, edge_index, hidden_fea):\n    # Compute edge scalar features (invariant to rotations)\n    # Uses atomic numbers and edge distance as inputs\n    if self.use_atom_edge_embedding:\n        source_element = atomic_numbers[edge_index[0]]  # Source atom atomic number\n        target_element = atomic_numbers[edge_index[1]]  # Target atom atomic number\n        source_embedding = self.source_embedding(source_element)\n        target_embedding = self.target_embedding(target_element)\n        x_edge = torch.cat(\n            (edge_distance, source_embedding, target_embedding), dim=1\n        )\n    else:\n        x_edge = edge_distance\n\n    # x_source = x.clone()\n    x_target = x.clone()\n    # x_source._expand_edge(edge_index[0, :]) #first dimension is the number of edges\n    x_target._expand_edge(edge_index[1, :])\n\n    # x_message_data = torch.cat((x_source.embedding, x_target.embedding, edge_fea.embedding), dim=2) #concatenate source and target node embeddings along channel dimension\n    x_message_data = torch.cat(\n        (x_target.embedding, hidden_fea.embedding), dim=2\n    )  # concatenate source and target node embeddings along channel dimension\n\n    x_message = SO3_Embedding(\n        0,\n        x_target.lmax,\n        x_target.num_channels * 2,\n        device=x_target.device,\n        dtype=x_target.dtype,\n    )\n    x_message.set_embedding(x_message_data)\n    x_message.set_lmax_mmax(self.lmax, self.mmax)\n\n    # radial function (scale all m components within a type-L vector of one channel with the same weight)\n    if self.use_m_share_rad:\n        x_edge_weight = self.rad_func(x_edge)\n        x_edge_weight = x_edge_weight.reshape(\n            -1, (self.lmax + 1), 2 * self.sphere_channels\n        )\n        x_edge_weight = torch.index_select(\n            x_edge_weight, dim=1, index=self.expand_index\n        )  # [E, (L_max + 1) ** 2, C]\n        x_message.embedding = x_message.embedding * x_edge_weight\n\n    # Rotate the irreps to align with the edge\n    x_message._rotate(self.SO3_rotation, self.lmax, self.mmax)\n\n    # First SO(2)-convolution\n    x_message, x_0_extra = self.so2_conv_1(x_message, x_edge)\n\n    # Activation (Gate activation)\n    x_alpha_num_channels = self.num_heads * self.attn_alpha_channels\n    x_0_gating = x_0_extra.narrow(\n        1, x_alpha_num_channels, x_0_extra.shape[1] - x_alpha_num_channels\n    )  # for activation\n    x_0_alpha = x_0_extra.narrow(\n        1, 0, x_alpha_num_channels\n    )  # for attention weights, shape [E, num_heads * attn_alpha_channels]\n    x_message.embedding = self.gate_act(x_0_gating, x_message.embedding)\n\n    # Second SO(2)-convolution\n    x_message = self.so2_conv_2(x_message, x_edge)\n\n    # Attention weights\n    x_0_alpha = x_0_alpha.reshape(\n        -1, self.num_heads, self.attn_alpha_channels\n    )  # shape of [E, num_heads, attn_alpha_channels]\n    x_0_alpha = self.alpha_norm(x_0_alpha)\n    x_0_alpha = self.alpha_act(x_0_alpha)\n    alpha = torch.einsum(\"bik, ik -&gt; bi\", x_0_alpha, self.alpha_dot)\n\n    alpha = torch_geometric.utils.softmax(alpha, edge_index[1])\n    alpha = alpha.reshape(\n        alpha.shape[0], 1, self.num_heads, 1\n    )  # shape of [E, 1, num_heads, 1]\n\n    # Attention weights * non-linear messages\n    attn = x_message.embedding  # shape of [E, (lmax+1)^2, # hidden channels]\n    attn = attn.reshape(\n        attn.shape[0], attn.shape[1], self.num_heads, self.attn_value_channels\n    )  # shape of [E, #channels, num_heads, attn_value_channels]\n    attn = attn * alpha\n    attn = attn.reshape(\n        attn.shape[0], attn.shape[1], self.num_heads * self.attn_value_channels\n    )\n    x_message.embedding = attn\n\n    # Rotate back the irreps\n    x_message._rotate_inv(self.SO3_rotation)\n\n    # Compute the sum of the incoming neighboring messages for each target node\n    x_message._reduce_edge(edge_index[1], len(x.embedding))\n\n    # Project\n    return self.proj(x_message)\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html","title":"utils","text":""},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.num_orbitals_per_atom","title":"num_orbitals_per_atom  <code>module-attribute</code>","text":"<pre><code>num_orbitals_per_atom = {'DZVP': num_orbitals_per_atom_DZVP, 'def2_SVP': num_orbitals_per_atom_def2_SVP, 'SZV': num_orbitals_per_atom_SZV}\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.num_orbitals_per_atom_DZVP","title":"num_orbitals_per_atom_DZVP  <code>module-attribute</code>","text":"<pre><code>num_orbitals_per_atom_DZVP = {'H': 5, 'O': 13, 'Ge': 13, 'Sb': 13, 'Te': 13, 'Hf': 26}\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.num_orbitals_per_atom_SZV","title":"num_orbitals_per_atom_SZV  <code>module-attribute</code>","text":"<pre><code>num_orbitals_per_atom_SZV = {'H': 1, 'O': 4, 'Hf': 10, 'Cl': 4, 'Na': 5, 'Ge': 4, 'Sb': 4, 'Te': 4, 'N': 4, 'Ti': 10, 'Pt': 10, 'V': 4}\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.num_orbitals_per_atom_def2_SVP","title":"num_orbitals_per_atom_def2_SVP  <code>module-attribute</code>","text":"<pre><code>num_orbitals_per_atom_def2_SVP = {'H': 5, 'O': 14, 'N': 14, 'C': 14}\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.orbital_type_dict","title":"orbital_type_dict  <code>module-attribute</code>","text":"<pre><code>orbital_type_dict = {'DZVP': orbital_type_dict_DZVP, 'def2_SVP': orbital_type_dict_def2_SVP, 'SZV': orbital_type_dict_SZV}\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.orbital_type_dict_DZVP","title":"orbital_type_dict_DZVP  <code>module-attribute</code>","text":"<pre><code>orbital_type_dict_DZVP = {'H': [0, 0, 1], 'O': [0, 0, 1, 1, 2], 'Ge': [0, 0, 1, 1, 2], 'Sb': [0, 0, 1, 1, 2], 'Te': [0, 0, 1, 1, 2], 'Hf': [0, 0, 0, 1, 1, 2, 2, 3]}\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.orbital_type_dict_SZV","title":"orbital_type_dict_SZV  <code>module-attribute</code>","text":"<pre><code>orbital_type_dict_SZV = {'H': [0], 'O': [0, 1], 'Hf': [0, 0, 1, 2], 'Cl': [0, 1], 'Na': [0, 0, 1], 'Ge': [0, 1], 'Sb': [0, 1], 'Te': [0, 1], 'N': [0, 1], 'Ti': [0, 0, 1, 2], 'Pt': [0, 0, 1, 2], 'V': [0, 1]}\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.orbital_type_dict_def2_SVP","title":"orbital_type_dict_def2_SVP  <code>module-attribute</code>","text":"<pre><code>orbital_type_dict_def2_SVP = {'H': [0, 0, 1], 'O': [0, 0, 0, 1, 1, 2], 'N': [0, 0, 0, 1, 1, 2], 'C': [0, 0, 0, 1, 1, 2]}\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.periodic_table","title":"periodic_table  <code>module-attribute</code>","text":"<pre><code>periodic_table = {'Ac': 89, 'Ag': 47, 'Al': 13, 'Am': 95, 'Ar': 18, 'As': 33, 'At': 85, 'Au': 79, 'B': 5, 'Ba': 56, 'Be': 4, 'Bi': 83, 'Bk': 97, 'Br': 35, 'C': 6, 'Ca': 20, 'Cd': 48, 'Ce': 58, 'Cf': 98, 'Cl': 17, 'Cm': 96, 'Co': 27, 'Cr': 24, 'Cs': 55, 'Cu': 29, 'Dy': 66, 'Er': 68, 'Es': 99, 'Eu': 63, 'F': 9, 'Fe': 26, 'Fm': 100, 'Fr': 87, 'Ga': 31, 'Gd': 64, 'Ge': 32, 'H': 1, 'He': 2, 'Hf': 72, 'Hg': 80, 'Ho': 67, 'I': 53, 'In': 49, 'Ir': 77, 'K': 19, 'Kr': 36, 'La': 57, 'Li': 3, 'Lr': 103, 'Lu': 71, 'Md': 101, 'Mg': 12, 'Mn': 25, 'Mo': 42, 'N': 7, 'Na': 11, 'Nb': 41, 'Nd': 60, 'Ne': 10, 'Ni': 28, 'No': 102, 'Np': 93, 'O': 8, 'Os': 76, 'P': 15, 'Pa': 91, 'Pb': 82, 'Pd': 46, 'Pm': 61, 'Po': 84, 'Pr': 59, 'Pt': 78, 'Pu': 94, 'Ra': 88, 'Rb': 37, 'Re': 75, 'Rh': 45, 'Rn': 86, 'Ru': 44, 'S': 16, 'Sb': 51, 'Sc': 21, 'Se': 34, 'Si': 14, 'Sm': 62, 'Sn': 50, 'Sr': 38, 'Ta': 73, 'Tb': 65, 'Tc': 43, 'Te': 52, 'Th': 90, 'Ti': 22, 'Tl': 81, 'Tm': 69, 'U': 92, 'Va': 23, 'W': 74, 'Xe': 54, 'Y': 39, 'Yb': 70, 'Zn': 30, 'Zr': 40, 'Rf': 104, 'Db': 105, 'Sg': 106, 'Bh': 107, 'Hs': 108, 'Mt': 109, 'Ds': 110, 'Rg': 111, 'Cn': 112, 'Nh': 113, 'Fl': 114, 'Mc': 115, 'Lv': 116, 'Ts': 117, 'Og': 118, 'V': 0}\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.assemble_hamiltonian","title":"assemble_hamiltonian","text":"<pre><code>assemble_hamiltonian(H_pred, numbers, edge_index, equivariant_blocks, atom_orbitals, out_slices)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def assemble_hamiltonian(\n    H_pred, numbers, edge_index, equivariant_blocks, atom_orbitals, out_slices\n):\n    H_full = {}\n\n    elements = torch.unique(numbers, sorted=True)\n\n    num_orbitals = [\n        np.sum(2 * np.array(atom_orbitals[str(element.item())]) + 1)\n        for element in elements\n    ]  # num of orbitals for each element\n\n    cum_num_orbitals = np.concatenate(([0], np.cumsum(num_orbitals)), axis=0)\n    print(cum_num_orbitals)\n\n    for index_edge in range(edge_index.shape[1]):\n        i = edge_index[0][index_edge].item()  # atom index\n        j = edge_index[1][index_edge].item()\n\n        # key_term = (i,j)#edge key term\n        key_term = (numbers[i].item(), numbers[j].item(), i, j)\n\n        num_orbitals_i = np.sum(2 * np.array(atom_orbitals[str(numbers[i].item())]) + 1)\n        num_orbitals_j = np.sum(2 * np.array(atom_orbitals[str(numbers[j].item())]) + 1)\n\n        fill = 0\n        non_zero_block = torch.full((num_orbitals_i, num_orbitals_j), fill, dtype=float)\n\n        full_block = torch.full(\n            (sum(num_orbitals), (sum(num_orbitals))), fill, dtype=float\n        )\n\n        internal_index_i = torch.where(elements == numbers[i])[0][0]\n        internal_index_j = torch.where(elements == numbers[j])[0][0]\n\n        # H_prev = torch.zeros((sum(num_orbitals),sum(num_orbitals)),dtype=float) #initialize the hamiltonian matrix\n\n        # H_prev[key_term] = init\n\n        for index_target, equivariant_block in enumerate(equivariant_blocks):\n            for N_M_str, block_slice in equivariant_block.items():\n                slice_row = slice(block_slice[0], block_slice[1])\n                slice_col = slice(block_slice[2], block_slice[3])\n                len_row = block_slice[1] - block_slice[0]\n                len_col = block_slice[3] - block_slice[2]\n                slice_out = slice(\n                    out_slices[index_target], out_slices[index_target + 1]\n                )\n\n                condition_atomic_number_i, condition_atomic_number_j = N_M_str.split()\n\n                if numbers[edge_index[0][index_edge]].item() == int(\n                    condition_atomic_number_i\n                ) and numbers[edge_index[1][index_edge]].item() == int(\n                    condition_atomic_number_j\n                ):\n                    # H_prev[key_term][slice_row, slice_col] = H_pred[index_edge][slice_out].reshape(len_row, len_col)\n                    non_zero_block[slice_row, slice_col] = H_pred[index_edge][\n                        slice_out\n                    ].reshape(len_row, len_col)\n\n        full_block[\n            cum_num_orbitals[internal_index_i] : cum_num_orbitals[internal_index_i + 1],\n            cum_num_orbitals[internal_index_j] : cum_num_orbitals[internal_index_j + 1],\n        ] = non_zero_block\n\n        if key_term not in H_full:\n            H_full[str(key_term)] = full_block\n\n        else:\n            H_full_temp = H_full[str(key_term)]\n            H_full[str(key_term)] = torch.cat((H_full_temp, full_block), dim=0)\n    return H_full\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.compute_rotation","title":"compute_rotation","text":"<pre><code>compute_rotation(axis_a, axis_b)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def compute_rotation(axis_a, axis_b):\n    # Normalize vectors\n    axis_a_norm = np.linalg.norm(axis_a)\n    axis_b_norm = np.linalg.norm(axis_b)\n    norm_axis_a = axis_a / axis_a_norm\n    norm_axis_b = axis_b / axis_b_norm\n\n    # Compute rotation axis\n    rotation_axis = np.cross(norm_axis_a, norm_axis_b)\n    rotation_axis_norm = np.linalg.norm(rotation_axis)\n    if rotation_axis_norm != 0:\n        rotation_axis /= rotation_axis_norm  # Normalize the rotation axis\n\n    # Compute rotation angle\n    dot_product = np.dot(norm_axis_a, norm_axis_b)\n    rotation_angle = np.arccos(\n        np.clip(dot_product, -1.0, 1.0)\n    )  # Clip to avoid numerical errors\n\n    return rotation_axis, np.degrees(rotation_angle)\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.create_rotation_dic","title":"create_rotation_dic","text":"<pre><code>create_rotation_dic(edge_indices, coordinates, structure)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def create_rotation_dic(edge_indices, coordinates, structure):\n    coordinates = np.array(coordinates)\n    vectors = [\n        (coordinates[edge_indices[1][i]] - coordinates[edge_indices[0][i]])\n        for i in range(len(edge_indices[0]))\n    ]\n    rotate_dic = {}\n\n    # print(vectors[0])\n\n    array_rcut = np.ones(len(structure.atomic_structure)) * structure.rcut\n\n    neighbour_list = NeighborList(\n        array_rcut, skin=0, self_interaction=False, bothways=True\n    )\n    neighbour_list.update(structure.atomic_structure)\n\n    # indicies = neighbour_list.get_neighbors(0)\n    # print(indices)\n\n    for i in range(len(edge_indices[0])):\n        key_str = (edge_indices[0][i].item(), edge_indices[1][i].item())\n\n        if edge_indices[1][i] == edge_indices[0][i]:\n            nearest_neighbours, offsets = neighbour_list.get_neighbors(\n                edge_indices[0][i]\n            )\n            nearest_vectors = [\n                (coordinates[nearest_neighbours[k]] - coordinates[edge_indices[0][i]])\n                for k in range(len(nearest_neighbours))\n            ]\n            norms = [np.linalg.norm(v) for v in nearest_vectors]\n            vectors[i] = nearest_vectors[np.argmin(norms)]\n\n        R = create_rotation_matrix(np.array([1, 0, 0]), np.array(vectors[i]))\n        rotate_dic[key_str] = R\n\n    return rotate_dic\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.create_rotation_matrix","title":"create_rotation_matrix","text":"<pre><code>create_rotation_matrix(initial_vector, final_vector)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def create_rotation_matrix(initial_vector, final_vector):\n    axis, angle = compute_rotation(initial_vector, final_vector)\n    x = np.array([1, 0, 0])\n    y = np.array([0, 1, 0])\n    z = np.array([0, 0, 1])\n\n    rotated_x = rotate_vector(x, angle, axis)\n    rotated_y = rotate_vector(y, angle, axis)\n    rotated_z = rotate_vector(z, angle, axis)\n\n    R = np.stack(\n        (rotated_x, rotated_y, rotated_z), axis=0\n    )  # each row vector represents a basis vector\n    return torch.tensor(R)\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.element_statistics","title":"element_statistics","text":"<pre><code>element_statistics(numbers)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def element_statistics(numbers):\n    index_to_Z, inverse_indices = torch.unique(\n        numbers, sorted=True, return_inverse=True\n    )\n    Z_to_index = torch.full((100,), -1, dtype=torch.int64)\n    Z_to_index[index_to_Z] = torch.arange(len(index_to_Z))\n    return index_to_Z, Z_to_index\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.get_number_orbitals_QM7","title":"get_number_orbitals_QM7","text":"<pre><code>get_number_orbitals_QM7(database)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def get_number_orbitals_QM7(database):\n    basis_def = database.metadata[\"basisdef\"]\n    basis_def = np.array(basis_def)\n    n_orbitals = np.zeros(basis_def.shape[0], dtype=int)\n\n    for i in range(basis_def.shape[0]):\n        n_orbitals[i] = int(np.count_nonzero(basis_def[i, :, 2]))\n\n    return n_orbitals\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.map_atom_to_orbital","title":"map_atom_to_orbital","text":"<pre><code>map_atom_to_orbital(atom_index, atomic_numbers, atom_orbitals)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def map_atom_to_orbital(atom_index, atomic_numbers, atom_orbitals):\n    num_orbitals_per_atom = [\n        np.sum(2 * np.array(atom_orbitals[str(atomic_number)]) + 1)\n        for atomic_number in atomic_numbers\n    ]\n    starting_index = int(\n        np.sum(num_orbitals_per_atom[:atom_index]) + 1\n    )  # since hamiltonian orbital index starts from 1\n    num_orbitals = num_orbitals_per_atom[atom_index]\n    return starting_index, num_orbitals\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.read_sparse_hamiltonian_csr","title":"read_sparse_hamiltonian_csr","text":"<pre><code>read_sparse_hamiltonian_csr(file_path)\n</code></pre> <p>Read a sparse Hamiltonian matrix in CSR format from a file and return the matrix in a dictionary format.</p> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def read_sparse_hamiltonian_csr(file_path):\n    \"\"\"\n    Read a sparse Hamiltonian matrix in CSR format from a file and return the matrix in a dictionary format.\n    \"\"\"\n\n    # indptr = []\n    indices = []\n    data = []\n\n    with Path(file_path).open() as f:\n        lines = f.readlines()\n        for line in lines:\n            data_str = line.strip().split()\n            if len(data_str) &gt;= 3:\n                indices.append([int(data_str[0]), int(data_str[1])])\n                data.append(float(data_str[2]))\n    full_hamiltonian = {}\n    for i in range(len(indices)):\n        full_hamiltonian[(indices[i][0], indices[i][1])] = data[i]\n        full_hamiltonian[(indices[i][1], indices[i][0])] = data[i]\n    return full_hamiltonian\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.read_xyz_file","title":"read_xyz_file","text":"<pre><code>read_xyz_file(file_path)\n</code></pre> <p>Read an XYZ file and return the atomic species (string list) and coordinates (float list).</p> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def read_xyz_file(file_path):\n    \"\"\"\n    Read an XYZ file and return the atomic species (string list) and coordinates (float list).\n    \"\"\"\n\n    atomic_species = []\n    coordinates = []\n\n    # file_path = Path(\"data.txt\")\n    # with open(file_path) as f:\n\n    with Path(file_path).open() as f:\n        lines = f.readlines()\n        # num_atoms = int(lines[0])\n\n        for line in lines[2:]:  # Skip the first two lines\n            data = line.split()\n            atomic_species.append(data[0])\n            coordinates.append([float(coord) for coord in data[1:4]])\n\n    return atomic_species, coordinates\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.rotate_data","title":"rotate_data","text":"<pre><code>rotate_data(y, edge_indices, coordinates, orbital_type_dict, atomic_species, rotate_dic)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def rotate_data(\n    y, edge_indices, coordinates, orbital_type_dict, atomic_species, rotate_dic\n):\n    coordinates = np.array(coordinates)\n    rotated_y = y\n    for i in range(len(y)):\n        R = rotate_dic[(edge_indices[0][i].item(), edge_indices[1][i].item())]\n        edge = [edge_indices[0][i], edge_indices[1][i]]\n        rotated_y[i] = rotate_ham(R.T, y[i], orbital_type_dict, atomic_species, edge)\n\n    return rotated_y\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.rotate_data_back","title":"rotate_data_back","text":"<pre><code>rotate_data_back(pred, y, edge_indices, rotate_dic, structures)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def rotate_data_back(pred, y, edge_indices, rotate_dic, structures):\n    rotated_pred = []\n    reduced_y = []\n\n    graph_num = 0\n    num_edges = int(len(edge_indices[0]) / len(structures))\n\n    # iterates over the edges in the graph\n    for i in range(len(y)):\n        # Gets the structure for the edge\n        if i &gt; 0 and i % num_edges == 0:\n            graph_num += 1\n\n        structure = structures[graph_num]\n        atomic_species = np.array(structure.atomic_structure.get_chemical_symbols())\n\n        # Calculate the starting index for each atom type in the orbital block\n        unique_elements = set(structure.atomic_structure.get_chemical_symbols())\n        mat_block_start = {}\n        block_start = 0\n        for element in unique_elements:\n            mat_block_start[element] = block_start\n            block_start += num_orbitals_per_atom[structure.basis][element]\n\n        atom_i_index = edge_indices[0][i] % len(atomic_species)  # local atom index\n        atom_j_index = edge_indices[1][i] % len(atomic_species)  # local atom index\n\n        atom_i_element = atomic_species[atom_i_index]\n        atom_j_element = atomic_species[atom_j_index]\n\n        atom_i_start = mat_block_start[atom_i_element]\n        atom_j_start = mat_block_start[atom_j_element]\n\n        # starting_index_i, num_orbitals_i = structure.map_atom_to_orbital(atom_i_index) #edge_indices[0][i])\n        # starting_index_j, num_orbitals_j = structure.map_atom_to_orbital(atom_j_index) #edge_indices[1][i])\n\n        # atom_i_end = atom_i_start+num_orbitals_i\n        # atom_j_end = atom_j_start+num_orbitals_j\n\n        atom_i_end = atom_i_start + structure.num_orbitals_per_atom[atom_i_index]\n        atom_j_end = atom_j_start + structure.num_orbitals_per_atom[atom_j_index]\n\n        prediction = (\n            pred[i][atom_i_start:atom_i_end, atom_j_start:atom_j_end].detach().numpy()\n        )\n        reshaped_y = (\n            y[i][atom_i_start:atom_i_end, atom_j_start:atom_j_end].detach().numpy()\n        )\n\n        R = rotate_dic[(atom_i_index.item(), atom_j_index.item())]\n\n        edge = [atom_i_index, atom_j_index]\n\n        # pad the blocks again\n        blocksize = structure.num_unique_orbitals\n        prediction_padded = np.zeros((blocksize, blocksize))\n        prediction_padded[atom_i_start:atom_i_end, atom_j_start:atom_j_end] = (\n            rotate_ham(\n                R, prediction, orbital_type_dict[structure.basis], atomic_species, edge\n            )\n        )\n        rotated_pred.append(prediction_padded)\n\n        reshaped_y_padded = np.zeros((blocksize, blocksize))\n        reshaped_y_padded[atom_i_start:atom_i_end, atom_j_start:atom_j_end] = (\n            rotate_ham(\n                R, reshaped_y, orbital_type_dict[structure.basis], atomic_species, edge\n            )\n        )\n        reduced_y.append(reshaped_y_padded)\n\n        # rotated_pred.append(rotate_ham(R,prediction,orbital_type_dict[structure.basis],atomic_species,edge))\n\n    rotated_pred = torch.tensor(np.array(rotated_pred))\n    reduced_y = torch.tensor(np.array(reduced_y))\n\n    return rotated_pred, reduced_y\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.rotate_ham","title":"rotate_ham","text":"<pre><code>rotate_ham(R, H, orbital_type_dict, atomic_species, edge)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def rotate_ham(R, H, orbital_type_dict, atomic_species, edge):\n    R = R.index_select(0, R.new_tensor([1, 2, 0]).int()).index_select(\n        1, R.new_tensor([1, 2, 0]).int()\n    )\n    l_lefts = orbital_type_dict[atomic_species[edge[0]]]\n    l_right = orbital_type_dict[atomic_species[edge[1]]]\n\n    irreps_left = Irreps([(1, (l_dx, 1)) for l_dx in l_lefts])\n    U_left = irreps_left.D_from_matrix(R)\n    irreps_right = Irreps([(1, (l_dx, 1)) for l_dx in l_right])\n    U_right = irreps_right.D_from_matrix(R)\n\n    return (U_left.transpose(-1, -2) @ H @ U_right).numpy()\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.rotate_irrep_vector","title":"rotate_irrep_vector","text":"<pre><code>rotate_irrep_vector(R, vector, irreps_left)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def rotate_irrep_vector(R, vector, irreps_left):\n    vector = torch.tensor(vector)\n    R = R.index_select(0, R.new_tensor([1, 2, 0]).int()).index_select(\n        1, R.new_tensor([1, 2, 0]).int()\n    )\n    # irreps_left = Irreps([(1, (l, 1)) for l in l_lefts])\n    U_left = irreps_left.D_from_matrix(R).float()\n    return vector @ U_left\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.rotate_vector","title":"rotate_vector","text":"<pre><code>rotate_vector(vector, angle, axis)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def rotate_vector(vector, angle, axis):\n    # Create a rotation object\n    rotation = Rotation.from_rotvec(np.radians(angle) * axis)\n    # Apply rotation to the vector\n    return rotation.apply(vector)\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.unflatten","title":"unflatten","text":"<pre><code>unflatten(H_pred, numbers, edge_index, equivariant_blocks, atom_orbitals, out_slices)\n</code></pre> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def unflatten(\n    H_pred, numbers, edge_index, equivariant_blocks, atom_orbitals, out_slices\n):\n    # Precompute number of orbitals for each atom\n    atom_orbitals_count = {\n        key: np.sum(2 * np.array(atom_orbitals[key]) + 1) for key in atom_orbitals\n    }\n\n    H_prev = {}\n\n    for index_edge in range(edge_index.shape[1]):\n        i = edge_index[0][index_edge].item()  # atom index\n        j = edge_index[1][index_edge].item()\n\n        key_term = (i, j)  # edge key term\n\n        # Precompute number of orbitals for atoms i and j\n        num_orbitals_i = atom_orbitals_count[str(numbers[i].item())]\n        num_orbitals_j = atom_orbitals_count[str(numbers[j].item())]\n\n        # Initialize H_prev for this edge\n        H_prev[key_term] = torch.zeros((num_orbitals_i, num_orbitals_j), dtype=float)\n\n        H_prev_edge = H_prev[\n            key_term\n        ]  # Avoid repeated dictionary lookup as in unoptimized version\n\n        for index_target, equivariant_block in enumerate(equivariant_blocks):\n            slice_out = slice(out_slices[index_target], out_slices[index_target + 1])\n\n            # Precompute block slices for this equivariant block\n            for N_M_str, block_slice in equivariant_block.items():\n                slice_row = slice(block_slice[0], block_slice[1])\n                slice_col = slice(block_slice[2], block_slice[3])\n                len_row = block_slice[1] - block_slice[0]\n                len_col = block_slice[3] - block_slice[2]\n\n                condition_atomic_number_i, condition_atomic_number_j = N_M_str.split()\n\n                if numbers[i].item() == int(condition_atomic_number_i) and numbers[\n                    j\n                ].item() == int(condition_atomic_number_j):\n                    H_prev_edge[slice_row, slice_col] = H_pred[index_edge][\n                        slice_out\n                    ].reshape(len_row, len_col)\n\n    return H_prev\n</code></pre>"},{"location":"reference/augmented_partition/model/utils.html#augmented_partition.model.utils.write_xyz_file","title":"write_xyz_file","text":"<pre><code>write_xyz_file(file_path, atomic_species, coordinates, indices)\n</code></pre> <p>Write an XYZ file with the given atomic species (string list) and coordinates (float list). Only the atoms with indices in the list 'indices' will be written to the file.</p> Source code in <code>augmented_partition/model/utils.py</code> <pre><code>def write_xyz_file(file_path, atomic_species, coordinates, indices):\n    \"\"\"\n    Write an XYZ file with the given atomic species (string list) and coordinates (float list).\n    Only the atoms with indices in the list 'indices' will be written to the file.\n    \"\"\"\n\n    # with open(file_path, \"w\") as f:\n    with Path(file_path).open(\"w\") as f:\n        f.write(str(len(indices)) + \"\\n\")\n        f.write(\"Generated by SO(3)-Equivariant Graph Neural Networks\\n\")\n\n        for i in range(len(atomic_species)):\n            if i in indices:\n                f.write(\n                    atomic_species[i]\n                    + \" \"\n                    + \" \".join([str(coord) for coord in coordinates[i]])\n                    + \"\\n\"\n                )\n</code></pre>"}]}